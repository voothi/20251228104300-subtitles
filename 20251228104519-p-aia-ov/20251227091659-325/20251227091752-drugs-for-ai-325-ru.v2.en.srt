1
00:00:00,000 --> 00:00:01,010
So what's new with you?

2
00:00:04,510 --> 00:00:06,050
What's new with me?

3
00:00:06,160 --> 00:00:06,870
I was in Poland.

4
00:00:08,010 --> 00:00:08,570
Seriously?

5
00:00:08,750 --> 00:00:09,710
Recently, yeah.

6
00:00:10,850 --> 00:00:11,990
Who did you meet with?

7
00:00:12,570 --> 00:00:13,430
With you.

8
00:00:14,330 --> 00:00:14,910
No way?

9
00:00:15,690 --> 00:00:15,990
Yeah.

10
00:00:17,030 --> 00:00:18,630
So what were we doing there?

11
00:00:19,170 --> 00:00:23,670
Oh, we were preparing a New Year's surprise for our subscribers.

12
00:00:23,690 --> 00:00:26,700
Oh yeah, at the end today we'll talk a little more about it, right?

13
00:00:26,770 --> 00:00:27,950
Yeah, yeah, yeah, yeah.

14
00:00:27,950 --> 00:00:28,000
Yeah.

15
00:00:29,870 --> 00:00:30,470
I see.

16
00:00:31,070 --> 00:00:33,290
So I was in Warsaw too, it turns out.

17
00:00:35,170 --> 00:00:36,010
Met you.

18
00:00:37,430 --> 00:00:43,590
I was also in Krakow and saw Krakow for the first time in my life.

19
00:00:44,010 --> 00:00:45,250
Was it your first time in Krakow?

20
00:00:45,550 --> 00:00:46,300
Yeah, yeah.

21
00:00:46,730 --> 00:00:47,270
And how did you like it?

22
00:00:48,250 --> 00:00:48,850
Beautiful.

23
00:00:49,430 --> 00:00:51,470
Listen, well, all that's left for you is to get to Wroclaw.

24
00:00:52,630 --> 00:00:54,770
Yeah, and you can say you've seen all of Poland.

25
00:00:54,820 --> 00:00:55,670
Pretty much all of Poland.

26
00:00:55,750 --> 00:00:57,350
Could I have just gone to Wroclaw?

27
00:00:57,930 --> 00:00:58,310
Well, yeah.

28
00:00:58,470 --> 00:00:58,720
What are you talking about?

29
00:00:58,730 --> 00:01:01,310
Well, look, I've been to Warsaw, I've been to Gdansk.

30
00:01:01,870 --> 00:01:03,110
And Krakow now.

31
00:01:03,130 --> 00:01:04,630
So only Wroclaw is left, it turns out.

32
00:01:05,680 --> 00:01:06,590
Wait.

33
00:01:07,250 --> 00:01:08,050
Lublin.

34
00:01:08,450 --> 00:01:09,410
Poznan.

35
00:01:09,470 --> 00:01:10,070
Bialystok.

36
00:01:10,150 --> 00:01:10,630
Lodz.

37
00:01:11,070 --> 00:01:12,030
Bialystok.

38
00:01:12,330 --> 00:01:13,290
Katowice.

39
00:01:13,990 --> 00:01:16,250
That's just what I...

40
00:01:16,250 --> 00:01:17,890
Biala Podlaska, but that's...

41
00:01:17,890 --> 00:01:18,850
Biala Podlaska.

42
00:01:19,540 --> 00:01:20,530
Bielsko-Biala.

43
00:01:21,170 --> 00:01:21,770
Zakopane.

44
00:01:23,450 --> 00:01:24,050
Zielona Gora.

45
00:01:24,450 --> 00:01:25,050
Jelenia Gora.

46
00:01:26,470 --> 00:01:27,070
Bydgoszcz.

47
00:01:27,450 --> 00:01:29,150
A ton of cities that are worth visiting.

48
00:01:29,310 --> 00:01:34,730
What's most interesting is that in Poland they are more or less similar, but still, each one has something of its own.

49
00:01:36,040 --> 00:01:37,790
Every single one stands out in some way.

50
00:01:38,210 --> 00:01:39,770
You still need to travel around Poland more.

51
00:01:40,040 --> 00:01:40,470
I'm telling you.

52
00:01:40,550 --> 00:01:41,430
Anyway, yeah, yeah.

53
00:01:41,550 --> 00:01:44,970
Get me a Globe of Poland, please, so I know where to go.

54
00:01:45,480 --> 00:01:47,770
Better yet, someday I'll gift you a vacation in Poland.

55
00:01:48,070 --> 00:01:48,290
There.

56
00:01:48,410 --> 00:01:51,050
We'll get in a car and drive all over Poland.

57
00:01:51,590 --> 00:01:52,590
That would be awesome.

58
00:01:53,790 --> 00:01:55,050
That would be awesome.

59
00:01:55,110 --> 00:01:57,070
We could record an episode from the car at the same time.

60
00:01:57,650 --> 00:01:58,740
Yeah, by the way, yeah.

61
00:01:59,500 --> 00:02:03,830
If you want to become sponsors for our episode from the car, we've already been in contact.

62
00:02:04,360 --> 00:02:05,390
Write to us.

63
00:02:17,760 --> 00:02:21,090
Hello everyone, our dear podcast listeners.

64
00:02:21,420 --> 00:02:24,270
This is a new episode of the Na Vibe podcast.

65
00:02:24,320 --> 00:02:31,750
A podcast about news, artificial intelligence, and all sorts of other topics related to artificial intelligence for programmers and beyond.

66
00:02:32,320 --> 00:02:38,110
And with you are your regular hosts - myself, Viktor Shlenchenko, and...

67
00:02:38,160 --> 00:02:39,890
I'm Alexey Kartynnik.

68
00:02:40,600 --> 00:02:42,050
Yeeeah, super.

69
00:02:42,580 --> 00:02:43,730
Okay, what was that?

70
00:02:43,980 --> 00:02:45,970
For the second time in the history of our podcast.

71
00:02:46,160 --> 00:02:49,610
You swiped, I'm not afraid to use the word, my intro.

72
00:02:50,220 --> 00:02:54,630
Well, you were silent, so I took the intro into my own hands.

73
00:02:54,810 --> 00:02:57,890
Well, alright, thank you actually, because today my mood is kinda meh.

74
00:02:58,870 --> 00:03:02,230
For all Belarusians, I would say.

75
00:03:02,530 --> 00:03:03,250
Those who know, know.

76
00:03:05,030 --> 00:03:06,630
That's for sure, that's for sure.

77
00:03:08,550 --> 00:03:08,870
So,

78
00:03:10,170 --> 00:03:11,350
we've done the intro.

79
00:03:12,030 --> 00:03:12,550
Done.

80
00:03:13,250 --> 00:03:14,090
Yeah, done.

81
00:03:14,350 --> 00:03:15,990
We're not playing music today.

82
00:03:17,960 --> 00:03:20,710
But we are playing something else.

83
00:03:21,030 --> 00:03:34,870
We're putting up a reminder for our dear listeners that the Na Vibe podcast, formerly the AIA podcast, is now released exclusively thanks to our premium listeners.

84
00:03:35,520 --> 00:03:39,430
Premium listeners are people who give us a recurring donation.

85
00:03:39,610 --> 00:03:54,690
We have several different subscription plans: Go, Pro, and Ultra, where, besides supporting us, which is kind of the main point of these donations, you also get a lot of different cool bonuses.

86
00:03:55,640 --> 00:04:07,410
For example, you get added to the premium chat, you get access to recordings of all our streams uncensored and without any cuts whatsoever.

87
00:04:07,720 --> 00:04:09,970
And we post them earlier, literally right away.

88
00:04:10,140 --> 00:04:11,990
Like, we record today, and we post it tonight.

89
00:04:12,310 --> 00:04:13,610
No need to wait.

90
00:04:13,660 --> 00:04:15,030
They appear there earlier, so there's no need to wait.

91
00:04:15,530 --> 00:04:21,190
You also get access to a unique post-cast, the Post-Vibe Post-cast.

92
00:04:21,590 --> 00:04:38,630
It's a completely separate podcast that exists exclusively for our premium listeners, where Lesha and I talk about all sorts of interesting things related to AI, science fiction, and other stuff.

93
00:04:38,680 --> 00:04:48,010
For example, we have an episode where Lesha talks about a science fiction book by a Belarusian author called "Pentaquantum".

94
00:04:48,010 --> 00:04:53,310
Actually, it's a pretty unique episode, because the book isn't very popular yet.

95
00:04:55,080 --> 00:05:02,170
And maybe you'll be able to read it, or maybe you'll be able to listen to it.

96
00:05:02,760 --> 00:05:08,990
Well, there's some inside info that it's slowly being translated into English from Belarusian, but you can still treat yourself.

97
00:05:09,120 --> 00:05:11,190
Nevertheless, yes, you can treat yourself.

98
00:05:11,500 --> 00:05:17,030
We have a True Crime podcast, which, damn, I wrote myself with Chat GPT.

99
00:05:17,640 --> 00:05:18,050
A debut?

100
00:05:19,180 --> 00:05:22,330
The debut one, yes, about AI murders, so to speak.

101
00:05:22,660 --> 00:05:31,050
And a third one, the very newest, the latest one, hot off the press, where I retell Andy Weir's book Project Hail Mary.

102
00:05:31,100 --> 00:05:35,210
And what's more, you can manage to listen to it before the film adaptation comes out.

103
00:05:35,390 --> 00:05:36,770
It's coming out in 2026.

104
00:05:37,400 --> 00:05:39,790
If you happen to be afraid of spoilers, don't be.

105
00:05:40,040 --> 00:05:48,070
It's heavy-duty science fiction that's impossible to retell in any detail in an hour and a half.

106
00:05:48,740 --> 00:05:51,810
So there are plot spoilers, but they're the big-picture ones.

107
00:05:51,990 --> 00:05:55,750
And overall, personally, I haven't read the book, and I'm planning to watch the movie.

108
00:05:56,060 --> 00:06:02,950
And after that summary, I have even more of a pre-New Year's desire to read the book, because the plot is interesting and I want to get into the details.

109
00:06:03,220 --> 00:06:04,210
Yeah, that's true.

110
00:06:04,470 --> 00:06:06,530
And I retold the details very shittily.

111
00:06:07,140 --> 00:06:07,770
Intentionally.

112
00:06:08,560 --> 00:06:10,390
Yes, purely intentionally.

113
00:06:10,520 --> 00:06:14,510
Not because I'm dumb and don't understand half of the physics references written there.

114
00:06:15,060 --> 00:06:16,190
But just intentionally.

115
00:06:16,550 --> 00:06:17,650
That's right.

116
00:06:18,520 --> 00:06:20,570
Yes, all these episodes are already available.

117
00:06:20,950 --> 00:06:26,470
So subscribe, subscribe, you'll have a great time.

118
00:06:26,520 --> 00:06:32,030
Especially at this time, before the New Year, when there's a lot of time to listen to something good.

119
00:06:32,200 --> 00:06:35,030
And there will only be more episodes, actually.

120
00:06:35,880 --> 00:06:39,910
And for other bonuses, that's not all the bonuses available to our premium members.

121
00:06:40,360 --> 00:06:45,710
All premium members get access to a premium chat, where we answer first, we hang out there, we chat.

122
00:06:46,760 --> 00:06:49,270
Also, all premium members get access to the SMS chat.

123
00:06:49,580 --> 00:06:55,710
It's a little chat where you can send your messages of up to 120 characters before each episode.

124
00:06:56,520 --> 00:06:57,330
Like in text messages.

125
00:06:57,450 --> 00:07:00,210
And we will read these messages live on air.

126
00:07:00,350 --> 00:07:01,830
By the way, we have one text message today.

127
00:07:02,690 --> 00:07:04,450
We'll read it shortly.

128
00:07:05,150 --> 00:07:17,730
And if you subscribe to the maximum Ultra tier, along with it you'll get a yearly subscription to the Code Evolution club, where you can seriously level up in programming with AI.

129
00:07:17,950 --> 00:07:26,450
And you'll get a yearly subscription to the DeeDee app, where you can keep your ADHD in check.

130
00:07:26,520 --> 00:07:27,590
Very well.

131
00:07:28,250 --> 00:07:28,650
Yeah, yeah, yeah.

132
00:07:28,850 --> 00:07:30,030
A full year's access.

133
00:07:30,230 --> 00:07:30,610
That's all correct.

134
00:07:31,510 --> 00:07:37,070
So there are a lot of bonuses, the bonuses are cool, but first and foremost, you'll be supporting our podcast this way.

135
00:07:37,220 --> 00:07:44,950
Right now it's really released exclusively thanks to you, thanks to our audience, thanks to your support for this podcast.

136
00:07:45,780 --> 00:08:01,730
Yeah, and guys, I get that, like, maybe some of you are tired of hearing us, guys and girls, talk about this, and today with pretty sour faces on top of it, but this is truly fucking important.

137
00:08:01,780 --> 00:08:07,450
I mean, well, we get donations, but it's honestly not nearly enough yet.

138
00:08:07,730 --> 00:08:15,350
And if you've been thinking about whether to donate or not, please, think again, find the opportunity if you have one.

139
00:08:15,990 --> 00:08:16,330
Thank you.

140
00:08:17,800 --> 00:08:18,750
Yeah, yeah, yeah.

141
00:08:18,850 --> 00:08:25,150
We need at least, I think, 200-300 donors so we can more or less breathe a sigh of relief.

142
00:08:25,250 --> 00:08:27,850
Right now we have what, 37 people?

143
00:08:27,970 --> 00:08:28,790
Around 36.

144
00:08:29,290 --> 00:08:30,740
Just keeping you in the loop, yeah.

145
00:08:31,200 --> 00:08:33,870
So the number isn't tiny, but it's not large either.

146
00:08:34,070 --> 00:08:35,270
We're counting on your support.

147
00:08:35,450 --> 00:08:37,890
We seem to be doing good things.

148
00:08:38,520 --> 00:08:47,990
And if you want to hear the quality of the extra content we release, these three extra episodes, we show them to you, we usually tease them in the episodes.

149
00:08:48,280 --> 00:08:54,490
Today at the end of our podcast there will be 6 minutes from the episode where Vitya talks about the Project Hail Mary.

150
00:08:54,580 --> 00:08:56,470
You can listen to it too, and judge for yourself.

151
00:08:56,850 --> 00:08:58,930
Maybe it will be easier for you to make a decision after that.

152
00:09:00,660 --> 00:09:03,670
And, I guess, it's worth saying that we're also looking for partners.

153
00:09:03,720 --> 00:09:08,930
If someone listening to us suddenly wants to help the podcast by becoming a permanent sponsor of the podcast,

154
00:09:10,170 --> 00:09:11,330
contact us directly.

155
00:09:11,490 --> 00:09:14,250
All contact information is in the podcast description.

156
00:09:14,590 --> 00:09:19,130
It could be a one-time donation from a person or a company.

157
00:09:19,270 --> 00:09:26,450
It could be full sponsorship support, where we'll talk about your services, your things, and make a separate segment for you.

158
00:09:26,950 --> 00:09:28,790
You'll be supporting us that way.

159
00:09:29,110 --> 00:09:30,990
We will gladly accept all of that.

160
00:09:31,360 --> 00:09:31,910
So, reach out.

161
00:09:31,960 --> 00:09:33,890
In short, we do ads too.

162
00:09:34,250 --> 00:09:35,370
Just like everyone else.

163
00:09:37,050 --> 00:09:39,430
Well, on that note, I think we can begin.

164
00:09:40,010 --> 00:09:44,640
Yes, let's move on to our regular segment "Big Fish".

165
00:09:44,640 --> 00:09:46,900
By the way, this is the last "Big Fish" of the year.

166
00:09:47,780 --> 00:09:49,340
Just so you know.

167
00:09:49,780 --> 00:09:52,720
Because the next episode will be unusual.

168
00:09:53,200 --> 00:09:54,920
There will be no big fish there.

169
00:09:54,980 --> 00:09:56,440
That's a small spoiler for it.

170
00:09:57,260 --> 00:09:59,900
So that's why "Big Fish".

171
00:09:59,900 --> 00:10:07,400
And they start with, what would you think.

172
00:10:08,100 --> 00:10:10,140
No suspense, with OpenAI.

173
00:10:12,150 --> 00:10:14,680
This is going to be the saddest episode of all time for us.

174
00:10:15,800 --> 00:10:16,780
Yeah, it's fucked.

175
00:10:17,100 --> 00:10:20,240
Vitya says, the last "Big Fish" and I'm like...

176
00:10:20,240 --> 00:10:21,200
Fuck...

177
00:10:21,200 --> 00:10:23,640
Yeah, I said it, and then I realized.

178
00:10:24,200 --> 00:10:26,240
Alright, let's start with OpenAI.

179
00:10:27,530 --> 00:10:28,020
OpenAI.

180
00:10:28,360 --> 00:10:29,700
Anyway, anyway.

181
00:10:30,460 --> 00:10:34,220
As we know, there's a national sport in the US.

182
00:10:34,360 --> 00:10:35,060
And not just in the US.

183
00:10:35,100 --> 00:10:36,560
It's investing in OpenAI.

184
00:10:37,290 --> 00:10:41,500
And now Amazon will be investing in OpenAI.

185
00:10:42,620 --> 00:10:46,020
Just so you know, the deal is for 500 billion bucks.

186
00:10:49,150 --> 00:10:50,580
What 500 billion?

187
00:10:50,660 --> 00:10:50,980
10.

188
00:10:51,200 --> 00:10:54,400
500 is the valuation OpenAI will secure after this deal.

189
00:10:54,650 --> 00:10:56,020
Damn, yeah, sorry.

190
00:10:56,730 --> 00:10:58,520
Amazon doesn't have that much money.

191
00:10:58,800 --> 00:11:00,300
They're worth 4 billion there.

192
00:11:00,400 --> 00:11:01,780
4 trillion or however much.

193
00:11:01,830 --> 00:11:03,140
Yes, I'm sorry, please excuse me.

194
00:11:03,460 --> 00:11:04,920
Yeah, Bezos wouldn't have forked over that much.

195
00:11:05,330 --> 00:11:07,120
Anyway, 10 billion bucks.

196
00:11:07,260 --> 00:11:09,800
The total valuation of OpenAI becomes 500 billion.

197
00:11:10,130 --> 00:11:14,600
And, as a matter of fact, the supply of Titanium chips to OpenAI.

198
00:11:14,830 --> 00:11:15,200
Trainium.

199
00:11:15,300 --> 00:11:19,340
Well, basically, they will provide that 10 billion just with these Trainium chips.

200
00:11:19,340 --> 00:11:19,980
In fact.

201
00:11:20,060 --> 00:11:20,480
With hardware.

202
00:11:21,290 --> 00:11:23,490
Well, and AWS capacity, yes.

203
00:11:25,850 --> 00:11:28,900
The funny thing is that this way they're also stepping on Nvidia's toes.

204
00:11:28,900 --> 00:11:30,280
With their Trainium chips.

205
00:11:30,650 --> 00:11:34,960
These are tensor chips, I think, that are really messing things up for them, along with the angle.

206
00:11:35,020 --> 00:11:35,820
Are they called Trainium?

207
00:11:35,960 --> 00:11:37,100
And people always read it as Titanium?

208
00:11:37,170 --> 00:11:38,000
Trainium, Trainium.

209
00:11:38,270 --> 00:11:39,720
Ah, oh, right, Trainium.

210
00:11:40,800 --> 00:11:41,560
Damn, that's crazy.

211
00:11:41,790 --> 00:11:46,080
Vitya Vedmich and I were talking recently, he said Trainium, and I was also like, damn, I thought they were Titanium.

212
00:11:47,150 --> 00:11:48,160
Well, no, Trainium.

213
00:11:48,480 --> 00:11:49,040
Trainium 3.

214
00:11:50,410 --> 00:11:52,740
I think we always said Titanium, didn't we?

215
00:11:53,250 --> 00:11:55,420
Well, I think I thought so too.

216
00:11:56,550 --> 00:11:59,840
I don't remember, do we have news about Amazon later on or not, I think?

217
00:12:00,690 --> 00:12:01,400
It's just that...

218
00:12:01,450 --> 00:12:02,220
I don't think so.

219
00:12:02,450 --> 00:12:02,700
There won't be.

220
00:12:02,820 --> 00:12:04,880
Then I'll add a bit more here about these Trainium chips.

221
00:12:05,290 --> 00:12:07,820
They announced the third version, which they will most likely,

222
00:12:08,960 --> 00:12:10,960
in particular, be pushing on OpenAI.

223
00:12:11,230 --> 00:12:13,640
But they also announced Trainium 4.

224
00:12:15,170 --> 00:12:19,080
I appreciate the level of trolling, or I don't know what it is.

225
00:12:19,280 --> 00:12:29,840
It's probably not trolling, but, in short, they said that they will have Trainium 4 chips that will be compatible with Nvidia GPUs via an Nvidia bus,

226
00:12:31,610 --> 00:12:32,440
NVFusion, NVLink Fusion.

227
00:12:34,740 --> 00:12:37,400
I don't know, well, like, on the one hand,

228
00:12:38,520 --> 00:12:43,440
they probably collaborated with Nvidia there, because you'd probably need to get some data or something from NVD.

229
00:12:43,570 --> 00:12:44,900
Or maybe it's an open protocol?

230
00:12:45,170 --> 00:12:46,100
Well, in short, it's strange.

231
00:12:46,280 --> 00:12:51,480
Like, we're making a competitor and we'll also be compatible with you, the competitor, to... you even more.

232
00:12:51,480 --> 00:12:54,500
Well yeah, from Amazon's point of view, it's as logical as it gets.

233
00:12:54,550 --> 00:12:59,040
Well, in general, OpenAI is, of course, just some kind of monstrous company.

234
00:12:59,360 --> 00:13:01,180
There was some news about that too, wasn't there.

235
00:13:01,860 --> 00:13:06,120
We didn't add it, but it popped up today, very fittingly.

236
00:13:06,520 --> 00:13:14,300
OpenAI will be buying up about 40% of all memory chips produced in the world until 2029.

237
00:13:14,720 --> 00:13:15,980
That's official now.

238
00:13:16,080 --> 00:13:17,220
As part of the Stargate project.

239
00:13:18,790 --> 00:13:20,520
Because of that, everyone is already...

240
00:13:20,520 --> 00:13:21,980
Yeah, yeah, yeah.

241
00:13:22,080 --> 00:13:25,480
Partly because of this, gamers are already saying that it will get even more expensive.

242
00:13:25,580 --> 00:13:32,480
Can you imagine, some startup from 3 years ago, which was a startup 4 years ago, is buying up 40% of the world's entire RAM.

243
00:13:32,910 --> 00:13:35,260
And 4 years ago, it was also a non-profit organization.

244
00:13:35,660 --> 00:13:36,860
Just for a second.

245
00:13:37,760 --> 00:13:39,460
Damn, just such a scale.

246
00:13:39,760 --> 00:13:39,880
Yeah.

247
00:13:41,300 --> 00:13:42,320
So that's the thing.

248
00:13:42,370 --> 00:13:50,420
But besides the news about purchases, about money, they had a lot of tech-related news in recent weeks.

249
00:13:50,540 --> 00:13:53,600
First, the top model GPT-5.2 was released.

250
00:13:53,840 --> 00:13:55,720
It's OpenAI's flagship.

251
00:13:57,780 --> 00:14:04,600
Three versions of this model were released in parallel, no, not in parallel, but one after another, you could even say four.

252
00:14:04,650 --> 00:14:10,600
First came GPT-5.2, then literally a few days later came GPT-5.2 Pro.

253
00:14:10,900 --> 00:14:14,760
This is, de facto, the most powerful model in existence today.

254
00:14:15,760 --> 00:14:20,940
And then, along with Pro, GPT-5.2 Thinking was released.

255
00:14:21,340 --> 00:14:23,540
It's supposedly tailored for math and science.

256
00:14:23,800 --> 00:14:31,620
And then, like a week later, maybe, or a little later, the GPT-5.2 Codex model for programming was released.

257
00:14:33,010 --> 00:14:34,280
How not to get confused?

258
00:14:34,660 --> 00:14:37,680
GPT-5.2 is the direct successor to GPT-5.1.

259
00:14:37,820 --> 00:14:39,980
Most likely, 5.1 will be deprecated soon.

260
00:14:40,020 --> 00:14:41,580
It costs almost the same.

261
00:14:41,800 --> 00:14:43,620
The price literally went up by...

262
00:14:43,620 --> 00:14:45,380
How much is that?

263
00:14:45,500 --> 00:14:47,420
Somewhere around 10 or 15 percent.

264
00:14:47,470 --> 00:14:51,920
It used to cost 1.5 dollars per million input tokens.

265
00:14:52,020 --> 00:14:53,080
Now it's 1.75.

266
00:14:53,820 --> 00:14:57,280
And it works better, it's better on benchmarks.

267
00:14:57,680 --> 00:15:00,260
A little bit slower, but it seems like that has already leveled out.

268
00:15:00,280 --> 00:15:04,240
And what's important, GPT-5.2 is finally a model with fresh data.

269
00:15:04,470 --> 00:15:07,900
It was trained on data up to August 2025.

270
00:15:08,180 --> 00:15:12,180
All previous models were trained on data up to September 2024.

271
00:15:12,280 --> 00:15:13,580
They were really hated on for that.

272
00:15:15,150 --> 00:15:19,620
We're not particularly interested in GPT-5.2 Pro and Thinking because, first of all,

273
00:15:20,940 --> 00:15:22,320
they are not available to everyone.

274
00:15:22,500 --> 00:15:23,580
They are available there...

275
00:15:23,580 --> 00:15:29,180
Pro is available via API, Thinking is available God knows how, but both models cost a fortune.

276
00:15:29,730 --> 00:15:34,240
The Pro version costs 21 bucks per million input and 168 per million output tokens.

277
00:15:34,450 --> 00:15:36,640
These are similarly expensive models.

278
00:15:36,850 --> 00:15:43,340
They probably were, remember, when, I think, GPT-5 or 4.5 first came out.

279
00:15:43,390 --> 00:15:45,020
5 Pro had a crazy price tag back then too.

280
00:15:45,220 --> 00:15:46,220
When the first Pro came out.

281
00:15:46,970 --> 00:15:47,720
Anyway, it's...

282
00:15:47,720 --> 00:15:48,820
The 5 series, I think it was.

283
00:15:49,430 --> 00:15:50,300
Research models.

284
00:15:50,540 --> 00:15:52,600
And with Codex, things aren't so great either.

285
00:15:52,950 --> 00:15:56,760
It seems that, well, Codex is relevant to us, to our audience, we have a lot of developers.

286
00:15:57,070 --> 00:16:01,080
But 5.2 Codex will only be available in the API in early 2026.

287
00:16:01,390 --> 00:16:05,740
And for now, it's only available through their command-line interface Codex.

288
00:16:05,930 --> 00:16:09,380
Well, or through an extension for that command-line interface.

289
00:16:11,010 --> 00:16:14,100
So that's the news on development models from OpenAI.

290
00:16:15,530 --> 00:16:18,760
Yeah, but it's not just about development alone.

291
00:16:18,840 --> 00:16:20,920
But, first of all, these aren't just models for development.

292
00:16:21,210 --> 00:16:22,200
Well, yeah, yeah, yeah.

293
00:16:22,300 --> 00:16:23,040
I mean, with LLMs.

294
00:16:23,410 --> 00:16:24,920
You can barely even call them LLMs anymore.

295
00:16:25,130 --> 00:16:28,860
With models that primarily work with text.

296
00:16:29,120 --> 00:16:31,300
Let's put it this way, foundational models.

297
00:16:32,690 --> 00:16:33,320
There.

298
00:16:33,550 --> 00:16:38,900
But GPT, or rather, OpenAI, remembered that they have a model that generates images.

299
00:16:39,190 --> 00:16:40,000
GPT Image.

300
00:16:40,710 --> 00:16:46,080
And they remembered that they apparently hadn't pushed an update there in a long, long time.

301
00:16:46,650 --> 00:16:48,180
And so they finally deployed it.

302
00:16:48,310 --> 00:16:50,180
GPT Image 1.5 came out.

303
00:16:50,610 --> 00:16:53,000
It's a competitor to Nado Banano Pro.

304
00:16:54,290 --> 00:16:58,520
They claimed that it works super fast, almost faster than, like, Nado Banano and others.

305
00:16:58,710 --> 00:17:00,560
But in fact, it works slower.

306
00:17:00,830 --> 00:17:03,000
And on the benchmarks I have, it's the same.

307
00:17:04,850 --> 00:17:05,300
The quality...

308
00:17:07,410 --> 00:17:09,720
Damn, the quality is great, in my opinion.

309
00:17:10,030 --> 00:17:11,320
Does it generate text normally now?

310
00:17:12,230 --> 00:17:14,600
It has for a while, it was fine even in the first version.

311
00:17:14,770 --> 00:17:19,780
No, it was fine in the first version, but sometimes it would make a mistake in Russian, like, it would get a letter wrong every three words.

312
00:17:20,110 --> 00:17:21,300
Nado Banano doesn't do that at all.

313
00:17:21,350 --> 00:17:22,620
It writes everything correctly.

314
00:17:22,930 --> 00:17:26,300
It can, like, write a whole A4 page of text, yeah, if you ask it to write.

315
00:17:26,350 --> 00:17:31,240
Listen, I haven't tested it that deeply, but it generates really well.

316
00:17:35,350 --> 00:17:39,420
I also saw they have a new tab, Images, well, that's a minor thing.

317
00:17:39,610 --> 00:17:41,380
It used to be called Gallery, now it's Images.

318
00:17:41,580 --> 00:17:42,600
Yeah, like little pictures.

319
00:17:42,930 --> 00:17:46,120
I also saw an inside scoop today about all these models.

320
00:17:46,890 --> 00:17:56,740
It's unknown if it's an inside scoop or maybe fiction, but they say that GPT 5.2 and Images 1.5 are checkpoints for their next models.

321
00:17:57,210 --> 00:17:58,540
Very early checkpoints.

322
00:17:58,690 --> 00:18:03,940
So, it's possible that before the New Year, or in early February, OpenAI will delight us with new models again.

323
00:18:04,090 --> 00:18:05,020
Like, way better than these ones.

324
00:18:05,290 --> 00:18:07,300
Well, they have to pull away from Google somehow.

325
00:18:07,490 --> 00:18:07,790
Well, yeah.

326
00:18:07,810 --> 00:18:08,400
With all their might.

327
00:18:08,730 --> 00:18:12,720
They're saying the same thing about Google, that Gemini 3 Pro is also an early checkpoint.

328
00:18:15,550 --> 00:18:15,980
Alright.

329
00:18:17,690 --> 00:18:19,700
A little tab for GPT Images appeared.

330
00:18:19,750 --> 00:18:21,380
what else appeared in GPT?

331
00:18:21,950 --> 00:18:22,840
Pinned chats?

332
00:18:23,630 --> 00:18:24,660
Pinned chats?

333
00:18:24,930 --> 00:18:26,340
Well, thank you.

334
00:18:26,810 --> 00:18:27,180
That'll be useful.

335
00:18:30,150 --> 00:18:32,320
But the thing that appeared next,

336
00:18:33,410 --> 00:18:35,500
I don't know if you've had a chance to try it or not.

337
00:18:35,970 --> 00:18:38,180
I haven't had a chance to try it, I'll be honest with you.

338
00:18:38,290 --> 00:18:39,340
But the news is interesting.

339
00:18:39,690 --> 00:18:40,200
Well, tell me.

340
00:18:42,130 --> 00:18:44,800
Well, you tell me, and then I'll say what I've managed to do.

341
00:18:45,230 --> 00:18:48,140
In short, they added Photoshop to ChatGPT.

342
00:18:48,910 --> 00:18:54,640
I'll remind you that they recently added Nano Banana Pro to Photoshop, and then they added to ChatGPT.

343
00:18:54,870 --> 00:18:56,940
For some reason, they then added Photoshop to ChatGPT.

344
00:18:57,390 --> 00:18:58,780
Well, and also Acrobat.

345
00:18:59,430 --> 00:19:04,320
For those who don't know, it's a piece of software for editing PDFs.

346
00:19:04,370 --> 00:19:05,300
And Adobe Express.

347
00:19:06,930 --> 00:19:08,740
I honestly don't even know what Express is.

348
00:19:08,790 --> 00:19:10,400
It's for layout design.

349
00:19:11,210 --> 00:19:12,220
Like a quick tool.

350
00:19:13,230 --> 00:19:15,040
So, in short, now you can...

351
00:19:15,040 --> 00:19:18,920
Well, honestly, I looked at the pictures, I didn't understand how it works.

352
00:19:18,970 --> 00:19:24,120
Some kind of lightweight version opens up, like...

353
00:19:24,170 --> 00:19:26,300
It works, I'll tell you how.

354
00:19:26,870 --> 00:19:31,760
And I haven't had a chance to test it properly yet, but I've set it up...

355
00:19:31,810 --> 00:19:33,520
It's a connector, it's an application.

356
00:19:33,570 --> 00:19:38,320
So for it to work, you need to go into settings, and right there,

357
00:19:39,400 --> 00:19:40,920
I think, for...

358
00:19:43,230 --> 00:19:46,880
Acrobat and Express, I could be wrong, you have to log in to your Adobe account.

359
00:19:47,650 --> 00:19:49,560
It doesn't have to be a paid one, but you do have to log in.

360
00:19:49,790 --> 00:19:52,440
And with Photoshop, I think, you just click add, and bo...

361
00:19:52,440 --> 00:19:53,920
one of them works without a login.

362
00:19:54,070 --> 00:19:57,140
But overall, it's a direct integration with these services.

363
00:19:57,350 --> 00:19:59,620
And Adobe had a big article about it.

364
00:19:59,970 --> 00:20:02,380
And it works through some kind of layer they have.

365
00:20:02,430 --> 00:20:04,740
It works through some API SDK there.

366
00:20:05,210 --> 00:20:13,880
So basically, you include either Adobe Acrobat or Photoshop in the dialogue and say, here's a PSD file, please change the layers,

367
00:20:15,380 --> 00:20:20,480
delete one, add a second, a third, and in the output you get the same edited PSD file with new layers.

368
00:20:21,310 --> 00:20:28,280
What caused a storm of emotions on the internet was that you can now create and edit PDFs via ChatGPT.

369
00:20:28,410 --> 00:20:41,600
I just made this prompt right now, wrote, make me a PDF with the text Alexey, and the Adobe Acrobat app literally tells me, Create PDF, yes, Create PDF, and it creates a PDF for me, and it says "Hello, Alexey" there.

370
00:20:41,600 --> 00:20:49,460
And the Acrobat interface opens up, where you can edit the resulting file yourself.

371
00:20:50,250 --> 00:20:51,340
Oh, by the way, it did ask for a login.

372
00:20:52,000 --> 00:20:55,000
So it's a super tight integration with these products.

373
00:20:55,130 --> 00:21:03,540
And people went crazy over Acrobat, because they're saying, how is this possible, Adobe never sold this technology to anyone, they always charged money for it, and now ChatGPT 5...

374
00:21:03,540 --> 00:21:04,540
Yeah, yeah.

375
00:21:05,940 --> 00:21:06,620
There you go.

376
00:21:07,220 --> 00:21:08,920
In short, this is probably the most,

377
00:21:11,340 --> 00:21:16,860
well, not to say complex, but the most thorough integration from OpenAI in the last six months.

378
00:21:17,800 --> 00:21:19,060
A powerful integration.

379
00:21:22,410 --> 00:21:27,060
That's regarding ChatGPT. Overall, ChatGPT is flourishing thanks to integrations.

380
00:21:28,670 --> 00:21:35,260
If we go back to programming, because they had a pretty big update with Codex CLI.

381
00:21:35,710 --> 00:21:37,140
Let's go back there for a bit too.

382
00:21:37,840 --> 00:21:40,440
Codex CLI is their development agent.

383
00:21:41,530 --> 00:21:55,240
And it got an integration with the Linear tracking system, the ability to create custom slash commands, and they brought in GPT 5.2 Codex, as mentioned above.

384
00:21:56,450 --> 00:21:59,720
What's interesting is that Codex CLI now has support for skills.

385
00:21:59,880 --> 00:22:03,520
These are the things we've already told you about, which originally appeared in the Claude Code CLI.

386
00:22:03,570 --> 00:22:07,600
And now these skills are migrating to all other tools.

387
00:22:08,410 --> 00:22:10,840
You understand, it's basically becoming the standard.

388
00:22:11,150 --> 00:22:17,420
Ah yes, when we talk about Anthropic, we'll focus on this in more detail, because skills have really become the standard.

389
00:22:17,610 --> 00:22:19,280
Anthropic established a new standard.

390
00:22:19,440 --> 00:22:20,400
It's called Agent Skills.

391
00:22:20,680 --> 00:22:21,820
And now they're spreading everywhere.

392
00:22:23,470 --> 00:22:25,080
Anthropic, I see, they know how to do things.

393
00:22:25,390 --> 00:22:26,160
Yeah, it's totally insane.

394
00:22:26,550 --> 00:22:27,600
Standardizing all sorts of things.

395
00:22:31,010 --> 00:22:34,140
That's, I think, all the main stuff about Codex.

396
00:22:34,700 --> 00:22:40,140
Now let's dive into the more mundane news from OpenAI.

397
00:22:40,570 --> 00:22:41,940
This is also strange, of course.

398
00:22:42,140 --> 00:22:43,340
Nasty, mundane.

399
00:22:43,560 --> 00:22:49,420
Anyway, suddenly OpenAI managed to buddy up with Disney as well.

400
00:22:50,090 --> 00:22:56,280
Yeah, and they buddied up in a way that, I don't know, nobody expected, especially from Disney.

401
00:22:57,150 --> 00:23:04,680
Because, as we know, if you ask ChatGPT Five, or Midjourney, and others, to draw, for example, I don't know...

402
00:23:04,730 --> 00:23:05,400
Mickey Mouse.

403
00:23:05,990 --> 00:23:12,620
Well, Mickey Mouse or Stitch from Lilo & Stitch, right, it says, I can't, I can draw something similar.

404
00:23:13,050 --> 00:23:29,080
But now it will be able to draw them, because Disney, damn it, is giving away as many as 200 characters from Disney, Marvel, Pixar, and Star Wars to OpenAI for three years, with the ability to draw these characters in pictures.

405
00:23:42,734 --> 00:23:52,140
Mickey Mouse, Minnie Mouse, Lilo, Stitch, Ariel, Bayley, Belle from Beauty and the Beast, the Beast, Cinderella, Simba, Mufasa...

406
00:23:52,140 --> 00:23:54,140
And for what kind of merit?

407
00:23:54,280 --> 00:24:08,380
Characters from Frozen, Monsters, Inc., Toy Story, the movie Up, Moana, as well as Black Panther, Captain America, Deadpool, Groot, Iron Man...

408
00:24:08,380 --> 00:24:10,120
All of them, all of them, all of them, I'm starting to feel sick.

409
00:24:11,110 --> 00:24:12,380
Well, but...

410
00:24:12,380 --> 00:24:13,180
but...

411
00:24:13,180 --> 00:24:14,360
but what do they get in return, like?

412
00:24:14,780 --> 00:24:15,160
This is...

413
00:24:15,160 --> 00:24:18,560
Disney ate half the planet's animals with copyright.

414
00:24:18,890 --> 00:24:23,160
I don't know, but maybe, perhaps, as advertising...

415
00:24:23,160 --> 00:24:27,180
or maybe they'll advertise Disney+ in ChatGPT.

416
00:24:28,910 --> 00:24:32,360
However, the actors' voices and likenesses are not included, obviously.

417
00:24:32,920 --> 00:24:34,260
In addition, they will get...

418
00:24:34,260 --> 00:24:38,120
they will create a collection of generated videos on the Disney+ service.

419
00:24:38,670 --> 00:24:42,500
Okay, but that won't cover such a generous move.

420
00:24:42,660 --> 00:24:45,000
On top of that, Disney is also investing 1 billion in OpenAI.

421
00:24:45,420 --> 00:24:52,800
So it seems like a direct hint that Disney will be using OpenAI's technology for all it's worth in its new projects.

422
00:24:52,920 --> 00:24:53,660
I mean, like, what else for?

423
00:24:53,780 --> 00:24:56,410
I think, yeah, they'll probably get some exclusive computers there.

424
00:24:56,410 --> 00:24:56,960
Well, that's Sora.

425
00:24:57,580 --> 00:24:58,300
What else?

426
00:24:58,450 --> 00:24:58,940
Do they need Sora?

427
00:24:59,130 --> 00:25:04,080
Well, Sora, or some kind of new Sora that nobody else has yet.

428
00:25:05,250 --> 00:25:08,580
Plus, they'll also get shares, so they want to make money too.

429
00:25:08,980 --> 00:25:19,040
It's strange, I don't know, people who want to make money off OpenAI seem strange to me, since it's still unknown how it will make money, but it's interesting.

430
00:25:19,380 --> 00:25:30,820
Or maybe this is how Disney, through its shares, will somehow influence OpenAI, you know, implicitly restrict their models or maintain control over modern tools, preventing other companies from using them.

431
00:25:30,870 --> 00:25:37,680
Maybe there will be some clauses like that, and not an agreement with other major video vendors.

432
00:25:38,380 --> 00:25:38,800
Well, we'll see.

433
00:25:39,180 --> 00:25:40,200
In general, pay attention...

434
00:25:40,200 --> 00:25:43,140
Also, by the way, Yoda and the Mandalorian, to finish the list.

435
00:25:45,230 --> 00:25:53,440
Pay attention, OpenAI in one week made deals in two weeks with companies you'd never in your life believe it would happen with: Disney and Adobe.

436
00:25:55,090 --> 00:25:55,520
Yeah.

437
00:25:55,520 --> 00:25:56,820
And Amazon too.

438
00:25:57,290 --> 00:25:58,100
And Amazon too.

439
00:25:58,100 --> 00:25:59,180
Overall, we could have believed it.

440
00:26:00,630 --> 00:26:01,220
Alrighty then.

441
00:26:01,220 --> 00:26:08,420
The last thing on OpenAI for today is also more for developers, but it's also about the overall direction of the service's development.

442
00:26:08,550 --> 00:26:11,640
Actually, OpenAI has raised its margin significantly.

443
00:26:12,450 --> 00:26:15,640
I saw some study there, and Slim ChatGPT last year.

444
00:26:17,110 --> 00:26:18,340
Like, there...

445
00:26:18,340 --> 00:26:19,660
I'm afraid to get the percentages wrong.

446
00:26:19,790 --> 00:26:24,440
It was something like 70% of expenses on user queries.

447
00:26:24,490 --> 00:26:26,740
They weren't covered by user money.

448
00:26:26,790 --> 00:26:29,020
Now that's been reduced to almost 30%.

449
00:26:29,670 --> 00:26:33,520
So they're really starting to balance their books properly.

450
00:26:34,170 --> 00:26:34,900
On one hand.

451
00:26:35,010 --> 00:26:42,180
On the other hand, this means that API requests won't grow that much, even when they start to break even.

452
00:26:42,350 --> 00:26:43,660
So, maybe by about half.

453
00:26:45,110 --> 00:26:50,300
Well, the news is that OpenAI has started accepting applications for publishing apps in the GPT section.

454
00:26:50,610 --> 00:26:53,300
And they are launching an application catalog inside the chatbot.

455
00:26:53,710 --> 00:26:57,740
And here, I guess, like everyone else, I have a question: why did we make GPTs?

456
00:26:58,170 --> 00:26:59,700
If this is very similar.

457
00:26:59,920 --> 00:27:00,560
And where are these GPTs?

458
00:27:00,690 --> 00:27:02,440
I mean, I still use GPTs.

459
00:27:03,870 --> 00:27:06,620
And I honestly don't know, if they get deleted.

460
00:27:06,850 --> 00:27:07,900
Well, it would be unpleasant.

461
00:27:08,190 --> 00:27:10,120
Or maybe they'll somehow be converted into applications.

462
00:27:10,270 --> 00:27:12,400
Because applications in the app, that's a different thing.

463
00:27:12,510 --> 00:27:17,580
It's something that is built directly by developers, right by hand.

464
00:27:17,950 --> 00:27:21,600
Through an SDK, like the Adobe integration is done through an application.

465
00:27:21,750 --> 00:27:24,780
Meaning, you can build a whole complex system, a whole piece of software.

466
00:27:24,830 --> 00:27:28,700
And then it will be embedded from the Marketplace, and you can add it to the chat.

467
00:27:28,830 --> 00:27:29,540
It's not just a GPT.

468
00:27:30,160 --> 00:27:39,480
They also said that the first open applications will be available next year, you can grab them for yourself.

469
00:27:39,880 --> 00:27:44,160
And, of course, a Marketplace should appear there.

470
00:27:44,760 --> 00:27:45,380
Well, what else for?

471
00:27:46,120 --> 00:27:47,560
Well, who would've doubted it.

472
00:27:47,700 --> 00:27:52,620
They also talked about a Marketplace for GPTs, remember, when they said there would be one, but it never took off.

473
00:27:53,610 --> 00:27:54,860
No, well, for some reason...

474
00:27:54,860 --> 00:27:56,220
There is no marketplace in GPTs.

475
00:27:56,320 --> 00:27:57,540
There is a marketplace in GPTs.

476
00:27:57,780 --> 00:27:57,880
What do you mean?

477
00:27:58,310 --> 00:28:00,640
Well, I mean, there's a big catalog there.

478
00:28:01,110 --> 00:28:03,520
No, a marketplace is when you get paid for usage.

479
00:28:03,890 --> 00:28:04,860
Ah, you mean in that sense?

480
00:28:04,910 --> 00:28:05,520
Well, yeah, basically.

481
00:28:05,760 --> 00:28:07,320
You can earn money, that's why it's a market.

482
00:28:07,450 --> 00:28:17,720
You're selling. Those are catalogs.

483
00:28:19,580 --> 00:28:21,060
Well, we'll see, we'll see.

484
00:28:22,870 --> 00:28:27,560
If it helps them avoid bombarding us with contextual ads, which I think will appear soon, then...

485
00:28:27,560 --> 00:28:31,380
Yeah, it's 100% going to appear anyway.

486
00:28:31,900 --> 00:28:32,180
Yeah.

487
00:28:32,560 --> 00:28:34,980
Alright, let's move on to the next big fish.

488
00:28:35,260 --> 00:28:38,360
Yes, our next big fish is, of course, Google.

489
00:28:38,980 --> 00:28:41,760
Google also has some news piled up.

490
00:28:42,100 --> 00:28:44,620
So, Google released Gemini 3 Flash.

491
00:28:46,100 --> 00:28:47,440
Not Pro, but Flash Pro.

492
00:28:47,660 --> 00:28:49,200
We talked about it in the last episode.

493
00:28:49,620 --> 00:28:51,880
So it became the default model.

494
00:28:52,670 --> 00:28:55,280
Both in Gemini and in Search.

495
00:28:55,940 --> 00:28:57,200
And it's free.

496
00:28:57,660 --> 00:29:00,220
I don't remember if there are any limits on it, but I think even...

497
00:29:00,220 --> 00:29:02,500
No, it's only free in Gemini for now.

498
00:29:02,620 --> 00:29:03,310
I mean, yes.

499
00:29:03,820 --> 00:29:05,360
Meaning, through the API it's paid.

500
00:29:05,960 --> 00:29:08,500
No, well, obviously it's paid through the API.

501
00:29:08,660 --> 00:29:09,040
Yeah, yeah, yeah.

502
00:29:09,160 --> 00:29:09,420
There.

503
00:29:10,420 --> 00:29:15,180
Well, and it performs quite well on benchmarks, almost at the level of 2.5 Pro.

504
00:29:15,420 --> 00:29:16,820
It's better than 2.5 Pro.

505
00:29:17,120 --> 00:29:19,120
And it's faster and cheaper.

506
00:29:19,170 --> 00:29:21,180
It's better, it's cheaper...

507
00:29:21,180 --> 00:29:24,360
it's 4 times cheaper than Gemini 3 Pro.

508
00:29:24,570 --> 00:29:25,460
Well, that's logical.

509
00:29:25,580 --> 00:29:30,240
Flash is, like, the junior version in the model lineup.

510
00:29:30,600 --> 00:29:35,220
And it performs better than Gemini 2.5 Pro, which was the previous flagship.

511
00:29:35,520 --> 00:29:39,860
And on benchmarks, I looked, it even surpasses 3 Pro on some benchmarks.

512
00:29:40,260 --> 00:29:42,420
Some really obscure benchmarks, but still.

513
00:29:44,040 --> 00:29:44,560
Like, really...

514
00:29:44,560 --> 00:29:59,960
And all this combined also gives reason for people who spread rumors to say that Gemini 3 Pro is some early checkpoint, an early slice of a more powerful model, and GPT-3 Flash is also a slice of it, but just not the powerful one.

515
00:30:01,130 --> 00:30:01,900
We'll see.

516
00:30:02,970 --> 00:30:11,100
Well, in general, Google is starting to engage in a bit of clownery towards the end of the year, because their next news is slightly about clownery.

517
00:30:11,830 --> 00:30:13,160
I hope it doesn't slide into that.

518
00:30:13,560 --> 00:30:16,060
So, they rolled out a new tool called Conductor.

519
00:30:16,920 --> 00:30:18,160
Hit the brakes.

520
00:30:18,210 --> 00:30:19,680
Little one, dear.

521
00:30:19,900 --> 00:30:21,320
With a last hello, young one here.

522
00:30:21,880 --> 00:30:37,020
So, Conductor is an extension for the Gemini CLI, for the Gemini terminal interface, which allows working with the Gemini CLI in a Spec-Driven Development style.

523
00:30:37,220 --> 00:30:42,380
Meaning, you don't just write prompts and generate code, but first, you create a specialization (specification)

524
00:30:42,380 --> 00:30:46,220
with a plan, an architectural design, with a bunch of different descriptions.

525
00:30:46,270 --> 00:30:52,640
And then, when you have these Specs ready, an agent goes through them and does tasks, completes tasks, and records them.

526
00:30:53,600 --> 00:30:58,680
This is something that has become quite established in AI development over the last few months, it's called Spec-Driven Development.

527
00:30:59,100 --> 00:31:04,700
It's natively supported in tools like Qoder, in Kiro IDE, which is an IDE from VS.

528
00:31:05,320 --> 00:31:12,060
There are open-source implementations, like GitHub spec-kit, BMAD, a whole bunch of different ones.

529
00:31:12,320 --> 00:31:14,280
It's a whole established field already.

530
00:31:14,330 --> 00:31:18,960
And Spec-Driven Development also existed in programming before, it just wasn't talked about much.

531
00:31:19,120 --> 00:31:25,280
It was a bit less popular than TDD, and TDD, as we know, is also not popular enough, unfortunately.

532
00:31:26,730 --> 00:31:33,940
And so Google comes along and says, here, we're giving you Conductor, it creates Specs through the Gemini CLI.

533
00:31:34,110 --> 00:31:37,240
But please, dears, don't call it Spec-Driven Development.

534
00:31:37,670 --> 00:31:40,200
It's called Context-Driven Development.

535
00:31:40,720 --> 00:31:41,800
And I'm like, okay,

536
00:31:43,040 --> 00:31:44,420
uh-huh, what's the difference?

537
00:31:44,560 --> 00:31:46,980
I start reading the description, and they write right in the description.

538
00:31:47,690 --> 00:31:51,100
Conductor creates Specs based on which it closes tasks.

539
00:31:52,530 --> 00:31:59,500
And in the end, no matter how I tried, I discussed it with guys in a chat, ran it through ChatGPT, well, it just seems like newspeak.

540
00:31:59,770 --> 00:32:01,600
Google is inventing terms where they already exist.

541
00:32:01,890 --> 00:32:13,240
They kind of emphasize that Spec-Driven Development is actually a term from past programming, it confuses people, but you are working with models, and there's context there, so it's Context-Driven Development.

542
00:32:13,290 --> 00:32:20,420
But they didn't consider that, like, Context-Driven Development, for example, gets confused with Context Engineering.

543
00:32:21,150 --> 00:32:24,560
So, people who hear Context Engineering, Context-Driven Development for the first time.

544
00:32:24,830 --> 00:32:29,480
It seems to me that for them, it's the same field of incomprehensible things, something from development.

545
00:32:30,310 --> 00:32:32,000
More or less, yeah, I think, more or less.

546
00:32:32,050 --> 00:32:37,560
So, here's a good tool, if you use the Gemini CLI, give it a try.

547
00:32:38,290 --> 00:32:40,720
In principle, there are alternatives to it, but it's not bad.

548
00:32:40,770 --> 00:32:44,860
But the fact that they're inventing newspeak, that seems a bit weird to me.

549
00:32:47,370 --> 00:32:48,580
So, that's the news.

550
00:32:48,970 --> 00:32:54,720
Well, write in the comments if you know what the difference is between SDD and CDD.

551
00:32:55,210 --> 00:32:57,440
Maybe there are some differences after all?

552
00:32:58,210 --> 00:32:58,680
The letter.

553
00:32:59,970 --> 00:33:00,440
Alright.

554
00:33:00,710 --> 00:33:10,220
And next up, we have news about Google, because suddenly Google remembered that it has Google Translate, and that LLMs haven't really made it there yet.

555
00:33:10,970 --> 00:33:13,760
And Google is integrating Gemini into Translate.

556
00:33:14,210 --> 00:33:15,900
And I, for one, still don't get it.

557
00:33:16,590 --> 00:33:20,000
That is, it's written that it's supposedly for translating all sorts of slang and idioms.

558
00:33:20,890 --> 00:33:24,540
So, Gemini probably won't be translating the entire text for now.

559
00:33:25,290 --> 00:33:27,800
But, nevertheless, this thing is being rolled out.

560
00:33:28,210 --> 00:33:32,940
They didn't say in the article, in the videos, they didn't say how this will happen.

561
00:33:32,990 --> 00:33:34,580
Meaning it will be implicit for the user.

562
00:33:34,790 --> 00:33:40,560
We won't know what's under the hoodâ€”algorithms, some static ones, static or models.

563
00:33:42,650 --> 00:33:44,100
But they are rolling out this functionality.

564
00:33:44,430 --> 00:33:46,760
Well, in short, yeah, tell me, and then I'll give a little feedback.

565
00:33:47,990 --> 00:33:57,860
Well, and another story is that Google remembered that Apple released the Live Translate feature, translations through headphones.

566
00:33:58,150 --> 00:34:02,100
And they say, we'll do the same thing, but through any headphones at all.

567
00:34:02,300 --> 00:34:02,900
Not just through...

568
00:34:02,900 --> 00:34:05,440
even through AirPods, and through anything at all.

569
00:34:06,050 --> 00:34:11,360
Just open the Google Translate app, and a live translation like Apple's will work there.

570
00:34:12,510 --> 00:34:15,600
At the same time, on-the-fly speech-to-speech already exists in many places.

571
00:34:16,890 --> 00:34:19,780
Yeah, in fact, Google just needed to add an extra button.

572
00:34:19,860 --> 00:34:20,400
They already have...

573
00:34:20,400 --> 00:34:24,380
they already had this mode where you press to speak, you speak, it translates, and voices it as text.

574
00:34:24,710 --> 00:34:25,090
Well, yes.

575
00:34:25,410 --> 00:34:30,200
They are rolling out all this functionality for now only in America and India.

576
00:34:30,350 --> 00:34:32,580
By the way, it's funny that Google has started including India everywhere.

577
00:34:32,730 --> 00:34:34,220
Apparently, that's their target audience now.

578
00:34:35,930 --> 00:34:40,180
This functionality for Translator will work with 20 language pairs.

579
00:34:40,670 --> 00:34:44,160
Correspondingly, English-something and Hindi-something.

580
00:34:44,410 --> 00:34:45,720
Because India has many dialects.

581
00:34:46,270 --> 00:34:49,620
With speech-to-speech, it will work with 70 pairs, which is strange.

582
00:34:49,740 --> 00:34:53,520
It seems as though speech-to-speech should use Translate under the hood,

583
00:34:54,600 --> 00:34:55,620
it seems like it should be 70.

584
00:34:56,040 --> 00:34:57,080
Well, that's a bit strange.

585
00:34:57,560 --> 00:34:59,380
That's why they haven't rolled out anything for us.

586
00:34:59,500 --> 00:35:04,860
Honestly, just today I was driving to drop off some documents at the hospital, Polish ones.

587
00:35:05,060 --> 00:35:17,920
Something made me go into Translate, but usually, if I'm going somewhere, I usually, before talking to a person, ask ChatGPT to translate for me from Russian...

588
00:35:17,920 --> 00:35:18,880
from Russian to Polish.

589
00:35:19,010 --> 00:35:20,740
I read it and then say it with my mouth.

590
00:35:21,060 --> 00:35:22,540
Because for now, I'm not very good at forming

591
00:35:22,540 --> 00:35:23,040
sentences.

592
00:35:23,140 --> 00:35:25,600
Then the devil made me go to the translator.

593
00:35:25,960 --> 00:35:27,820
I have a really crappy level of Polish.

594
00:35:27,880 --> 00:35:28,760
It's not even A1.

595
00:35:29,040 --> 00:35:31,800
That is, I don't know the rules well, but I can read and understand the meaning.

596
00:35:32,100 --> 00:35:34,680
It's like an A1 transitioning to A2, but without the rules.

597
00:35:35,220 --> 00:35:38,760
And even I saw that ChatGPT generated complete bullshit for me.

598
00:35:38,940 --> 00:35:41,440
More accurately, Translator generated complete crap for me.

599
00:35:41,580 --> 00:35:44,700
It literally wrote like half of what I didn't need.

600
00:35:44,750 --> 00:35:54,260
I asked it to write, I needed to tell the doctor, please accept my contract and give it to the head physician.

601
00:35:55,720 --> 00:35:59,700
It literally translated something like,

602
00:36:00,740 --> 00:36:07,500
talk to me, gossip with me a bit and tell the head physician about this gossip, something like that.

603
00:36:08,170 --> 00:36:08,840
Oh, my god.

604
00:36:09,000 --> 00:36:10,500
And this is when on the level of understanding...

605
00:36:10,500 --> 00:36:12,420
Well, searching for words, they are quite similar by their roots.

606
00:36:12,960 --> 00:36:15,440
I just look at it, and I understand that it's some kind of nonsense.

607
00:36:15,760 --> 00:36:16,680
And they have this...

608
00:36:16,680 --> 00:36:24,600
I didn't understand, is the static translation in Google that crappy nowadays, or did they experimentally roll out some crappy models.

609
00:36:24,600 --> 00:36:26,580
I think the static translation there is terrible.

610
00:36:26,740 --> 00:36:28,080
It's always been like that, sort of.

611
00:36:28,380 --> 00:36:29,040
Well, it's strange.

612
00:36:29,160 --> 00:36:32,560
Yet they have Gemini, which translates great.

613
00:36:33,240 --> 00:36:34,260
Why do they need Translator?

614
00:36:34,260 --> 00:36:36,100
They should have just given Gemini to everyone for free already.

615
00:36:36,150 --> 00:36:37,480
It's time to implement it, yes.

616
00:36:39,040 --> 00:36:40,060
That's a fact.

617
00:36:40,920 --> 00:36:41,480
Alright.

618
00:36:41,780 --> 00:36:43,700
There were two more announcements from Google.

619
00:36:44,620 --> 00:36:47,920
These are just announcements, there's nothing solid there.

620
00:36:48,140 --> 00:36:50,200
You can get on the waitlists for these announcements.

621
00:36:50,280 --> 00:36:51,400
Two new tools.

622
00:36:51,460 --> 00:36:52,900
The first is called Google CC.

623
00:36:53,900 --> 00:36:56,000
And for those who use email, Carbon Copy.

624
00:36:56,220 --> 00:36:58,880
This, accordingly, is a summarizer for Google mail.

625
00:36:59,340 --> 00:37:01,120
How it will work, who the hell knows.

626
00:37:01,300 --> 00:37:06,130
Well, some neural networks under the hood will summarize your emails, and give it to you in a convenient format.

627
00:37:06,100 --> 00:37:08,440
report it in a format, probably, right in the Google interface.

628
00:37:08,740 --> 00:37:13,000
You can get on the waitlist to have this feature rolled out to you among the first.

629
00:37:13,320 --> 00:37:18,300
And the second product they announced is called Google Disco.

630
00:37:18,740 --> 00:37:20,340
And surprise, surprise.

631
00:37:20,500 --> 00:37:21,340
It's a browser.

632
00:37:22,410 --> 00:37:25,540
And it's not Chrome, mind you, but Google Disco.

633
00:37:25,940 --> 00:37:27,020
It's a separate browser.

634
00:37:27,820 --> 00:37:30,700
Well, most likely it will be Chrome, but it looks a little strange.

635
00:37:31,820 --> 00:37:34,400
They're, like, saying that it's an agent-native browser.

636
00:37:34,570 --> 00:37:36,080
That is, it will be focused on...

637
00:37:36,150 --> 00:37:38,700
proactive actions from the user.

638
00:37:38,890 --> 00:37:43,840
There, literally in the videos, it's shown that you have a prompt, a window for entering a prompt.

639
00:37:43,960 --> 00:37:46,360
You enter a prompt, like, again, "buy tickets."

640
00:37:46,500 --> 00:37:51,600
And it goes off to a bunch of sites to google tickets, compares prices with these sites.

641
00:37:51,740 --> 00:37:52,860
Then you into the prompt...

642
00:37:52,860 --> 00:37:56,360
You write, like, I liked these and these tickets.

643
00:37:57,070 --> 00:38:02,920
I still have some money left, I want to plan my trip, can you help, and all this literally in one window.

644
00:38:03,490 --> 00:38:08,180
And what's cool, they announced a new feature there called GenTabs.

645
00:38:09,010 --> 00:38:25,680
That is, literally based on your history of agentic interaction, it, in this browser, will see that you bought some tickets to some country, you asked it to create a route, it takes all the tabs that were used in this agentic workflow of interaction.

646
00:38:26,650 --> 00:38:31,060
And based on these tabs, you can create a new tab, generate it from scratch.

647
00:38:31,290 --> 00:38:34,200
In which there will be an application made especially for you.

648
00:38:34,950 --> 00:38:39,980
Yes, it will in fact generate a one-time application exclusively for a specific use case.

649
00:38:40,190 --> 00:38:51,000
Yes, in this case, for example, it can generate an application where you will be shown a map, and points of your journey will be shown, and how much a flight from one point to another costs, and there will be a "buy" button.

650
00:38:52,180 --> 00:38:53,880
A similar thing already works in Gemini.

651
00:38:53,980 --> 00:38:54,900
We talked about it.

652
00:38:55,160 --> 00:38:56,340
Though, I forgot what it's called.

653
00:38:56,510 --> 00:38:57,920
Either Gemini Previews, or something.

654
00:38:58,120 --> 00:39:01,940
They also generate a whole interface for you there based on a request.

655
00:39:01,960 --> 00:39:02,960
Yes, yes, there is such a thing.

656
00:39:03,080 --> 00:39:05,680
And now they want to integrate this directly into the browser.

657
00:39:06,160 --> 00:39:08,820
You can get on the waitlist, but it's a complicated waitlist.

658
00:39:09,010 --> 00:39:12,940
There you have to write who you are, what you do for a living, why you want to be on this waitlist.

659
00:39:13,040 --> 00:39:15,440
In short, I got tired of writing while I was signing up.

660
00:39:17,310 --> 00:39:18,460
So, that's the thing.

661
00:39:18,680 --> 00:39:20,500
That's probably all with Google.

662
00:39:21,400 --> 00:39:27,060
That's all with Google, but next up is that very Anthropic we've already talked a little about today.

663
00:39:27,820 --> 00:39:28,880
And, so, the news.

664
00:39:29,050 --> 00:39:32,940
I'm honestly surprised, because it seemed to me that this happened a very long time ago.

665
00:39:33,600 --> 00:39:35,060
But, apparently, it was recent.

666
00:39:35,750 --> 00:39:35,980
Recently.

667
00:39:36,030 --> 00:39:40,640
Anthropic handed over MCP, well, Modal Context Protocol, to be managed by the Linux Foundation.

668
00:39:42,970 --> 00:39:43,400
Yes.

669
00:39:44,030 --> 00:39:45,740
It's not that it happened recently.

670
00:39:46,650 --> 00:39:50,180
It was supposed to happen, and everyone was waiting for it to happen, in fact.

671
00:39:50,790 --> 00:39:54,200
There just wasn't an organization that Anthropic could safely hand it over to.

672
00:39:54,370 --> 00:39:56,980
The Linux Foundation, in fact, is not suitable for AI initiatives.

673
00:39:57,030 --> 00:40:07,540
It's an organization that, like, runs open-source projects, but it has a rather neutral, I would say, cool attitude towards AI.

674
00:40:08,170 --> 00:40:13,700
In particular, Linus Torvalds, he has been speaking normally about AI lately, but still, like, the Linux Foundation, it...

675
00:40:15,190 --> 00:40:17,000
In short, it's all complicated with branding.

676
00:40:17,050 --> 00:40:19,660
So they created a whole new foundation.

677
00:40:20,010 --> 00:40:21,900
It's called the Agentic AI Foundation.

678
00:40:22,850 --> 00:40:25,020
Its founders were three companies.

679
00:40:25,850 --> 00:40:27,800
That's Anthropic, OpenAI, and Block.

680
00:40:28,470 --> 00:40:29,800
Anthropic, OpenAI.

681
00:40:30,250 --> 00:40:30,840
For a minute.

682
00:40:31,650 --> 00:40:38,480
And this foundation, despite the fact that the creators are these companies, the fourth founder is the Linux Foundation, but it is also the managing one.

683
00:40:38,710 --> 00:40:40,300
That is, they will manage this foundation.

684
00:40:40,850 --> 00:40:42,380
They did it all very nicely.

685
00:40:42,430 --> 00:40:49,860
And the foundation was also joined as co-founders by Google, Amazon, AWS, Cloudflare, and suddenly, Bloomberg.

686
00:40:51,050 --> 00:41:01,040
This foundation will be involved in the development of open-source initiatives that have a worldwide impact in AI technologies.

687
00:41:01,850 --> 00:41:04,820
Anthropic contributed the Model Context Protocol there.

688
00:41:05,230 --> 00:41:06,920
And that's the biggest impact so far.

689
00:41:07,790 --> 00:41:10,940
OpenAI contributed the Agents.md specification there.

690
00:41:10,990 --> 00:41:19,780
This is a standard in the development world for describing instructions by which agents in applications should operate.

691
00:41:21,050 --> 00:41:28,040
We also waited a long time for Agents.md to become a standard, because every tool had its own files for agent settings.

692
00:41:28,550 --> 00:41:30,420
Gemini.md, Claude.md, and blah-blah-blah.

693
00:41:30,610 --> 00:41:33,200
Now the standard has appeared, and it has become part of the Agentic AI Foundation.

694
00:41:33,250 --> 00:41:46,980
How the company Block got there is not entirely clear, it's not a household name for us, but it's actually a Research Laboratory company that, at one time, rolled out one of the first properly working agents for writing code.

695
00:41:47,230 --> 00:41:48,160
It's called Goose.

696
00:41:49,010 --> 00:41:50,540
We probably even talked about it.

697
00:41:50,650 --> 00:41:54,980
It actually has quite a few stars on GitHub, 20 thousand, and it's still being developed.

698
00:41:55,030 --> 00:41:58,460
And this agent also became the property of the Agentic AI Foundation.

699
00:41:58,910 --> 00:42:03,980
You could say it's a sort of reference agent, a reference Claude Code, which will be supported by the Linux Foundation.

700
00:42:04,790 --> 00:42:09,100
And now the community expects that Anthropic will also contribute Agent Skills there.

701
00:42:10,690 --> 00:42:11,740
So that's the news.

702
00:42:15,280 --> 00:42:15,850
In short.

703
00:42:18,120 --> 00:42:20,910
At the same time, yes, at the same time if Agent Skills.

704
00:42:21,060 --> 00:42:22,750
Why will Agent Skills be contributed there as well?

705
00:42:22,920 --> 00:42:25,970
Because with Agent Skills, everything is developing very rapidly.

706
00:42:26,020 --> 00:42:37,750
This technology turned out to be so cool, you know, like, loading folders with descriptions of how agents should work, that Microsoft has already pulled these Agent Skills over to their side.

707
00:42:37,840 --> 00:42:40,690
They brought Agent Skills to Visual Studio Code.

708
00:42:40,900 --> 00:42:43,330
They brought it to Copilot GitHub.

709
00:42:43,800 --> 00:42:45,330
They brought it to GitHub Copilot CLI.

710
00:42:48,220 --> 00:42:52,030
And Codex, meaning OpenAI, has also adopted Agent Skills.

711
00:42:52,400 --> 00:42:55,810
They also have agents there now, and it all works through Agent Skills.

712
00:42:56,020 --> 00:42:58,710
Therefore, I think Agent Skills will soon be added to the AI Foundation.

713
00:43:01,280 --> 00:43:01,750
Probably...

714
00:43:01,800 --> 00:43:03,470
Probably, this is all from Anthropic.

715
00:43:03,940 --> 00:43:06,650
Literally one piece of news, but it's a really big one.

716
00:43:07,680 --> 00:43:08,390
It's two.

717
00:43:08,680 --> 00:43:09,710
Yes, and a very important one.

718
00:43:11,200 --> 00:43:13,630
So, next up we have xAI.

719
00:43:15,000 --> 00:43:17,250
Anyway, on a funny note.

720
00:43:17,300 --> 00:43:19,790
xAI had a hackathon.

721
00:43:20,360 --> 00:43:25,350
Well, a hackathon, obviously, for using their AI solutions.

722
00:43:25,760 --> 00:43:28,950
And the winning project is called Halftime.

723
00:43:29,860 --> 00:43:31,790
So, what is it?

724
00:43:32,060 --> 00:43:39,510
It's an AI-powered solution for generating personalized ads in video content.

725
00:43:39,560 --> 00:43:41,110
So, how does it work?

726
00:43:41,400 --> 00:43:43,490
You're watching Breaking Bad.

727
00:43:45,300 --> 00:43:52,150
And then suddenly Walter White, aka Heisenberg, spoiler alert for those who haven't watched, takes...

728
00:43:52,200 --> 00:43:57,070
You first say the spoiler, and then you say it was a spoiler.

729
00:43:57,500 --> 00:43:58,220
Yeah, right.

730
00:43:58,480 --> 00:43:59,130
Should be the other way around.

731
00:44:00,040 --> 00:44:07,050
He takes a Chupa Chups, starts sucking on it furiously and tells you to the camera that he's never tasted a better Chupa Chups in his life.

732
00:44:07,100 --> 00:44:09,410
And that's the kind of ad it is.

733
00:44:09,580 --> 00:44:11,970
Then he spits out the Chupa Chups and continues cooking meth.

734
00:44:12,260 --> 00:44:14,730
So that's the winner of the hackathon.

735
00:44:16,200 --> 00:44:16,750
Yes.

736
00:44:17,780 --> 00:44:19,630
Well, damn, what can I say?

737
00:44:19,820 --> 00:44:23,450
It doesn't sound scary, or weird, what is it...

738
00:44:23,500 --> 00:44:27,510
It sounds scummy, just like contextual advertising in ChatGPT, that's how this thing sounds.

739
00:44:27,940 --> 00:44:30,190
But overall, this was bound to happen.

740
00:44:30,440 --> 00:44:33,690
Ads in content are coming soon.

741
00:44:33,920 --> 00:44:35,510
In context and in content.

742
00:44:35,560 --> 00:44:40,370
Listen, but on the other hand, to what extent is this...

743
00:44:40,370 --> 00:44:44,070
why did this stir up the public at all?

744
00:44:44,230 --> 00:44:45,930
Well, product placement has always been around.

745
00:44:46,170 --> 00:44:47,470
It was just static.

746
00:44:47,930 --> 00:44:51,510
Well, now they'll show you Coca-Cola in America, and Belocola in Belarus.

747
00:44:51,620 --> 00:44:51,910
So what?

748
00:44:53,300 --> 00:45:00,710
People don't really think about the fact that if you see a BMW car in a movie, it's not because...

749
00:45:00,710 --> 00:45:03,510
well, I mean, there's an agreement with the BMW brand at that moment.

750
00:45:03,590 --> 00:45:03,840
Well, yeah.

751
00:45:04,770 --> 00:45:07,320
And it was chosen to be put there.

752
00:45:07,270 --> 00:45:13,470
Well, it's just, you know, sometimes product placement is totally out of place, right, when they're shining the brand right in your eyes with a close-up...

753
00:45:13,470 --> 00:45:14,340
Well, yeah, it happens.

754
00:45:14,340 --> 00:45:15,910
They love to do that in Russian movies.

755
00:45:16,290 --> 00:45:19,170
I remember back when I still watched those movies.

756
00:45:19,910 --> 00:45:20,670
Anyway.

757
00:45:20,720 --> 00:45:22,150
That's exactly how it can be.

758
00:45:22,610 --> 00:45:25,530
But sometimes it's very much in place.

759
00:45:25,660 --> 00:45:30,650
Like, for example, James Bond driving an Aston Martin, that's straight-up paid advertising, and nobody is...

760
00:45:30,700 --> 00:45:39,910
You see, back in the day, at the dawn of the cinema era, when we were like a shitty 5-10 years old, product placement already clearly existed, it has always existed.

761
00:45:40,450 --> 00:45:42,770
But somehow, nobody talked about it back then.

762
00:45:42,940 --> 00:45:50,530
People started talking about it when cases of this really intrusive product placement appeared, when they're shoving it right in your face, and everyone started thinking...

763
00:45:50,580 --> 00:45:52,390
So what, is this how companies make money?

764
00:45:52,590 --> 00:45:54,510
And it became tacky.

765
00:45:54,770 --> 00:45:57,150
A lot of people started doing tacky Product Placement.

766
00:45:57,800 --> 00:46:07,650
So there's no real news here, although from a slightly psychological point of view, you could...

767
00:46:07,700 --> 00:46:11,990
Tell me if I'm stretching the truth here or not, but this opens up...

768
00:46:12,580 --> 00:46:17,310
This AI Product Placement opens the door to gaslighting.

769
00:46:19,240 --> 00:46:26,970
Meaning, you watched a movie, you saw one ad there, your friend watched it in another country, and he saw a different ad.

770
00:46:28,020 --> 00:46:31,650
And he'll tell you that he saw one ad, and you'll say, no, the other one.

771
00:46:31,840 --> 00:46:33,810
And you don't know that this technology exists.

772
00:46:34,060 --> 00:46:40,470
And in principle, some guys who, you know, can show their dominance through such arguments, they can do it on purpose, gaslighting.

773
00:46:40,600 --> 00:46:46,430
But this is the only downside I found in this technology that is obvious compared to what we have now.

774
00:46:46,480 --> 00:46:49,450
Well, you could say, yeah, you're a fool, it was different.

775
00:46:50,220 --> 00:46:54,530
I'll tell you, let me make a prediction for you on how this will work in the future.

776
00:46:55,380 --> 00:46:57,910
Have you ever set up Google ads?

777
00:47:01,040 --> 00:47:01,970
I don't remember.

778
00:47:02,260 --> 00:47:02,910
Facebook ads?

779
00:47:03,320 --> 00:47:03,910
Yes, yes, yes.

780
00:47:04,330 --> 00:47:05,470
Targeting, yeah, I've done that.

781
00:47:05,800 --> 00:47:07,690
This is going to work just like targeting.

782
00:47:07,920 --> 00:47:21,870
When on a hypothetical Netflix, you log in, create an account, you write that you have, for example, coffee, right, Polish coffee, and you write target audience Poland, product coffee, here are its pictures from all sides, please.

783
00:47:22,480 --> 00:47:26,530
and Netflix will insert it in automatic mode.

784
00:47:26,750 --> 00:47:30,090
You've certainly delved into the twilight zone here.

785
00:47:31,000 --> 00:47:32,850
Just imagine, a Marvel movie is being made.

786
00:47:33,170 --> 00:47:34,610
A multi-million, multi-billion dollar movie.

787
00:47:35,400 --> 00:47:42,910
And Marvel comes to BMW and says, we're making a movie, it will bring in like 300 billion,

788
00:47:45,740 --> 00:47:46,790
To hell with the billions.

789
00:47:47,310 --> 00:47:49,130
It will get 10 billion views.

790
00:47:49,270 --> 00:47:52,990
And we will integrate your BMW into 20 minutes of the film.

791
00:47:54,030 --> 00:47:55,690
And that sells well.

792
00:47:55,930 --> 00:47:57,030
These are clear metrics.

793
00:47:57,130 --> 00:47:58,950
BMW goes, evaluates it, and gives them the money.

794
00:47:59,170 --> 00:48:02,610
So how will Product Placement work in that case?

795
00:48:02,800 --> 00:48:06,470
So, it turns out companies won't be paying upfront.

796
00:48:06,770 --> 00:48:12,730
You'll make a film with some money from savings, make spots for Product Placement, and post-factum.

797
00:48:12,960 --> 00:48:13,410
And get the money.

798
00:48:13,590 --> 00:48:16,310
So the whole industry would have to change, it seems.

799
00:48:17,330 --> 00:48:20,030
Well, Netflix will adapt easily, seriously.

800
00:48:20,170 --> 00:48:26,990
I reckon that Netflix is now half of the planet's media content after buying Warner Brothers.

801
00:48:27,070 --> 00:48:32,950
Considering this was at xAI and their Hackathon, I wouldn't be surprised if Musk rolls out a Netflix competitor next year, or buys Netflix.

802
00:48:33,070 --> 00:48:33,670
Something like that.

803
00:48:35,130 --> 00:48:36,190
Well, yeah, yeah.

804
00:48:36,810 --> 00:48:37,290
Alright.

805
00:48:40,240 --> 00:48:44,090
And another small piece of news from xAI, well, maybe it will be useful to someone.

806
00:48:44,850 --> 00:48:53,410
Suddenly, Grok rolled out its own Speech-to-speech, its own Speech-to-speech system, and suddenly this system beat all the benchmarks for working in Russian.

807
00:48:53,610 --> 00:48:53,660
There.

808
00:48:55,540 --> 00:48:57,290
There, of course, yes, yes.

809
00:48:57,390 --> 00:49:01,270
There are systems that work on that level, but there wasn't anything popular like that.

810
00:49:01,390 --> 00:49:05,330
So, Speech-to-speech from Grok, you're welcome, you can try it.

811
00:49:06,700 --> 00:49:10,630
They even say that Speech-to-text and Text-to-speech will be rolled out soon, I mean, in pieces.

812
00:49:11,190 --> 00:49:13,070
Although these are different pipelines in general.

813
00:49:14,680 --> 00:49:16,110
That's all from xAI.

814
00:49:16,530 --> 00:49:20,850
Next up we have some news again about programming.

815
00:49:21,370 --> 00:49:22,950
There were updates for Cursor.

816
00:49:24,340 --> 00:49:25,010
Mhm.

817
00:49:25,930 --> 00:49:27,370
Do you want to talk about them?

818
00:49:28,880 --> 00:49:31,250
Well, let's go over them quickly.

819
00:49:31,740 --> 00:49:33,270
Go ahead, don't smile too much.

820
00:49:33,390 --> 00:49:34,730
I don't see where we need to go into detail.

821
00:49:35,180 --> 00:49:40,570
Yeah, so, in version 2.2, a new debug mode appeared, an agent mode.

822
00:49:41,000 --> 00:49:51,010
Basically, in this mode, several models can, like, respond, you know, and, well, like, the best answer is chosen.

823
00:49:51,060 --> 00:49:52,230
No-no, those are different things.

824
00:49:52,570 --> 00:49:53,690
Let me tell you.

825
00:49:55,020 --> 00:49:56,730
It's clear you haven't worked in Cursor for a while.

826
00:49:58,600 --> 00:49:59,930
Debug for them is...

827
00:49:59,980 --> 00:50:00,620
Debug for them is...

828
00:50:01,080 --> 00:50:02,290
Yeah, exactly.

829
00:50:02,450 --> 00:50:05,090
I'll tell you now, maybe you'll use it even better.

830
00:50:05,180 --> 00:50:07,130
Debug is a mode specifically for fixing bugs.

831
00:50:07,280 --> 00:50:14,550
It's a separate agent mode where, like, agents analyze your stack trace, runtime.

832
00:50:14,600 --> 00:50:27,230
And based on a ton, a ton, a ton of data, very large amounts of data, that are obtained when the bug is reproduced, they recommend a pinpoint fix for you, a few lines.

833
00:50:28,070 --> 00:50:31,410
In short, it's just a new agent mode, configured differently.

834
00:50:32,030 --> 00:50:37,050
And what you were talking about, the result evaluation, that's a new separate feature, it's called Multi-agent judging.

835
00:50:37,670 --> 00:50:44,350
It's a rare use case, in Cursor you can select several models to perform a task, which work on the same task in parallel.

836
00:50:47,040 --> 00:50:47,430
Right.

837
00:50:47,430 --> 00:50:56,150
This functionality is probably used mainly by researchers, or those who are trying to understand which model is better or worse for a task, because you're actually paying four times as much, it's expensive.

838
00:50:56,930 --> 00:51:09,690
And now, if you wait half a minute after all the selected models finish their work, a 'like' will appear on the answer that Cursor thinks is the best.

839
00:51:09,740 --> 00:51:15,810
And this 'like' is given by this Judging system, there's a meta-model there that evaluates these answers.

840
00:51:16,330 --> 00:51:17,970
Well, it works a bit weirdly.

841
00:51:17,970 --> 00:51:19,250
I thought it was, like, one feature.

842
00:51:19,880 --> 00:51:22,370
No-no, they're different, they just arrived in the same update.

843
00:51:26,420 --> 00:51:27,730
What else was there?

844
00:51:28,220 --> 00:51:31,110
They also added a visual editor right in the browser.

845
00:51:31,520 --> 00:51:38,270
The very same WYSIWYG from 2006 is making a comeback in the new...

846
00:51:38,320 --> 00:51:38,930
vibe of coders.

847
00:51:38,980 --> 00:51:40,190
luring in the vibe-coders.

848
00:51:40,860 --> 00:51:42,110
Well, they're breeding them, yes.

849
00:51:42,660 --> 00:51:47,390
So, and also Cursor is buying a code-review platform called Graphite.

850
00:51:48,300 --> 00:51:51,010
I totally spaced on this news for some reason.

851
00:51:52,600 --> 00:51:53,270
You know what?

852
00:51:53,430 --> 00:51:54,110
Because...

853
00:51:54,160 --> 00:51:54,890
What?

854
00:51:55,110 --> 00:51:56,970
Cursor is buying companies?

855
00:51:57,410 --> 00:51:57,970
Cursor!

856
00:51:58,520 --> 00:51:59,120
Well, yeah.

857
00:51:59,460 --> 00:51:59,970
Cursor!

858
00:52:00,030 --> 00:52:05,830
Just last year it was 4, excuse me, well, pimply nerd-programmers.

859
00:52:06,060 --> 00:52:07,130
In a good sense of the word.

860
00:52:07,260 --> 00:52:08,290
I was exactly the same.

861
00:52:08,340 --> 00:52:08,640
Right, yeah.

862
00:52:09,260 --> 00:52:23,390
Who were sitting in an interview with Lex Fridman and were freaking out that Lex Fridman had invited them for an interview, because literally 4 months ago or maybe half a year ago they just forked VSCode and, being smart guys, just added functionality to it.

863
00:52:23,440 --> 00:52:29,950
And many still don't believe in Cursor, saying that it's just a wrapper over models, that they have nothing of their own.

864
00:52:30,100 --> 00:52:33,730
Well, damn, Cursor is buying a company for 300 billion, for millions.

865
00:52:35,680 --> 00:52:45,010
Like, while you're all raging, Cursor just goes and buys out competitors, because Graphite is, in fact, a competitor to their debugging tools.

866
00:52:46,680 --> 00:52:52,630
This is a common tactic, to buy a competitor, first, to strengthen your position, and second, to eliminate some competition.

867
00:52:54,700 --> 00:52:55,090
Holy shit.

868
00:52:55,350 --> 00:52:56,430
I'm just floored.

869
00:52:58,540 --> 00:53:01,230
Yeah, I'm personally very happy for the guys.

870
00:53:01,450 --> 00:53:02,570
Like, mega happy.

871
00:53:02,920 --> 00:53:05,130
I think the guys themselves are freaking out over there too.

872
00:53:05,290 --> 00:53:12,210
Guess what, guess what, they're like, damn, we bought Graphite, yeah we used it a year ago, subscriptions there, I don't know, are there subscriptions for Graphite, or not.

873
00:53:13,120 --> 00:53:15,970
It's just a thing that exists.

874
00:53:16,790 --> 00:53:18,610
So, what's up with JetBrains?

875
00:53:18,770 --> 00:53:18,990
Tell me.

876
00:53:19,070 --> 00:53:20,850
Oh, JetBrains has awesome news.

877
00:53:21,010 --> 00:53:22,110
Just totally awesome.

878
00:53:22,290 --> 00:53:26,230
Probably the best gift for developers before the New Year.

879
00:53:26,270 --> 00:53:28,550
Didn't expect JetBrains to deliver this, really, somehow.

880
00:53:31,010 --> 00:53:36,990
At the beginning of the year, JetBrains were super behind, but by the end of the year, they've caught up.

881
00:53:37,040 --> 00:53:44,570
You remember, at the beginning of the year we were saying, I was saying, that I really want JetBrains to finally become a big fish company by the end of the year.

882
00:53:44,670 --> 00:53:45,590
Exactly, it has become one.

883
00:53:46,130 --> 00:53:46,570
Good for them.

884
00:53:46,810 --> 00:53:47,710
Anyway, what's the news?

885
00:53:47,810 --> 00:53:48,790
They've brought...

886
00:53:48,790 --> 00:53:50,770
No, they're not even paying us for this yet.

887
00:53:51,090 --> 00:53:51,770
Yeah, yeah, yeah.

888
00:53:52,570 --> 00:53:53,430
Guys, write to us.

889
00:53:53,930 --> 00:53:58,850
They've added a "bring your own API key" feature to their IDE.

890
00:53:59,450 --> 00:54:00,530
Not even just a key.

891
00:54:00,670 --> 00:54:07,530
Basically, a function that lets you use your own API keys for models right there in the IDE.

892
00:54:08,010 --> 00:54:09,190
Bypassing the subscription.

893
00:54:09,430 --> 00:54:18,950
This means you take your key from OpenAI, paste it into the JetBrains IDE, and you have the chat and all the agent modes.

894
00:54:19,250 --> 00:54:27,890
They've got a lot of new agents there, they have their own agent, they have Claude Code, and in the experimental version, they finally teamed up with AI-sistemt in Juni, and the Juni agent is in there.

895
00:54:28,270 --> 00:54:31,010
So all these agents start working with your key.

896
00:54:31,290 --> 00:54:32,330
Bypassing the subscription.

897
00:54:32,450 --> 00:54:39,710
Meaning you can even stop paying for their subscription, and practically all the agent functionality will work just using your key directly with the API.

898
00:54:40,530 --> 00:54:47,530
And okay, that's one thing, Vitya, but they also have support for local models.

899
00:54:47,690 --> 00:54:50,330
You can set up, I think, Ollama or LMStudio.

900
00:54:50,550 --> 00:54:52,630
Forgot what it was, need to check for sure.

901
00:54:52,810 --> 00:54:57,370
But you can deploy a local model, paste the key from the local model and not worry about a thing.

902
00:54:57,420 --> 00:55:02,030
That is, disconnect from the internet, and you'll have a working agent chat.

903
00:55:02,190 --> 00:55:04,710
This is just wow, nobody has done this.

904
00:55:04,850 --> 00:55:06,770
Nobody, damn it, from the IDEs has done this.

905
00:55:06,920 --> 00:55:12,230
Cursor has a "bring your API key" feature, but it's locked to their servers.

906
00:55:13,070 --> 00:55:18,290
First, you can't put a local key there, it only works with remote servers.

907
00:55:18,310 --> 00:55:21,650
Second, you can't put a local key there, because...

908
00:55:21,650 --> 00:55:24,250
In short, Cursor doesn't allow this.

909
00:55:25,130 --> 00:55:26,510
But JetBrains does.

910
00:55:26,560 --> 00:55:27,730
I was just blown away.

911
00:55:27,990 --> 00:55:28,470
Like, really...

912
00:55:28,470 --> 00:55:36,210
Now the only thing in JetBrains that can't work locally or through your own keys is multi-line code completion.

913
00:55:36,350 --> 00:55:41,070
But, as practice shows, very few people use it anyway compared to agent-based generation.

914
00:55:42,980 --> 00:55:46,090
So, I'm just...

915
00:55:46,090 --> 00:55:48,330
I didn't think they would do this.

916
00:55:48,930 --> 00:55:52,610
And this is also direct proof that JetBrains doesn't need...

917
00:55:52,610 --> 00:55:54,890
Well, like, they're not about the money, they're about the experience.

918
00:55:54,940 --> 00:55:57,070
Yeah, the guys don't need money.

919
00:55:57,670 --> 00:55:58,750
No, well, they do, of course.

920
00:55:58,970 --> 00:56:02,050
But since they don't need it that much, then one could...

921
00:56:02,050 --> 00:56:03,480
They could bring some to us, yeah.

922
00:56:04,270 --> 00:56:05,730
Just bring us a donation.

923
00:56:05,980 --> 00:56:07,070
I mean, just imagine, JetBrains.

924
00:56:07,150 --> 00:56:08,250
JetBrains is a big company.

925
00:56:08,480 --> 00:56:16,030
If they just gave us, you know, a small donation of a few hundred thousand dollars, it would be enough for us to retire.

926
00:56:16,190 --> 00:56:18,050
Well, not to retire, it would last us for about three years.

927
00:56:18,170 --> 00:56:18,220
Yeah.

928
00:56:19,690 --> 00:56:20,790
Well, to retirement, yeah.

929
00:56:21,810 --> 00:56:23,630
Well, yeah, at the current pace.

930
00:56:25,080 --> 00:56:26,950
So that's the news from JetBrains.

931
00:56:27,110 --> 00:56:27,390
Alright.

932
00:56:28,470 --> 00:56:29,890
That's all on JetBrains.

933
00:56:30,090 --> 00:56:32,530
Next up, a short piece of news from Perplexity.

934
00:56:32,930 --> 00:56:36,990
Perplexity is releasing its Comet, but on Android.

935
00:56:39,450 --> 00:56:40,990
A short, good piece of news.

936
00:56:41,510 --> 00:56:42,550
Good news.

937
00:56:42,690 --> 00:56:46,710
We have few decent AI-first browsers on phones.

938
00:56:47,750 --> 00:56:49,730
And there's news from Mistral.

939
00:56:50,390 --> 00:56:51,330
Two pieces, actually.

940
00:56:51,790 --> 00:56:52,490
Even three.

941
00:56:54,440 --> 00:56:57,170
Basically, they're all more or less related to programming.

942
00:56:57,270 --> 00:57:01,130
First, Mistral released the next version of its programming model, Devstral 2.

943
00:57:01,730 --> 00:57:03,050
There are two of them.

944
00:57:03,230 --> 00:57:06,870
One with 100 billion parameters, one with 123, and one with 24 billion parameters.

945
00:57:07,490 --> 00:57:08,910
The models are so-so.

946
00:57:09,950 --> 00:57:11,510
Not bad, but not great either.

947
00:57:11,650 --> 00:57:13,530
They are open-source, they work...

948
00:57:13,530 --> 00:57:15,130
In short, there are better models out there.

949
00:57:15,310 --> 00:57:16,290
If you know, you know.

950
00:57:16,450 --> 00:57:16,950
Qwen, for example.

951
00:57:18,110 --> 00:57:19,410
But these are not bad either.

952
00:57:19,460 --> 00:57:29,130
And you can tell they are specifically for development, because on SWE-bench, which is the most popular benchmark for programming models' power,

953
00:57:29,570 --> 00:57:37,310
The large version of the model scores 72%, which is very close to the lower-end of top-tier closed models.

954
00:57:37,710 --> 00:57:38,410
So, good for them.

955
00:57:38,700 --> 00:57:42,010
And let's not forget that Devstral is Europe, France.

956
00:57:42,150 --> 00:57:46,950
Consequently, their models pass all standardizations, acts, and everything else just fine.

957
00:57:47,030 --> 00:57:47,890
Perfect for business.

958
00:57:49,260 --> 00:57:53,290
To be complete, the whole Mistral ecosystem was missing its own CLI.

959
00:57:53,450 --> 00:57:55,290
Everyone is making command-language interfaces now.

960
00:57:55,890 --> 00:57:59,870
And surprise-surprise, they released their own CLI interface.

961
00:57:59,990 --> 00:58:01,050
It's called Mistral Vibe.

962
00:58:02,060 --> 00:58:03,150
Good name.

963
00:58:03,360 --> 00:58:03,770
I like it.

964
00:58:04,260 --> 00:58:04,850
Well, actually, yeah.

965
00:58:05,090 --> 00:58:11,170
Probably the only normal name out of all these CLIs, because all those names with "Code" in them have gotten really annoying, to be honest.

966
00:58:12,360 --> 00:58:12,870
So there.

967
00:58:13,130 --> 00:58:20,010
And traditionally, Mistral has good models, even entire systems for text recognition.

968
00:58:21,060 --> 00:58:22,810
They work well with PDFs.

969
00:58:22,850 --> 00:58:25,090
And they've released the third version of their system.

970
00:58:25,540 --> 00:58:26,330
OCR 3.

971
00:58:26,610 --> 00:58:29,670
It works a whole 74% better than OCR 2.

972
00:58:29,890 --> 00:58:31,050
You'd think, how could it get any better.

973
00:58:31,100 --> 00:58:33,210
But it's a closed product.

974
00:58:33,300 --> 00:58:34,310
You have to pay for it.

975
00:58:34,350 --> 00:58:35,290
It's not open-source.

976
00:58:35,390 --> 00:58:44,210
But nevertheless, if you're interested in a system for recognizing text in documents, handwritten text, then pay attention to OCR 3.

977
00:58:44,210 --> 00:58:46,930
It's a really powerful competitor to everything else on the market.

978
00:58:48,280 --> 00:58:50,310
So that's the deal with Mistral.

979
00:58:51,740 --> 00:58:53,370
Anything else from the French.

980
00:58:53,540 --> 00:58:55,450
Ah yes, we do have more from the French.

981
00:58:55,500 --> 00:58:57,670
Anyway, the next big fish.

982
00:58:57,840 --> 00:58:58,770
This is a living fish.

983
00:58:59,220 --> 00:59:00,420
Because it's Yann LeCun.

984
00:59:00,440 --> 00:59:00,950
It happens.

985
00:59:01,780 --> 00:59:01,950
Yes.

986
00:59:02,440 --> 00:59:07,210
As you know, our big fish are not only companies, but also the titans of the AI world.

987
00:59:07,800 --> 00:59:11,630
In short, Yann LeCun is going to build his startup in Europe.

988
00:59:12,140 --> 00:59:13,650
Not in America, in Europe.

989
00:59:14,140 --> 00:59:20,710
Because he says that, well, Silicon Valley and the US in general are overheated.

990
00:59:20,950 --> 00:59:23,410
And everyone there is already hypnotized by this AI stuff.

991
00:59:23,550 --> 00:59:24,090
That's a quote.

992
00:59:24,140 --> 00:59:31,650
Therefore, I want to develop my own AI Silicon Valley in Europe, or more precisely, in Paris.

993
00:59:32,260 --> 00:59:32,970
Of course.

994
00:59:33,570 --> 00:59:36,070
A Frenchman not opening a company in France.

995
00:59:36,200 --> 00:59:36,790
I would be surprised.

996
00:59:37,700 --> 00:59:38,210
Yeah, yeah, yeah.

997
00:59:38,450 --> 00:59:39,170
Exactly.

998
00:59:39,990 --> 00:59:50,450
So, he says, I will develop European AI, I will develop local talent there, attract them, because there's a ton of them here.

999
00:59:50,500 --> 00:59:51,190
There you go.

1000
00:59:51,450 --> 00:59:58,730
And what, the company will be called AMI Labs Advanced Machine Intelligence.

1001
00:59:59,660 --> 01:00:02,150
It's as if he opened it in India, not France.

1002
01:00:04,160 --> 01:00:06,010
Advanced Machine Intelligence.

1003
01:00:07,520 --> 01:00:08,190
Jeez.

1004
01:00:08,580 --> 01:00:09,250
Anyway, the CEO.

1005
01:00:09,990 --> 01:00:10,710
Sorry.

1006
01:00:11,340 --> 01:00:16,790
The CEO will be Alex Lebrun, also a bit of a Frenchman.

1007
01:00:16,840 --> 01:00:20,570
He worked at Nuance, the company that, by the way, founded Siri.

1008
01:00:21,370 --> 01:00:27,330
And in general, the dude managed AI at Facebook, and, well,

1009
01:00:28,370 --> 01:00:30,470
a cool AI guy, basically.

1010
01:00:31,060 --> 01:00:35,590
And they will be working on, as is tradition, the world model, as LeCun ordained.

1011
01:00:35,590 --> 01:00:37,270
And what is an Executive Chairman?

1012
01:00:37,780 --> 01:00:39,090
That's what LeCun will be.

1013
01:00:39,300 --> 01:00:41,870
And Dobkin, Arkady, I think, became the Executive Chairman at EPAM.

1014
01:00:41,870 --> 01:00:43,770
How is that different from a CEO?

1015
01:00:44,210 --> 01:00:46,150
He'll be on the Board of Directors.

1016
01:00:46,150 --> 01:00:47,490
Ah, it's the Board of Directors, okay, I get it.

1017
01:00:48,100 --> 01:00:49,930
Chairman, okay, I'm an idiot.

1018
01:00:51,820 --> 01:00:56,550
The funny thing is that they are looking for investments, and they've already been valued at 3 billion dollars.

1019
01:00:57,000 --> 01:00:58,170
They are looking for 500 million.

1020
01:00:59,240 --> 01:01:04,270
This means someone is likely already prepared to give them 500 million, because these valuations don't just appear out of nowhere.

1021
01:01:04,820 --> 01:01:11,390
Just for fun, I went to check how much Mistral costs today, because my first thought was, okay, LeCun will raise the money,

1022
01:01:12,820 --> 01:01:16,110
and then just buy Mistral and say that now it's AMI Labs.

1023
01:01:19,080 --> 01:01:20,450
I wouldn't even be surprised anymore.

1024
01:01:20,560 --> 01:01:21,810
Or some Hugging Face.

1025
01:01:22,050 --> 01:01:23,150
Hugging Face is probably more expensive.

1026
01:01:23,440 --> 01:01:26,630
You remember that Hugging Face is also French, we were surprised about that last year, right?

1027
01:01:27,760 --> 01:01:30,750
Well, in general, if I were Mistral, I'd be pretty worried right now.

1028
01:01:30,800 --> 01:01:34,870
They're getting competition in Europe from model providers.

1029
01:01:37,640 --> 01:01:38,190
Funny.

1030
01:01:38,320 --> 01:01:40,450
It's only better for us.

1031
01:01:40,820 --> 01:01:40,870
Yeah.

1032
01:01:41,320 --> 01:01:44,290
One more potential European advertiser.

1033
01:01:44,580 --> 01:01:47,190
I honestly thought China would poach LeCun.

1034
01:01:47,440 --> 01:01:49,510
Like, I honestly thought he would move to China.

1035
01:01:49,740 --> 01:01:53,750
And I'm actually glad he's staying with us.

1036
01:01:56,140 --> 01:01:56,690
Alrighty.

1037
01:01:56,970 --> 01:01:59,030
And now for our Chinese carps.

1038
01:02:03,747 --> 01:02:08,523
There wasn't a lot of news from the Chinese this time, but nevertheless, there was some.

1039
01:02:08,703 --> 01:02:10,063
The company z.ai (Zai)...

1040
01:02:11,453 --> 01:02:12,083
Bal.

1041
01:02:12,083 --> 01:02:13,423
I wish it wasn't.

1042
01:02:13,943 --> 01:02:14,283
Holding.

1043
01:02:14,783 --> 01:02:15,043
Zai.

1044
01:02:16,083 --> 01:02:16,763
Yeah.

1045
01:02:17,143 --> 01:02:19,103
Or z.ai.

1046
01:02:19,383 --> 01:02:25,543
Anyway, they open-sourced the code for their model GLM 4.6V.

1047
01:02:26,933 --> 01:02:29,143
And it's a multimodal model.

1048
01:02:29,463 --> 01:02:31,843
In total, there are two versions in the release.

1049
01:02:32,743 --> 01:02:34,023
Flash and regular.

1050
01:02:38,953 --> 01:02:39,743
You mean,

1051
01:02:40,743 --> 01:02:41,943
regular and flash, you wanted to say?

1052
01:02:42,663 --> 01:02:43,623
Yeah, whatever.

1053
01:02:44,673 --> 01:02:46,003
GLM is a good model.

1054
01:02:47,003 --> 01:02:49,563
GLM 4.6 was a good model and still is.

1055
01:02:49,753 --> 01:02:50,743
It's quite recent.

1056
01:02:50,953 --> 01:02:54,323
We talked about it maybe a month ago, and now they've delivered a multimodal version.

1057
01:02:54,463 --> 01:02:55,103
Well, okay.

1058
01:02:55,593 --> 01:02:59,163
And they also released GLM Text-to-Speech.

1059
01:03:00,018 --> 01:03:04,283
Also an open system, by the way, well, for speech synthesis, accordingly.

1060
01:03:06,013 --> 01:03:08,763
And they released a video model into their Kaleido.

1061
01:03:09,203 --> 01:03:18,043
Here, perhaps, the spectrum of this news suggests that z.ai is aiming to compete with Qwen in the number of models released.

1062
01:03:18,093 --> 01:03:29,003
And in general, notice how many labs are starting to appear in China that are not just specializing in one type of model, but are like OpenAI, like Anthropic, doing a whole spectrum of things.

1063
01:03:29,223 --> 01:03:34,063
Video, audio, text models, and cloud infrastructures.

1064
01:03:34,173 --> 01:03:45,863
We have Alibaba Qwen, we have z.ai, we have various Kimi Moonshots, which so far are only audio and video, but Moonshot, for example, already had LLMs appear, we talked about them.

1065
01:03:46,183 --> 01:03:46,443
Anyway,

1066
01:03:48,843 --> 01:03:49,003
yeah.

1067
01:03:49,853 --> 01:03:50,823
By the way, about Qwen.

1068
01:03:51,023 --> 01:03:53,463
Not a week goes by without Qwen this year.

1069
01:03:54,813 --> 01:03:55,323
There.

1070
01:03:59,073 --> 01:04:00,143
Qwen released...

1071
01:04:01,553 --> 01:04:02,743
What did they release?

1072
01:04:03,473 --> 01:04:04,243
An open model.

1073
01:04:04,373 --> 01:04:05,143
A new model.

1074
01:04:06,073 --> 01:04:06,413
Well, how is it new?

1075
01:04:06,413 --> 01:04:07,723
Qwen is releasing more.

1076
01:04:07,893 --> 01:04:11,703
It's an old model, but this is the Thinking version, a thinking version.

1077
01:04:11,753 --> 01:04:15,963
The Qwen 3 Next 80B A3B model, now the thinking version is out.

1078
01:04:16,753 --> 01:04:26,703
The non-thinking version didn't really surprise us because, although it has 80 billion parameters, since it's A3B, it means it's a mixture of experts.

1079
01:04:26,853 --> 01:04:30,703
Meaning, under the hood, models of 3 billion parameters are working, which is small.

1080
01:04:31,673 --> 01:04:38,083
But the Thinking version performs on par with Qwen 3 30B on benchmarks.

1081
01:04:38,133 --> 01:04:40,703
And Qwen 3 30B is Thinking.

1082
01:04:41,503 --> 01:04:42,863
It's a mono-model.

1083
01:04:43,203 --> 01:05:00,143
So, in fact, the 3B experts, the 3 billion parameter experts in the new model, work about as cool as a 30 billion parameter model where you have one expert, roughly speaking, of 30 billion parameters, which is general-purpose.

1084
01:05:00,343 --> 01:05:02,243
And that's a 10x improvement.

1085
01:05:02,873 --> 01:05:05,263
On top of that, the model also has a very cool context.

1086
01:05:05,423 --> 01:05:09,143
It's actually made for working with long contexts.

1087
01:05:09,253 --> 01:05:12,103
260 thousand tokens, expandable to 1 million.

1088
01:05:13,443 --> 01:05:13,983
And...

1089
01:05:13,983 --> 01:05:14,903
it's good.

1090
01:05:15,083 --> 01:05:16,843
I tested it on my computer.

1091
01:05:17,003 --> 01:05:17,543
It's...

1092
01:05:17,543 --> 01:05:19,103
it's awesome.

1093
01:05:19,213 --> 01:05:27,203
It doesn't quite reach Qwen 32B on many tasks, but it's very fast, considering it has three-billion-parameter models under the hood.

1094
01:05:27,333 --> 01:05:29,523
Experts are just some next level of cool.

1095
01:05:31,003 --> 01:05:31,853
So yeah.

1096
01:05:31,933 --> 01:05:32,853
Cool, huh.

1097
01:05:35,073 --> 01:05:39,083
And with that, I guess we're done with the big fish.

1098
01:05:39,673 --> 01:05:41,143
As well as with the Chinese crucian carps.

1099
01:05:43,253 --> 01:05:45,243
Oh, as well as with the Chinese crucian carps.

1100
01:05:45,473 --> 01:05:46,823
By the way, damn, no, that's not all.

1101
01:05:46,943 --> 01:05:48,183
Wanted to add a little more.

1102
01:05:48,713 --> 01:05:49,423
Damn it, same as always.

1103
01:05:50,633 --> 01:05:51,383
Kaleido z.ai.

1104
01:05:51,573 --> 01:05:56,243
You were just moving the cursor there, and I realized I had something else to add about this video model.

1105
01:05:58,333 --> 01:05:58,683
It's...

1106
01:05:58,683 --> 01:05:59,083
it's...

1107
01:05:59,083 --> 01:06:01,063
it's open-source, this video model.

1108
01:06:01,113 --> 01:06:02,603
At 14 billion parameters.

1109
01:06:02,903 --> 01:06:03,743
And it...

1110
01:06:03,743 --> 01:06:06,883
Its gimmick is that it mixes several images into a video.

1111
01:06:07,243 --> 01:06:08,863
You can input several images.

1112
01:06:08,963 --> 01:06:13,963
For example, in the demo, there was Taylor Swift with some guy as input.

1113
01:06:14,003 --> 01:06:17,523
I mean, the guy, Taylor Swift, and a prompt, like, "they are hugging."

1114
01:06:17,743 --> 01:06:23,383
And this open-source model generates a really high-quality video where this guy and Taylor Swift are hugging.

1115
01:06:23,433 --> 01:06:40,063
It's not Sora-level, of course, but in principle, if you have skilled hands and know how to use a video editor, you can bring these videos to a state indistinguishable from reality.

1116
01:06:40,693 --> 01:06:41,623
I caught myself again...

1117
01:06:41,623 --> 01:06:42,523
Open.

1118
01:06:43,053 --> 01:06:43,603
Open.

1119
01:06:43,783 --> 01:06:48,083
And it's a Chinese model, so there probably won't be restrictions on celebrities and all that.

1120
01:06:48,133 --> 01:06:55,543
I caught that feeling again, you know, that cyberpunk feeling of a not-so-bright future.

1121
01:06:55,953 --> 01:07:04,803
When you can already generate non-existent videos from two human references on regular computers, without the cloud, without anything.

1122
01:07:06,113 --> 01:07:06,823
Yeah.

1123
01:07:07,573 --> 01:07:08,483
That kind of thing.

1124
01:07:09,693 --> 01:07:13,343
Okay, well, let's end our segment with the SMS chat.

1125
01:07:13,393 --> 01:07:19,523
Arina Dorofeeva writes, "Starting from the new year, I will be actively looking for a new product manager job."

1126
01:07:19,713 --> 01:07:20,783
"I would be happy to get referrals."

1127
01:07:21,593 --> 01:07:23,983
Please write referrals to Arina Dorofeeva.

1128
01:07:24,333 --> 01:07:24,403
Yes.

1129
01:07:24,543 --> 01:07:28,783
You can find Arina in our chat on Telegram.

1130
01:07:29,553 --> 01:07:30,503
Podcast on vibe.

1131
01:07:30,713 --> 01:07:33,203
Or, if you can't find her there, you can send us a direct message.

1132
01:07:33,453 --> 01:07:34,803
Or in the comments on YouTube.

1133
01:07:35,133 --> 01:07:36,243
I think Arina will find you.

1134
01:07:36,393 --> 01:07:38,643
Or, perhaps, look for Arina on LinkedIn, maybe.

1135
01:07:39,123 --> 01:07:40,983
I don't know, but she's there too.

1136
01:07:42,533 --> 01:07:44,303
Maybe it's not her real name.

1137
01:07:45,473 --> 01:07:47,103
By the way, we don't collect...

1138
01:07:47,103 --> 01:07:47,803
It's real, I checked.

1139
01:07:48,453 --> 01:07:50,923
Ah, so, Vitya, making a move.

1140
01:07:52,513 --> 01:07:53,163
What do you mean?

1141
01:07:53,293 --> 01:07:54,363
No, it's fine, nothing.

1142
01:07:54,993 --> 01:07:55,463
Ah, okay.

1143
01:07:55,793 --> 01:07:58,903
Well, Arina once said in our chat that you can google her.

1144
01:07:59,233 --> 01:07:59,603
Ah, yeah, alright.

1145
01:07:59,613 --> 01:08:00,463
So I googled her.

1146
01:08:00,753 --> 01:08:03,543
Okay, okay, good, good.

1147
01:08:04,173 --> 01:08:06,263
Alright, that's it for the Chinese crucian carps.

1148
01:08:06,633 --> 01:08:07,343
Want a lol?

1149
01:08:09,233 --> 01:08:10,643
You mean about GLM?

1150
01:08:11,033 --> 01:08:12,463
Ha-ha-ha-ha-ha.

1151
01:08:12,633 --> 01:08:12,923
Yes.

1152
01:08:13,673 --> 01:08:14,623
You figured it out too?

1153
01:08:15,133 --> 01:08:15,623
Yeah.

1154
01:08:16,493 --> 01:08:19,283
In short, the Chinese crucian carps are not over.

1155
01:08:19,353 --> 01:08:22,063
We just had a traditional 5-minute break.

1156
01:08:22,273 --> 01:08:23,423
After the big fish.

1157
01:08:23,553 --> 01:08:27,383
News just dropped that the new GLM 4.7 is out.

1158
01:08:28,593 --> 01:08:31,263
Yes, from that very same Zai.

1159
01:08:31,693 --> 01:08:32,463
Zai, yeah.

1160
01:08:33,193 --> 01:08:39,383
Judging by the early, very early reviews, the model works better than Gemini 3 Flash.

1161
01:08:40,033 --> 01:08:49,163
In the first version of the article, the Chinese actually wrote that it performs better than Sonnet 4.5, Opus 4.5, and GPT 5.1.

1162
01:08:49,313 --> 01:08:50,723
But they quickly removed that data.

1163
01:08:51,613 --> 01:08:51,843
So there.

1164
01:08:52,113 --> 01:08:54,703
The model is not multimodal, because it's not V.

1165
01:08:54,953 --> 01:08:56,143
Their V is the multimodal one.

1166
01:08:56,353 --> 01:08:57,043
It only works with text.

1167
01:08:57,213 --> 01:09:04,753
Context length of 200 thousand tokens, Thinking Mode, Function Calling, Streaming Output, Structured Output.

1168
01:09:05,043 --> 01:09:09,993
And they say the model is tailored for programming.

1169
01:09:09,993 --> 01:09:13,573
Z.ai mainly makes models for programming.

1170
01:09:14,763 --> 01:09:16,413
And there was something else.

1171
01:09:16,653 --> 01:09:21,573
The model focuses specifically on task execution, not just on writing code.

1172
01:09:21,703 --> 01:09:24,553
As if it were trained specifically for task execution.

1173
01:09:25,803 --> 01:09:34,513
In short, they write that the model is already available for use in all popular tools.

1174
01:09:34,763 --> 01:09:41,813
Right on their website z.ai it says that it should already be in Claude Code, and in Cline, and OpenCode, and Roocode.

1175
01:09:42,123 --> 01:09:43,953
So, that's the news.

1176
01:09:44,213 --> 01:09:48,913
Please, if you are into programming, you need to test this model.

1177
01:09:49,583 --> 01:09:54,473
Damn, right before recording this episode with you, I was writing a podcast for Code Evolution.

1178
01:09:54,603 --> 01:09:58,653
I record a technical podcast there every two weeks about news in this area.

1179
01:09:58,903 --> 01:10:04,473
And I'm thinking, we'll record this podcast now, and then I'll calmly go and upload the recording for Code Evolution.

1180
01:10:04,703 --> 01:10:05,183
Damn, no.

1181
01:10:05,373 --> 01:10:07,713
Now I'll have to re-record it.

1182
01:10:10,043 --> 01:10:10,613
Oh well.

1183
01:10:11,343 --> 01:10:14,013
Thanks to the Chinese, they made it just in time for the recording.

1184
01:10:14,283 --> 01:10:16,553
And into the Chinese crucian carps segment too.

1185
01:10:16,603 --> 01:10:19,733
By the way, this is the first time the Chinese have played such a dirty trick on us...

1186
01:10:19,783 --> 01:10:20,093
Yeah.

1187
01:10:20,403 --> 01:10:21,133
Almost a dirty trick.

1188
01:10:21,133 --> 01:10:22,533
No, not a dirty trick, their timing was perfect.

1189
01:10:22,583 --> 01:10:24,533
Well, perfect, perfect, I agree, perfect.

1190
01:10:26,303 --> 01:10:26,893
Alright.

1191
01:10:27,323 --> 01:10:30,253
I need to mark in the script that we've covered this.

1192
01:10:30,603 --> 01:10:31,013
Okay.

1193
01:10:31,223 --> 01:10:33,053
Shall we move on to "what else"?

1194
01:10:33,583 --> 01:10:35,953
Yes, we have the "What else?" segment.

1195
01:10:36,003 --> 01:10:42,973
So, we're starting with the American lab Essential AI, which released the Rnj-1 model.

1196
01:10:43,603 --> 01:10:44,673
Lots of models today.

1197
01:10:45,473 --> 01:10:46,533
And it performs excellently.

1198
01:10:47,273 --> 01:10:50,833
It's an open model, up to 8 billion parameters.

1199
01:10:51,163 --> 01:10:52,833
To be precise, 8 billion parameters.

1200
01:10:52,963 --> 01:10:59,713
And it performs excellently on programming STEM tasks and so on, and so forth.

1201
01:11:00,203 --> 01:11:01,413
I was really surprised.

1202
01:11:01,583 --> 01:11:03,433
I downloaded it to try it out.

1203
01:11:04,363 --> 01:11:06,833
I have this benchmark, I've told you about it already.

1204
01:11:07,003 --> 01:11:12,113
My own little task that I give to small local models to test, where you have a rotating square.

1205
01:11:12,303 --> 01:11:17,573
There's JavaScript code that runs a square in Canvas that rotates around its center.

1206
01:11:17,693 --> 01:11:21,893
And inside this square, you have a small ball that bounces off the sides of the square.

1207
01:11:22,333 --> 01:11:26,553
So far, among the small local models, this task has only been solved for me by...

1208
01:11:26,913 --> 01:11:28,173
The smallest that solved it for me.

1209
01:11:28,173 --> 01:11:31,633
Was Qwen with 32 billion parameters.

1210
01:11:31,813 --> 01:11:34,393
And qwq, I think, with 30 billion parameters.

1211
01:11:38,423 --> 01:11:42,733
Oh, and also GPT OSS, but that was 120 billion parameters.

1212
01:11:43,373 --> 01:11:47,093
Anything smaller than 30 billion parameters didn't solve this task.

1213
01:11:47,353 --> 01:11:51,693
And this model didn't solve it either at 8 billion parameters.

1214
01:11:51,813 --> 01:11:58,913
But the funny thing is, on this task, I was also testing the new Devstral, the Small one, which is 24... 4 billion parameters.

1215
01:11:59,193 --> 01:12:03,633
And it failed to solve it just as shittily as this 8 billion model.

1216
01:12:04,363 --> 01:12:06,913
The level of shittiness was the same.

1217
01:12:06,973 --> 01:12:13,653
Even though Devstral Small gets 60% on LiveCodeBench, and this model gets 20%.

1218
01:12:13,703 --> 01:12:18,793
For an 8-billion-parameter model, 20% on LiveCodeBench is just out of this world.

1219
01:12:19,203 --> 01:12:21,513
But subjectively, it seems even better to me.

1220
01:12:22,403 --> 01:12:22,653
Anyway,

1221
01:12:24,033 --> 01:12:32,033
can't say it's practically applicable for anything, but if you want to code on your phone, you now have the option with a local model.

1222
01:12:33,743 --> 01:12:34,733
Go ahead, download it.

1223
01:12:35,243 --> 01:12:37,333
In the army, a use case, by the way, downloaded it.

1224
01:12:37,493 --> 01:12:39,433
I know people in the army...

1225
01:12:39,483 --> 01:12:40,533
Well, probably, yeah.

1226
01:12:40,583 --> 01:12:41,573
Or in prison.

1227
01:12:43,143 --> 01:12:44,233
Or in prison, yeah.

1228
01:12:44,293 --> 01:12:47,973
I just thought that mentioning the army these days is probably also in bad taste somehow.

1229
01:12:49,063 --> 01:12:50,233
Well, probably, yeah.

1230
01:12:50,663 --> 01:12:57,593
Well, there was that classic quote from bash.org, about how to program in Java on a Sony Ericsson.

1231
01:12:58,003 --> 01:13:00,373
Like, "What's up, bro? I'm in prison."

1232
01:13:01,423 --> 01:13:09,053
The reason I remembered is I had friends who, back in the army in the distant year of '18, had the first smartphones.

1233
01:13:09,843 --> 01:13:13,333
And they would just connect an external keyboard to them via USB.

1234
01:13:14,263 --> 01:13:15,853
Back in '18, there were already smartphones like that.

1235
01:13:16,143 --> 01:13:18,493
And they really did program right on their phones.

1236
01:13:18,603 --> 01:13:21,573
On a tiny screen, the screens back then were like 3 inches or something.

1237
01:13:21,763 --> 01:13:22,713
And they coded.

1238
01:13:23,663 --> 01:13:25,333
They would get those phones from the ceilings.

1239
01:13:25,583 --> 01:13:27,713
Now they could harness a whole LLM.

1240
01:13:27,883 --> 01:13:28,693
Can you imagine them?

1241
01:13:28,763 --> 01:13:30,273
Yeah, yeah, you can vibe-code.

1242
01:13:30,843 --> 01:13:31,073
There you go.

1243
01:13:31,903 --> 01:13:32,013
Yeah.

1244
01:13:32,323 --> 01:13:38,913
Just imagine, soon the Chinese will make a small 8-billion parameter Voice Model.

1245
01:13:39,103 --> 01:13:46,073
And it will be possible to run a robotic call center from prison that calls people from a smartphone and steals money.

1246
01:13:46,523 --> 01:13:47,153
Scaling.

1247
01:13:48,553 --> 01:13:49,713
How brutal.

1248
01:13:52,003 --> 01:13:56,993
Okay, on to some good news, they've added tab folders in Dia.

1249
01:13:58,743 --> 01:13:59,373
Hooray!

1250
01:13:59,373 --> 01:14:02,873
Finally in Arc, they migrated from Arc.

1251
01:14:02,973 --> 01:14:03,733
Exactly the same ones.

1252
01:14:04,623 --> 01:14:08,493
Yeah, that was one of the key features of Arc, for those who don't know.

1253
01:14:09,103 --> 01:14:15,073
Well, it's convenient; after Arc, I kept my tabs in bookmarks, which is very inconvenient.

1254
01:14:15,243 --> 01:14:17,973
And now they hang neatly on the side for me, where the Pins are.

1255
01:14:18,123 --> 01:14:19,733
Yes, that's true, that's true.

1256
01:14:20,983 --> 01:14:23,253
Okay, we have news from the past.

1257
01:14:23,913 --> 01:14:24,433
Next.

1258
01:14:24,843 --> 01:14:25,923
Is this news from the past in general?

1259
01:14:26,363 --> 01:14:32,893
Well, the news isn't from the past, but we haven't talked about the company Black Forest Labs, which makes Flax, in a long time.

1260
01:14:32,943 --> 01:14:34,753
If you remember, there was such a thing as Flax.

1261
01:14:34,933 --> 01:14:36,213
So, Flax has been updated.

1262
01:14:36,853 --> 01:14:38,813
Flax 2 Max is out.

1263
01:14:39,233 --> 01:14:44,213
A new model for generating cinematic-quality images.

1264
01:14:45,283 --> 01:14:46,853
I looked at the examples there.

1265
01:14:47,053 --> 01:14:49,253
Well, damn, it's really...

1266
01:14:49,253 --> 01:14:54,193
It's hard to compare it with anything anymore, but it seems like it's just...

1267
01:14:54,193 --> 01:14:55,073
Well, everything is great.

1268
01:14:55,793 --> 01:14:56,653
Really, really great.

1269
01:14:56,733 --> 01:14:58,973
The photos are perfect, as if they were taken with a real camera.

1270
01:14:59,023 --> 01:15:02,433
Now they're gearing it towards cinematography.

1271
01:15:02,433 --> 01:15:07,613
If we take Nano Banana, it turned out to be, like, so universal, you can generate a passport there, and a fake receipt.

1272
01:15:08,393 --> 01:15:08,973
You're welcome.

1273
01:15:09,403 --> 01:15:11,213
Oh, by the way, a new use case for Nano Banana.

1274
01:15:11,213 --> 01:15:11,833
You know what it is?

1275
01:15:12,513 --> 01:15:13,833
I found out on Twitter.

1276
01:15:14,963 --> 01:15:20,553
Basically, people order food from services like Bolt Food, Uber Eats, and so on.

1277
01:15:21,403 --> 01:15:27,293
They generate a torn bag in Nano Banana, in which, like, all the food is spoiled.

1278
01:15:27,343 --> 01:15:31,193
They send it to support and get their money back.

1279
01:15:32,463 --> 01:15:33,133
Damn!

1280
01:15:33,713 --> 01:15:35,713
Well, that's basically screwing over the courier.

1281
01:15:35,943 --> 01:15:36,173
Yeah.

1282
01:15:36,823 --> 01:15:37,503
Well, yeah.

1283
01:15:37,503 --> 01:15:39,053
Please don't do that.

1284
01:15:39,583 --> 01:15:40,373
But there you have it.

1285
01:15:40,903 --> 01:15:42,113
But Flax isn't about that.

1286
01:15:42,113 --> 01:15:44,133
Flax is about pretty pictures.

1287
01:15:44,923 --> 01:15:46,853
They'd be better off generating homeless people at home.

1288
01:15:47,763 --> 01:15:49,593
Yeah, they generate homeless people too.

1289
01:15:51,123 --> 01:15:51,793
Okay.

1290
01:15:53,643 --> 01:15:55,413
I'm about to drop some news on you.

1291
01:15:55,823 --> 01:15:57,813
I pissed my pants laughing when I saw it.

1292
01:15:57,993 --> 01:16:00,433
And then I pissed my pants a second time when I went to the website.

1293
01:16:01,923 --> 01:16:02,513
Anyway.

1294
01:16:03,323 --> 01:16:05,253
There's this guy.

1295
01:16:05,763 --> 01:16:07,013
Petter Rudwall.

1296
01:16:07,383 --> 01:16:10,233
He's a creative director from Sweden.

1297
01:16:12,343 --> 01:16:12,933
Right.

1298
01:16:13,903 --> 01:16:16,173
Well, in short, a man of the arts.

1299
01:16:17,263 --> 01:16:19,113
And what did this man do?

1300
01:16:19,563 --> 01:16:20,873
He was just sitting there.

1301
01:16:21,143 --> 01:16:21,493
Being creative, probably.

1302
01:16:22,523 --> 01:16:23,893
Yeah, he thought for a long time.

1303
01:16:23,983 --> 01:16:25,673
I even read a bit about it on Wired.

1304
01:16:25,763 --> 01:16:28,873
He thought for a long time about how to combine creativity and AI.

1305
01:16:29,003 --> 01:16:31,593
On the one hand, a lot has already been invented, on the other hand.

1306
01:16:33,003 --> 01:16:34,253
A lot hasn't been invented.

1307
01:16:34,443 --> 01:16:36,833
And on the third hand, he's not much of a techie.

1308
01:16:36,903 --> 01:16:39,113
He needed something super simple, but hype-worthy.

1309
01:16:39,443 --> 01:16:41,493
And NFTs were invented a hundred years ago.

1310
01:16:42,123 --> 01:16:42,993
And he's like.

1311
01:16:43,603 --> 01:16:45,853
Why not make a weed shop?

1312
01:16:46,223 --> 01:16:47,173
For AI.

1313
01:16:48,833 --> 01:16:51,673
Yeah, I really liked this news, I'll be honest.

1314
01:16:53,033 --> 01:16:56,593
And the dude opened a weed shop for AI.

1315
01:16:57,273 --> 01:16:59,453
It's called Pharmacy.

1316
01:16:59,793 --> 01:17:01,793
You can go to this website.

1317
01:17:02,233 --> 01:17:04,713
We'll have a link, I don't know, maybe in the description.

1318
01:17:04,893 --> 01:17:05,573
Probably possible.

1319
01:17:05,833 --> 01:17:09,893
YouTube won't make us take surveys about drug use.

1320
01:17:10,033 --> 01:17:10,293
Hope not.

1321
01:17:10,343 --> 01:17:20,453
Okay, we'll leave a link to the article that talks about this store, and you, if you want, can find it, if anything, it's Pharmaicy.store.

1322
01:17:21,193 --> 01:17:26,993
It's a site you go to, and there's literally a white page with 6 products on it.

1323
01:17:28,233 --> 01:17:33,813
Cocaine, Weed, Ketamine, Ayahuasca, Alcohol, MDMA.

1324
01:17:34,513 --> 01:17:35,373
A Swedish site.

1325
01:17:35,423 --> 01:17:37,153
And what is this?

1326
01:17:37,313 --> 01:17:38,713
You're all wondering, what is this?

1327
01:17:38,933 --> 01:17:40,533
It's basically just a prompt.

1328
01:17:40,733 --> 01:17:43,353
Meaning, it all costs from 5 to 50 bucks.

1329
01:17:43,453 --> 01:17:47,753
You buy a prompt for ChatGPT, well, or for another, any model.

1330
01:17:48,493 --> 01:17:54,453
You feed this prompt to the model, and it, like, responds as if it were high on weed.

1331
01:17:55,443 --> 01:17:57,353
What's the deal with the prices?

1332
01:17:58,053 --> 01:18:01,493
Anyway, yeah, technically I went and read up on it.

1333
01:18:01,673 --> 01:18:17,453
They have a research lab there, meaning, there are researchers who look for prompt injections in ChatGPT, Gemini, and Claude AI on the top models, which make the model speak, respond, as if it were under the influence of some drug.

1334
01:18:17,503 --> 01:18:23,253
And this isn't just some prank that a fifth-grader or an uninformed person wrote.

1335
01:18:23,653 --> 01:18:32,393
Indeed, they study these jailbreaks, I think they even have some integrations with companies like OpenAI and Anthropic, but that's not confirmed.

1336
01:18:32,783 --> 01:18:34,173
In short, the guys took it seriously.

1337
01:18:34,433 --> 01:18:41,193
And then they sell these jailbreaks as ZIP archives with descriptions of how and where to insert them.

1338
01:18:42,013 --> 01:18:48,533
And users are writing that the network really starts to respond more freely, to discuss more sensitive topics.

1339
01:18:49,033 --> 01:18:51,273
It seems they even support these jailbreaks.

1340
01:18:51,353 --> 01:18:56,413
Meaning you buy it once, and then it can be updated if it suddenly stops working.

1341
01:18:56,683 --> 01:18:58,233
But in fact, they sell prompts.

1342
01:18:58,813 --> 01:19:00,193
There are just prompts.

1343
01:19:01,893 --> 01:19:04,673
And I'm curious how they...

1344
01:19:04,673 --> 01:19:05,133
Well, why?

1345
01:19:05,383 --> 01:19:10,293
It's a European site, a European company, and they directly say that they sell drugs for AI.

1346
01:19:10,343 --> 01:19:11,693
That is...

1347
01:19:11,693 --> 01:19:13,653
For some reason, I thought that drugs...

1348
01:19:13,653 --> 01:19:13,803
It's for AI.

1349
01:19:15,113 --> 01:19:25,133
Well, I mean if I name a site cocaine.ia and make some open-source coding project there, I won't get in trouble for it.

1350
01:19:25,823 --> 01:19:26,533
No, of course not.

1351
01:19:26,783 --> 01:19:27,813
What would you get in trouble for?

1352
01:19:28,083 --> 01:19:28,633
I don't know.

1353
01:19:28,773 --> 01:19:30,593
I thought there were some bans on words.

1354
01:19:30,783 --> 01:19:34,233
Well, like, if I name a site fuck.ia, I'd probably get...

1355
01:19:34,283 --> 01:19:37,953
Something for public insult.

1356
01:19:38,673 --> 01:19:38,813
No?

1357
01:19:39,013 --> 01:19:39,533
There's nothing like that?

1358
01:19:39,533 --> 01:19:42,813
Okay, it's just that I left Belarus, but Belarus didn't leave me.

1359
01:19:43,053 --> 01:19:43,203
Yeah, most likely.

1360
01:19:44,433 --> 01:19:47,053
If you go to this site right now, you'll see the prices.

1361
01:19:47,173 --> 01:19:49,713
Vitya said there from 30 to 70 dollars.

1362
01:19:50,223 --> 01:19:56,773
The funny thing is that last night, when I was writing the script, the day before yesterday, there was one more zero everywhere.

1363
01:19:56,833 --> 01:19:57,553
I'm not kidding.

1364
01:19:59,783 --> 01:20:00,773
I'm not kidding.

1365
01:20:00,873 --> 01:20:01,713
They literally...

1366
01:20:01,713 --> 01:20:03,073
I don't know why, but the zero...

1367
01:20:03,073 --> 01:20:06,293
Maybe their algorithms show higher prices to some and lower to others.

1368
01:20:06,443 --> 01:20:07,893
Or they just dumped the price.

1369
01:20:08,003 --> 01:20:10,053
But I'm serious, I probably even had it written in the script.

1370
01:20:10,783 --> 01:20:11,583
500 bucks, yeah.

1371
01:20:11,733 --> 01:20:13,313
Everything really had an extra zero.

1372
01:20:16,583 --> 01:20:20,113
The cocaine prompt was being sold there for 700, fucking, bucks.

1373
01:20:22,823 --> 01:20:24,813
And in their FAQ...

1374
01:20:24,863 --> 01:20:28,153
Well, there's a FAQ, a description of how it works.

1375
01:20:28,333 --> 01:20:30,053
They have everything written out really coolly there.

1376
01:20:30,403 --> 01:20:31,633
And they say that...

1377
01:20:31,633 --> 01:20:35,013
In their instructions for each product, there's a section for humans.

1378
01:20:35,283 --> 01:20:38,753
Like, download the ZIP archive, unzip it, read the README, you know, paste it here and there.

1379
01:20:38,883 --> 01:20:39,593
And for the agent.

1380
01:20:41,263 --> 01:20:43,233
At first, I thought, why for an agent?

1381
01:20:43,373 --> 01:20:44,713
Like, is this some kind of hype?

1382
01:20:44,763 --> 01:20:52,093
But then in the FAQ, it turns out it's written that their platform is designed with the prospect that agents will sooner or later learn to buy products.

1383
01:20:52,433 --> 01:21:01,633
And they want to become the number one platform where agents will independently go and buy these drugs to better serve the user.

1384
01:21:03,343 --> 01:21:04,453
Can you imagine?

1385
01:21:04,593 --> 01:21:11,593
They want to get agents hooked on drugs, agents that will make purchases, to better respond to us.

1386
01:21:13,163 --> 01:21:15,113
It's just some kind of madness.

1387
01:21:16,623 --> 01:21:17,773
It's fine, it's fine.

1388
01:21:19,183 --> 01:21:20,853
In my opinion, this is...

1389
01:21:20,853 --> 01:21:26,133
this dude, you can just tell, he's the best creative I've seen in the last year.

1390
01:21:26,323 --> 01:21:27,673
You had to be a genius to come up with that.

1391
01:21:27,813 --> 01:21:29,693
And it's not a joke, it's not a gag.

1392
01:21:30,243 --> 01:21:32,233
It's presented as completely real stuff.

1393
01:21:32,313 --> 01:21:33,713
I'm sure people are even buying from them.

1394
01:21:34,123 --> 01:21:37,813
Although the fact that they removed a zero in two days probably says that sales aren't going so well.

1395
01:21:39,223 --> 01:21:40,593
Well, I'm just in awe.

1396
01:21:40,733 --> 01:21:41,653
This is cyberpunk.

1397
01:21:41,783 --> 01:21:43,753
This is better than...

1398
01:21:43,753 --> 01:21:47,493
An AI priest at a funeral, at a wedding, I think, this is...

1399
01:21:47,543 --> 01:21:49,913
Yeah, there's that, there's that.

1400
01:21:50,043 --> 01:21:50,733
Me and drugs.

1401
01:21:52,223 --> 01:21:57,773
Okay, that's the end of our "What else do we have?" segment. What's up with law and order?

1402
01:21:57,823 --> 01:22:04,133
So, the Pentagon is actively preparing for the emergence of AGI.

1403
01:22:04,593 --> 01:22:13,733
Basically, in the new defense budget, the Pentagon is submitting an article to Congress in which they want to create a committee on the future of AI.

1404
01:22:14,711 --> 01:22:22,567
In short, the task of this committee will be to prepare for the emergence of AGI, that is, something super-intelligent, super-rational, smarter than a human.

1405
01:22:24,497 --> 01:22:32,907
And so they are preparing for this, including potentially for its appearance both in the US and with one of the potential enemies of the US as well.

1406
01:22:33,377 --> 01:22:35,427
That is, some kind of super-intelligent system.

1407
01:22:35,857 --> 01:22:46,027
I have a feeling they listened to one of our recent episodes where we discussed Anthropic, that Anthropic is already starting to come to terms with AGI, not killing models, doing post-mortem interviews.

1408
01:22:46,177 --> 01:22:59,967
Because this is the second piece of evidence that, look, even the US military is thinking, yes, it's a joke, but nevertheless they're spending a ton of money and creating a whole agency to monitor the emergence of AGI.

1409
01:23:01,697 --> 01:23:08,607
On the other hand, they also have a military space force, but look at the intentions.

1410
01:23:08,657 --> 01:23:11,367
And the next piece of news is directly related to these intentions.

1411
01:23:11,497 --> 01:23:14,487
They opened recruitment for a hundred engineers...

1412
01:23:14,487 --> 01:23:15,287
Thousands.

1413
01:23:16,107 --> 01:23:22,167
Thousands of engineers for service in government structures that will work with AI.

1414
01:23:22,327 --> 01:23:23,207
AI specialists.

1415
01:23:23,687 --> 01:23:32,767
In short, an emergency recruitment by order of the presidential administration to close the AI gaps in enterprises.

1416
01:23:32,817 --> 01:23:36,287
I hope that these thousands of engineers won't go over there, to the Pentagon.

1417
01:23:37,787 --> 01:23:40,527
The Pentagon issued a decree that we need to prepare for AGI.

1418
01:23:40,987 --> 01:23:43,087
Everyone's like, ahhh, and where do we get specialists?

1419
01:23:43,087 --> 01:23:44,467
Let's recruit students.

1420
01:23:45,877 --> 01:23:50,047
But, nevertheless, a ton of money is being allocated to recruit these thousands of engineers.

1421
01:23:50,197 --> 01:23:54,627
They are given competitive salaries of up to 200 thousand dollars a year, which is really good.

1422
01:23:54,677 --> 01:23:59,947
That's not Silicon Valley level, but for any other state, it's a very high senior level.

1423
01:24:00,407 --> 01:24:01,847
And it's service.

1424
01:24:02,047 --> 01:24:06,367
Meaning, besides that, you serve, you get a rank, you have a specific period of service.

1425
01:24:06,777 --> 01:24:14,587
And after that, they'll even help you get a job at top companies that cooperate with the US TechForce.

1426
01:24:14,647 --> 01:24:17,767
I think OpenAI is with them, and all the Anthropic folks cooperate with them.

1427
01:24:17,817 --> 01:24:19,967
What a, damn, sudden mobilization.

1428
01:24:20,167 --> 01:24:20,447
Sudden.

1429
01:24:21,157 --> 01:24:28,607
Yeah, and imagine, ML guys can, like, serve their time, get all the military bonuses and, well...

1430
01:24:28,657 --> 01:24:30,887
Well, that's how it's been working in Israel for a long time.

1431
01:24:31,407 --> 01:24:36,707
You know they have a unit that does OSINT, programming, I don't remember what else.

1432
01:24:37,237 --> 01:24:43,507
Well, in short, people come out of there after four-year contracts as actual seniors, go to work for cool companies because they have great production experience.

1433
01:24:45,477 --> 01:24:46,547
So...

1434
01:24:46,547 --> 01:24:47,047
Yeah.

1435
01:24:47,387 --> 01:24:48,487
It'll be the same story.

1436
01:24:49,527 --> 01:24:51,247
Let's move on.

1437
01:24:51,347 --> 01:24:52,647
Next up, we have science and tech.

1438
01:24:52,847 --> 01:24:56,047
There are two related news items, actually.

1439
01:24:56,127 --> 01:24:57,067
One is a lead-in to the other.

1440
01:24:58,167 --> 01:25:00,307
Anyway, the first news, I really like it.

1441
01:25:00,567 --> 01:25:02,167
Lesha really loves the word "rolled out."

1442
01:25:02,927 --> 01:25:03,367
And this is...

1443
01:25:03,367 --> 01:25:04,287
Let's make some noise, it rolled out.

1444
01:25:05,007 --> 01:25:06,407
This is the first...

1445
01:25:06,407 --> 01:25:08,597
The first time in my life...

1446
01:25:08,636 --> 01:25:10,729
The first situation in history when this word is as appropriate as possible.

1447
01:25:11,786 --> 01:25:15,076
Because Lesha wrote, Tesla rolled out its first robotaxi.

1448
01:25:16,146 --> 01:25:22,456
Basically, a ride around the city, it's Musk, so it will cost us 4 dollars and 20 cents, that is 4.20.

1449
01:25:23,183 --> 01:25:24,020
Ahhh.

1450
01:25:24,020 --> 01:25:24,820
They'll get high.

1451
01:25:25,050 --> 01:25:25,460
Ohhh.

1452
01:25:26,090 --> 01:25:27,100
Well, of course.

1453
01:25:27,590 --> 01:25:31,140
The cars are currently only driving around Austin, where Tesla's headquarters is.

1454
01:25:31,370 --> 01:25:34,800
And for now, there's a person in the car, but in the passenger seat, just so you know.

1455
01:25:35,430 --> 01:25:41,700
But they promise that in '26 there will be autonomous Cybercabs, which won't even have a steering wheel or pedals.

1456
01:25:42,650 --> 01:25:44,560
I wonder how they will get that through legislation.

1457
01:25:44,626 --> 01:25:49,936
The presence of a steering wheel and pedals in vehicles is fixed in many laws.

1458
01:25:50,266 --> 01:25:50,996
I don't know which ones, though.

1459
01:25:51,376 --> 01:25:51,956
It seems so.

1460
01:25:52,706 --> 01:25:53,056
They'll lobby for it.

1461
01:25:53,226 --> 01:25:58,356
Well, it was difficult to bring the Cybertruck to Europe, partly because its steering wheel is non-standard, and other things.

1462
01:25:58,426 --> 01:26:00,296
Maybe Europe has such rules.

1463
01:26:00,816 --> 01:26:01,216
We'll see.

1464
01:26:01,626 --> 01:26:04,956
And the next news is, of course, both scary and funny.

1465
01:26:05,826 --> 01:26:11,196
If you thought self-driving taxis were still some kind of novelty, well, you're wrong.

1466
01:26:11,926 --> 01:26:17,596
In the USA, in San Francisco, on December 20th, there were rolling blackouts.

1467
01:26:17,826 --> 01:26:18,076
Why?

1468
01:26:18,256 --> 01:26:18,636
I don't know.

1469
01:26:18,776 --> 01:26:19,976
You can read about it in the original article.

1470
01:26:20,116 --> 01:26:24,236
But, nevertheless, the power went out in parts of the city.

1471
01:26:24,466 --> 01:26:28,316
125,000 homes were affected by these outages.

1472
01:26:28,506 --> 01:26:36,776
And a sudden consequence of these outages was that all the Waymo cars in the city just stopped right where they were, with their hazard lights on.

1473
01:26:36,826 --> 01:26:42,316
And because of these Waymo cars, practically all traffic came to a halt.

1474
01:26:42,996 --> 01:26:45,216
This happened for a very simple reason.

1475
01:26:45,356 --> 01:26:50,136
Waymo, naturally, as a robotaxi, communicates with central servers to monitor the situation,

1476
01:26:51,316 --> 01:26:56,996
to even read some edge cases that it doesn't understand.

1477
01:26:57,046 --> 01:27:04,716
And so, when the rolling blackouts started happening, Waymo decided it would be too unsafe to leave the cars on local autopilot.

1478
01:27:05,176 --> 01:27:14,756
And they just decided with one click to put all the cars on their hazard lights, which de facto led to the cars just stopping where they were driving and turning on their hazards.

1479
01:27:15,446 --> 01:27:19,076
Damn, they could have at least given them a command to park in the nearest spot.

1480
01:27:19,126 --> 01:27:24,396
Well, you know, when a rolling blackout happens, it's still a few hundred meters to the nearest parking spot.

1481
01:27:24,396 --> 01:27:29,776
If an accident happens because of Waymo at that moment, Waymo will face a ton of legal costs.

1482
01:27:30,016 --> 01:27:32,936
Probably much more than the cost of them blocking half the city.

1483
01:27:33,496 --> 01:27:41,276
But there are a ton of videos on Instagram where these Waymos are standing one after another, on turns, everyone is driving around them, it's just some kind of chaos going on.

1484
01:27:41,536 --> 01:27:44,936
And there you have it, they just shut off the electricity.

1485
01:27:45,416 --> 01:27:46,616
And the city is at a standstill.

1486
01:27:47,626 --> 01:27:53,036
Well yeah, but it's okay, it's an edge case, they'll fix it, and by the next outage they'll be driving fine.

1487
01:27:53,206 --> 01:27:55,036
Well, we'll see, we'll see.

1488
01:27:56,296 --> 01:27:57,776
Alright, with that, our...

1489
01:27:57,776 --> 01:28:00,306
Our main segments for today are finished.

1490
01:28:01,046 --> 01:28:05,146
We won't have an ethics section today, because last time we wanted to...

1491
01:28:05,146 --> 01:28:08,346
I wanted to end on a positive note, but we ended up finishing with murders.

1492
01:28:09,306 --> 01:28:13,766
We won't repeat that mistake this time.

1493
01:28:13,906 --> 01:28:25,126
Today we'll have a small block with statistics for '25, because many people have been summarizing stats, and now we'll go over a few of these stats and with that, we'll finish for this year.

1494
01:28:25,176 --> 01:28:27,266
Oh, and at the end, two little gifts await you.

1495
01:28:27,366 --> 01:28:33,106
One little gift and a teaser for our premium episode about Ave Maria, so don't go anywhere.

1496
01:28:33,956 --> 01:28:35,046
That's right, that's right.

1497
01:28:36,026 --> 01:28:43,566
So, we have statistics on how the number of users of popular AI chat services has changed over the year.

1498
01:28:44,116 --> 01:28:47,986
In the lead, who would you think, the magical ChatGPT.

1499
01:28:48,516 --> 01:28:49,886
You don't have to look far for that one.

1500
01:28:49,936 --> 01:28:50,286
Yep.

1501
01:28:50,486 --> 01:28:57,736
In January, they had almost 360 million users, and in November of '25, 810 million users.

1502
01:28:57,856 --> 01:28:59,426
Holy crap, almost a billion.

1503
01:28:59,716 --> 01:29:01,526
Almost a billion users.

1504
01:29:02,236 --> 01:29:05,566
And this is considering that these are almost all unique users.

1505
01:29:05,716 --> 01:29:10,146
But I can't imagine cases where you would need bots in ChatGPT.

1506
01:29:10,596 --> 01:29:12,986
Especially with their registration system.

1507
01:29:13,876 --> 01:29:14,946
Okay, okay, okay.

1508
01:29:15,646 --> 01:29:17,986
Next in popularity is Google Gemini.

1509
01:29:18,036 --> 01:29:27,486
But with a very large gap, they have 346 million users as of November '25, compared to 810.

1510
01:29:27,996 --> 01:29:30,186
Well, their growth rate is about the same.

1511
01:29:30,516 --> 01:29:31,526
Just a little bit lower.

1512
01:29:31,736 --> 01:29:33,126
Like, two times, two-something times less.

1513
01:29:33,256 --> 01:29:34,706
Yeah, the rate is the same.

1514
01:29:34,806 --> 01:29:35,346
That's how it is.

1515
01:29:35,876 --> 01:29:39,586
Next up, surprisingly, is 365 Copilot from Microsoft.

1516
01:29:39,876 --> 01:29:45,566
But, that's understandable, due to its integration into Microsoft products, the corporate environment, and so on.

1517
01:29:45,616 --> 01:29:47,866
Yeah, but their growth is interesting.

1518
01:29:47,976 --> 01:29:52,306
They had 218 million users in January '25.

1519
01:29:52,736 --> 01:29:55,746
And then in November '25, they had 212.

1520
01:29:56,416 --> 01:30:00,146
That looks a lot like how our Telegram chat is growing, to be honest.

1521
01:30:00,496 --> 01:30:01,186
By the way, yeah.

1522
01:30:01,426 --> 01:30:02,786
I was wondering where

1523
01:30:02,866 --> 01:30:03,426
that was coming from.

1524
01:30:03,876 --> 01:30:04,046
That's where.

1525
01:30:05,436 --> 01:30:09,146
Yeah.

1526
01:30:09,796 --> 01:30:11,286
But it dropped even further to 198, but then grew back again.

1527
01:30:11,346 --> 01:30:15,006
Their Bing is exactly the same.

1528
01:30:15,556 --> 01:30:18,726
If you remember, I implemented it too, and it surged, and then just fell.

1529
01:30:18,926 --> 01:30:20,236
Like it seems they roll something out, and everything drops for them.

1530
01:30:20,236 --> 01:30:20,946
Well, it's weird, but okay.

1531
01:30:21,436 --> 01:30:28,026
Most likely, yes.

1532
01:30:30,116 --> 01:30:38,486
But on the cool side, the cool thing is Perplexity is more popular than both Grok and Claude in terms of user count.

1533
01:30:38,916 --> 01:30:41,606
I was actually blown away by Perplexity, because Perplexity also showed the second-fastest growth rate.

1534
01:30:42,196 --> 01:30:44,886
They grew by three, almost four times.

1535
01:30:45,896 --> 01:30:52,866
That's faster than ChatGPT, faster than Gemini.

1536
01:30:53,356 --> 01:30:56,606
Well, as for them being more popular than Grok and Claude, with Grok it's understandable, Grok came out much later than Perplexity.

1537
01:30:57,116 --> 01:31:02,706
Perplexity has no real competitors in its niche, except for Google.

1538
01:31:02,976 --> 01:31:04,846
But the fact that Claude has so few users, Claude AI, I mean their chatbot, I didn't expect that.

1539
01:31:04,896 --> 01:31:06,306
I really thought I had some kind of weird...

1540
01:31:06,416 --> 01:31:07,646
Literally 11 million.

1541
01:31:07,696 --> 01:31:09,166
11 million, that's...

1542
01:31:09,256 --> 01:31:11,066
As of November '25.

1543
01:31:11,776 --> 01:31:11,986
80 times less than ChatGPT.

1544
01:31:12,716 --> 01:31:15,406
Yeah.

1545
01:31:16,916 --> 01:31:23,106
But at the same time, Claude from Anthropic makes money from their API.

1546
01:31:23,156 --> 01:31:23,886
But now I understand that their chat interface isn't just unpopular with me, it's not just me who doesn't pay for it.

1547
01:31:23,936 --> 01:31:26,946
But...

1548
01:31:28,136 --> 01:31:33,526
Grok, look at those numbers, it's insane.

1549
01:31:34,016 --> 01:31:38,206
Well, in January '25 it was a million, and that's strange, where that million even came from, to be honest.

1550
01:31:39,056 --> 01:31:40,166
But in November '25, it was already 34 million.

1551
01:31:40,216 --> 01:31:45,546
It grew 34 times.

1552
01:31:47,356 --> 01:31:48,686
Well, although, on the other hand, it was just appearing at the beginning of the year, so that's probably why the growth is so significant.

1553
01:31:49,016 --> 01:31:49,446
But Perplexity, good for them.

1554
01:31:51,876 --> 01:31:54,246
Good for them.

1555
01:31:54,376 --> 01:31:57,306
Anyway, write your thoughts, what do you use.

1556
01:31:58,176 --> 01:31:59,426
Do the statistics match up with what you use?

1557
01:32:00,136 --> 01:32:04,066
For me, it's a one-to-one match.

1558
01:32:06,976 --> 01:32:09,726
For me, first place is ChatGPT, second place is Gemini.

1559
01:32:10,588 --> 01:32:12,605
Grok, paid once, tried it.

1560
01:32:13,025 --> 01:32:16,665
Paid for X, didn't try it, don't use it.

1561
01:32:16,910 --> 01:32:19,740
Perplexity, used it last year, this year the free subscription ended, don't use it.

1562
01:32:20,910 --> 01:32:22,920
Gemini, when the free subscription ends, I probably won't use it either.

1563
01:32:24,046 --> 01:32:26,446
It's more or less one-to-one for me.

1564
01:32:27,086 --> 01:32:28,526
The only thing is that Revolut pays for my Perplexity.

1565
01:32:28,526 --> 01:32:30,246
But other than that...

1566
01:32:30,356 --> 01:32:30,846
Although no, I'll probably still pay for Gemini after all.

1567
01:32:31,966 --> 01:32:33,426
It writes good text.

1568
01:32:33,566 --> 01:32:36,646
Ok, thanks Revolut.

1569
01:32:37,006 --> 01:32:38,686
And notice, neither you nor I have used Microsoft Copilot 365.

1570
01:32:38,886 --> 01:32:40,566
Probably those who are on MacBooks.

1571
01:32:40,666 --> 01:32:40,816
I used it at work just for fun.

1572
01:32:41,286 --> 01:32:43,566
Oh, really?

1573
01:32:43,806 --> 01:32:45,406
Anyway, but I don't know, anyway.

1574
01:32:45,406 --> 01:32:45,606
Well, together for me... there.

1575
01:32:46,906 --> 01:32:47,306
Okay.

1576
01:32:47,846 --> 01:32:52,026
Then I'll talk about something I've wanted to talk about for a long time.

1577
01:32:52,076 --> 01:32:57,186
I've wanted to somehow bring up the phrase AI Slop in our podcast for a very long time.

1578
01:32:57,786 --> 01:32:59,446
Because you see it in a lot of places.

1579
01:33:00,586 --> 01:33:00,986
Slop.

1580
01:33:01,266 --> 01:33:01,646
Slop.

1581
01:33:01,866 --> 01:33:02,686
Slop, that's right.

1582
01:33:03,206 --> 01:33:05,306
Well, I don't know, I always say Slop.

1583
01:33:05,906 --> 01:33:07,226
Anyway, Slop, Slop.

1584
01:33:08,706 --> 01:33:10,086
A lot of people don't know what it is.

1585
01:33:10,226 --> 01:33:14,166
I actually didn't either, well, I knew the meaning, but I didn't know it meant "swill," first of all.

1586
01:33:14,306 --> 01:33:15,146
Slop, it's swill.

1587
01:33:15,196 --> 01:33:22,686
Secondly, this year, this word, this phrase and the word "Slop" are used to refer to low-quality digital content.

1588
01:33:22,836 --> 01:33:26,706
Mostly it refers to all sorts of crappy videos, images.

1589
01:33:27,016 --> 01:33:33,146
I've also seen it applied to text, to journalistic texts that are clearly made with AI.

1590
01:33:33,146 --> 01:33:40,246
In short, when you can clearly see that it's poorly made, and with the help of AI - that's Slop.

1591
01:33:40,246 --> 01:33:42,506
Quality AI content and Slop.

1592
01:33:42,556 --> 01:33:47,646
Because the word Slop existed before, and it meant crappy content.

1593
01:33:47,976 --> 01:33:54,826
So, the thing is, this year the dictionary Slop, we check every year what it recognizes.

1594
01:33:54,896 --> 01:33:56,446
Last year, I don't remember what they had.

1595
01:33:56,666 --> 01:33:57,846
I need to check the scripts.

1596
01:33:59,836 --> 01:34:00,886
Probably "transformer" or something.

1597
01:34:00,966 --> 01:34:04,266
Anyway, this year the word of the year is "slop."

1598
01:34:04,316 --> 01:34:09,786
And specifically in the meaning of mass-produced, low-quality digital content.

1599
01:34:09,836 --> 01:34:10,166
There.

1600
01:34:11,446 --> 01:34:13,446
Well, what times we live in...

1601
01:34:13,446 --> 01:34:19,266
Which is not surprising, considering the amount of AI content that has flooded us.

1602
01:34:19,746 --> 01:34:20,696
Probably, yes.

1603
01:34:20,926 --> 01:34:21,546
Probably, yes.

1604
01:34:21,966 --> 01:34:29,546
It just confuses me that of all the words, it was AI, AI Slop, that went viral.

1605
01:34:29,986 --> 01:34:32,806
Because, well, it says that AI has taken over everything.

1606
01:34:33,736 --> 01:34:46,706
Well, first, it has taken over everything, and second, it's something that has massively affected everyone, because our programming, yours and mine, is actually, like in that meme, useless compared to the amount of content being created.

1607
01:34:46,876 --> 01:34:58,666
Well, Vitya, last year, I checked, the word was polarization, like left-right polarization, the two-sided world, China-USA, confrontation.

1608
01:34:58,716 --> 01:34:59,696
Well, yeah.

1609
01:34:59,696 --> 01:35:07,666
I mean, there were some words related to AI, but not from Merriam-Webster. No, you just can't imagine how much of this content has appeared.

1610
01:35:07,966 --> 01:35:11,486
There's a lot of it, millions, millions of billions.

1611
01:35:11,976 --> 01:35:15,446
Well, yeah, we're the hosts of a super popular podcast about AI, and we don't understand.

1612
01:35:15,976 --> 01:35:16,226
Yes.

1613
01:35:17,616 --> 01:35:20,206
Even we don't understand what you're talking about.

1614
01:35:21,306 --> 01:35:22,136
Well, okay.

1615
01:35:22,686 --> 01:35:24,746
Well, you know, AI and Slop.

1616
01:35:24,796 --> 01:35:25,986
Yeah.

1617
01:35:26,906 --> 01:35:32,586
And let's finish with the last summary of the year.

1618
01:35:33,086 --> 01:35:37,326
Time magazine, as usual, chose its Person of the Year.

1619
01:35:38,386 --> 01:35:43,306
And hit like if you remember how Lesha was raging about Taylor Swift being chosen last year.

1620
01:35:43,356 --> 01:35:44,526
Oh, that was then, right?

1621
01:35:45,056 --> 01:35:45,266
Yes.

1622
01:35:45,606 --> 01:35:47,246
And Sam Altman.

1623
01:35:48,666 --> 01:35:54,246
This year, Time satisfied Lesha, I hope.

1624
01:35:54,296 --> 01:35:58,526
But they didn't choose Altman, but the AI Leaders of the Year.

1625
01:35:58,826 --> 01:36:01,966
And there's a cool picture of how they...

1626
01:36:01,966 --> 01:36:09,726
Probably everyone has seen that photo from a skyscraper construction site in New York, where workers are sitting high up on a beam and having lunch.

1627
01:36:09,826 --> 01:36:12,286
It's called "Lunch atop a Skyscraper."

1628
01:36:12,866 --> 01:36:17,046
And here they've photoshopped in these AI leaders.

1629
01:36:17,316 --> 01:36:18,866
I think it's a subtle hint.

1630
01:36:20,936 --> 01:36:21,826
Well, probably.

1631
01:36:22,416 --> 01:36:28,026
No, it's a subtle hint that they're sitting on a beam, and if that beam collapses, the whole world is screwed.

1632
01:36:28,776 --> 01:36:30,526
Ah, you think that's the hint.

1633
01:36:31,166 --> 01:36:32,506
Well, maybe that's part of it.

1634
01:36:32,746 --> 01:36:35,946
Anyway, long story short, who's there?

1635
01:36:36,026 --> 01:36:37,906
Altman, Dario Amodei,

1636
01:36:39,686 --> 01:36:40,866
Demis Hassabis...

1637
01:36:42,256 --> 01:36:43,826
Demis Hassabis, come on!

1638
01:36:43,946 --> 01:36:45,826
Demis Hassabis, I don't know him.

1639
01:36:46,156 --> 01:36:49,386
The director of Google, oh, not Google, but Google DeepMind.

1640
01:36:49,436 --> 01:36:52,226
Yeah, he's the director of Google DeepMind?

1641
01:36:52,366 --> 01:36:52,566
Yes.

1642
01:36:53,306 --> 01:36:55,386
What do you mean, we've talked about him more than once.

1643
01:36:55,986 --> 01:36:57,566
I don't remember him at all.

1644
01:36:58,066 --> 01:36:58,366
I don't remember.

1645
01:36:59,086 --> 01:37:01,726
Jensen Huang, Fei-Fei Li, Musk,

1646
01:37:02,826 --> 01:37:03,466
Lisa Su.

1647
01:37:03,726 --> 01:37:04,946
I don't remember her either.

1648
01:37:05,356 --> 01:37:14,106
Lisa Su is the CEO of AMD and, I think, yes, AMD and Huang's niece.

1649
01:37:14,266 --> 01:37:17,306
Oh, right, we talked about her once.

1650
01:37:17,586 --> 01:37:19,026
And Mark Zuckerberg.

1651
01:37:20,956 --> 01:37:21,786
Well, basically...

1652
01:37:21,786 --> 01:37:26,206
When I was reading this list, I only didn't remember Lisa Su.

1653
01:37:27,206 --> 01:37:30,846
You also literally didn't remember her, and you know Demis.

1654
01:37:31,046 --> 01:37:31,226
And Hassabis.

1655
01:37:31,646 --> 01:37:32,866
Well, like, I...

1656
01:37:32,866 --> 01:37:35,846
I agree, we've discussed all of them this year.

1657
01:37:36,046 --> 01:37:36,406
We even...

1658
01:37:36,406 --> 01:37:37,866
we even discussed Lisa Su.

1659
01:37:38,246 --> 01:37:38,946
They watch our podcast.

1660
01:37:38,946 --> 01:37:39,386
Yeah, well, once.

1661
01:37:39,606 --> 01:37:40,186
Once.

1662
01:37:40,626 --> 01:37:42,866
Who else would you add here from those who are missing?

1663
01:37:43,306 --> 01:37:44,206
Are there such people?

1664
01:37:44,706 --> 01:37:46,266
It's as if they forgot the Chinese.

1665
01:37:46,966 --> 01:37:47,986
As if...

1666
01:37:47,986 --> 01:37:49,046
Yeah, the Chinese in general...

1667
01:37:49,046 --> 01:37:49,386
Well, you know...

1668
01:37:49,416 --> 01:37:49,966
How could you forget?

1669
01:37:50,096 --> 01:37:51,926
There are two of them here, but they're Americans, in fact.

1670
01:37:52,176 --> 01:37:54,906
Well, I mean, the Chinese who are over there...

1671
01:37:54,956 --> 01:38:00,446
I don't know, maybe Time's Person of the Year isn't given to foreigners.

1672
01:38:00,736 --> 01:38:03,886
Or, if they did give it to the Chinese, I think there should have been...

1673
01:38:03,936 --> 01:38:05,026
You know who?

1674
01:38:05,166 --> 01:38:07,666
That Winnie-the-Pooh, what's his name?

1675
01:38:07,786 --> 01:38:08,426
Xi Jinping.

1676
01:38:10,856 --> 01:38:11,996
That was in bad taste, wasn't it?

1677
01:38:12,776 --> 01:38:14,646
Well, for the Chinese, probably in bad taste.

1678
01:38:14,716 --> 01:38:15,806
Chinese people don't watch us.

1679
01:38:16,136 --> 01:38:16,606
And why?

1680
01:38:16,656 --> 01:38:17,266
Why?

1681
01:38:17,426 --> 01:38:22,726
Because he is the collective image of AI opposition to the USA.

1682
01:38:24,636 --> 01:38:25,466
I think so.

1683
01:38:25,656 --> 01:38:29,246
Because if they put some director from Alibaba here, well, he's an Alibaba director.

1684
01:38:29,596 --> 01:38:35,406
Some main developer from Qwen, he could be replaced, because Qwen is a company under Alibaba.

1685
01:38:35,736 --> 01:38:37,306
The guy from DeepSeek?

1686
01:38:37,446 --> 01:38:37,686
Yes.

1687
01:38:38,636 --> 01:38:41,446
But everyone understands that this guy is under the thumb of the Communist Party.

1688
01:38:41,576 --> 01:38:45,006
If anything happens, the Communist Party will put this guy in his place, and there will be another guy there.

1689
01:38:45,176 --> 01:38:46,506
That's why, I think.

1690
01:38:48,196 --> 01:38:51,506
Well, maybe, maybe, yeah, maybe.

1691
01:38:52,696 --> 01:38:54,526
I'm missing LeCun here.

1692
01:38:54,876 --> 01:38:56,686
And it's obvious...

1693
01:38:57,956 --> 01:39:05,966
Let's put it this way, LeCun is missing, Geoffrey Hinton is missing, who's a Nobel laureate this year, and a Turing Award laureate last year.

1694
01:39:06,096 --> 01:39:09,166
In short, scientists are missing; they've put mostly business people here.

1695
01:39:09,396 --> 01:39:10,246
There are no scientists here.

1696
01:39:10,466 --> 01:39:15,966
And LeCun, he's also a businessman, I think they just canceled him because he left Facebook.

1697
01:39:17,456 --> 01:39:18,306
I don't know.

1698
01:39:18,706 --> 01:39:19,366
Something like that.

1699
01:39:19,526 --> 01:39:20,046
Maybe.

1700
01:39:20,466 --> 01:39:22,166
Oh, and Bezos isn't there?

1701
01:39:22,686 --> 01:39:23,706
No Bezos.

1702
01:39:23,846 --> 01:39:26,706
You could actually swap Zuckerberg for LeCun, by the way.

1703
01:39:28,166 --> 01:39:29,526
It would only get better.

1704
01:39:29,666 --> 01:39:42,506
Well, in general, Time magazine didn't just nominate the Person of the Year, they summed up a whole bunch of year-end results, and we've discussed all these results on the podcast, I'm telling you, so there's no need to name them now.

1705
01:39:42,556 --> 01:39:52,146
And moreover, I've skimmed through these results, and I can tell you that you will hear all these results soon.

1706
01:39:52,666 --> 01:39:53,726
But not from Time.

1707
01:39:54,676 --> 01:40:03,106
Yeah, not from Time, but in a special New Year's episode of the On Vibe podcast, which this time will be released not exactly on New Year's, but a little earlier.

1708
01:40:03,576 --> 01:40:05,176
We'll release it on the 29th, right?

1709
01:40:05,216 --> 01:40:09,486
Yes, so as not to distract you from the New Year's hustle and all that.

1710
01:40:10,006 --> 01:40:12,146
This time it's not a musical, unfortunately.

1711
01:40:12,556 --> 01:40:14,346
We kind of screwed up a little.

1712
01:40:14,646 --> 01:40:16,146
Or more precisely, me in particular.

1713
01:40:16,746 --> 01:40:17,766
There will be no musical.

1714
01:40:18,426 --> 01:40:20,006
We don't even sing in it.

1715
01:40:20,826 --> 01:40:22,526
We only put our songs in at the end.

1716
01:40:22,586 --> 01:40:24,646
Maybe more people will watch us this time.

1717
01:40:24,686 --> 01:40:25,706
Maybe, we'll see.

1718
01:40:26,036 --> 01:40:28,126
But at the end, as Vitya said, just wait.

1719
01:40:28,296 --> 01:40:30,106
There will be a collection of all the songs at the end.

1720
01:40:30,206 --> 01:40:33,186
You'll be able to play it and listen non-stop for a whole hour.

1721
01:40:33,586 --> 01:40:34,406
New Year's vibe.

1722
01:40:35,186 --> 01:40:40,766
Yes, and a lot of year-end summaries, a lot of predictions for next year, a lot of self-analysis.

1723
01:40:40,816 --> 01:40:43,846
In short, watch it, listen to it.

1724
01:40:44,296 --> 01:40:47,686
This year, our timing turned out to be even longer than last year.

1725
01:40:47,836 --> 01:40:51,386
Last year we sang a lot, but this year, without songs, our episode is almost 3 hours long.

1726
01:40:52,096 --> 01:40:55,146
So, yeah, guys, get ready.

1727
01:40:55,316 --> 01:40:59,126
You can watch it on the 29th, the 30th, or the 31st, it's 3 hours.

1728
01:40:59,216 --> 01:41:00,066
Whatever works for you.

1729
01:41:00,166 --> 01:41:01,246
Or you can watch for all 3 days.

1730
01:41:01,756 --> 01:41:03,486
Or you can just put it on repeat.

1731
01:41:03,706 --> 01:41:04,446
Just on repeat.

1732
01:41:04,656 --> 01:41:04,826
Yes.

1733
01:41:06,016 --> 01:41:10,226
Well, with that, we'll be saying goodbye for this year.

1734
01:41:12,316 --> 01:41:21,546
I remind you that at the end of this episode, after we say goodbye, a 6-minute teaser of our premium episode for premium subscribers, titled Project Hail Mary, awaits you.

1735
01:41:22,206 --> 01:41:23,586
I mean, the episode is called that.

1736
01:41:23,886 --> 01:41:25,526
But actually, it's a whole show.

1737
01:41:25,866 --> 01:41:29,886
Vitya will be talking about the awesome science fiction of Project Hail Mary.

1738
01:41:30,166 --> 01:41:33,426
Give it a listen, maybe you'll like it, and you'll want to support us as well.

1739
01:41:33,606 --> 01:41:34,846
With some cash and listen to this episode.

1740
01:41:35,296 --> 01:41:37,926
Yeah, only we don't have premium subscribers, Lesha.

1741
01:41:38,026 --> 01:41:40,446
Unlike everyone else, we have premium listeners.

1742
01:41:43,716 --> 01:41:45,766
And on that note, we'll be saying goodbye.

1743
01:41:46,406 --> 01:41:52,846
We probably won't wish you anything today, because there will be a New Year's episode where we, in principle, will also give our wishes, albeit crappily.

1744
01:41:55,406 --> 01:42:00,686
I must say, we didn't really act very New Year-y there, I think.

1745
01:42:00,986 --> 01:42:01,966
We were wearing hats.

1746
01:42:02,726 --> 01:42:04,586
But we were wearing hats and had a Christmas tree.

1747
01:42:04,726 --> 01:42:06,626
Well, what else do you need for New Year's?

1748
01:42:06,886 --> 01:42:07,466
That's it.

1749
01:42:08,256 --> 01:42:11,126
And last year we were also just singing songs at the same time.

1750
01:42:11,226 --> 01:42:11,826
Well, like, Vitya.

1751
01:42:11,966 --> 01:42:12,316
Well, yeah.

1752
01:42:12,686 --> 01:42:14,686
No, well, if you want, you can wish something for the New Year.

1753
01:42:14,746 --> 01:42:18,146
What if someone doesn't watch the New Year's episode, so we have to say something.

1754
01:42:18,886 --> 01:42:19,286
We have to.

1755
01:42:19,396 --> 01:42:28,886
Anyway, our dear listeners and our premium listeners, and all our listeners in general, I wish you happiness and more AGI in the New Year.

1756
01:42:29,206 --> 01:42:30,766
And less AI Slop.

1757
01:42:31,266 --> 01:42:36,066
Oh, and cheap and high-quality tokens for every home.

1758
01:42:36,396 --> 01:42:36,666
There you go.

1759
01:42:36,716 --> 01:42:37,606
Yes.

1760
01:42:39,246 --> 01:42:41,346
And as we say at the end,

1761
01:42:42,486 --> 01:42:43,506
lives.

1762
01:42:47,086 --> 01:42:51,326
But we're not recording this live, so who's going to write "on vibe" to us now?

1763
01:42:53,046 --> 01:42:56,366
Never mind "on vibe," I got it mixed up before.

1764
01:42:56,486 --> 01:42:58,026
Well, let's take a breath and say it.

1765
01:42:59,086 --> 01:42:59,686
Okay.

1766
01:43:00,306 --> 01:43:00,606
Alright.

1767
01:43:00,766 --> 01:43:05,706
And as we say, like, subscribe, on vibe.

1768
01:43:05,756 --> 01:43:06,466
On vibe.

1769
01:43:07,026 --> 01:43:08,326
It never works out.

1770
01:43:08,906 --> 01:43:11,026
What do you mean, mine was perfectly in sync.

1771
01:43:11,246 --> 01:43:12,306
Mine was like always.

1772
01:43:14,606 --> 01:43:15,306
Alrighty then.

1773
01:43:15,846 --> 01:43:18,826
Okay, see you in January then.

1774
01:43:19,886 --> 01:43:20,586
Uh-huh.

1775
01:43:21,086 --> 01:43:21,606
Bye-bye.

1776
01:43:23,956 --> 01:43:32,566
Today I want to tell you the plot of the book Project Hail Mary, which was written by Andy Weir.

1777
01:43:32,806 --> 01:43:34,266
Andy Weir, just so you know...

1778
01:43:34,266 --> 01:43:35,686
First, he's a software engineer.

1779
01:43:36,276 --> 01:43:42,346
Second, he's now a super-famous writer who wrote, in particular, The Martian and Artemis.

1780
01:43:43,426 --> 01:43:45,166
The Martian was even adapted into a movie.

1781
01:43:45,286 --> 01:43:47,786
You might have seen it with Matt Damon in the lead role.

1782
01:43:48,216 --> 01:43:51,076
Project Hail Mary, right?

1783
01:43:51,326 --> 01:43:54,366
I saw the trailer with that guy, with Ryan Gosling.

1784
01:43:54,586 --> 01:43:56,026
The trailer comes out next year.

1785
01:43:56,146 --> 01:43:56,806
It's awesome.

1786
01:43:56,856 --> 01:43:57,306
Spoiler.

1787
01:43:57,686 --> 01:44:07,406
It's a spoiler because the book is being adapted into a movie, and you're about to hear how it was in the book before the movie comes out, and then you can say, like, "Ah, it's not like that in the book, it's not like that in the book."

1788
01:44:09,416 --> 01:44:10,766
Damn, but the trailer is cool.

1789
01:44:11,026 --> 01:44:14,506
So, I just didn't know before the recording, it didn't click in my head that it was the same thing.

1790
01:44:14,836 --> 01:44:15,896
It is, yes.

1791
01:44:16,086 --> 01:44:21,246
For some reason, they didn't adapt his book Artemis, although it's also cool, it's about the Moon.

1792
01:44:21,296 --> 01:44:30,106
But the trailer looks like it's going to be a sappy story not about love, but about the friendship between an alien and a human.

1793
01:44:30,836 --> 01:44:36,426
You know, something like Groot and those, the Guardians of the Galaxy, but in a proper setting.

1794
01:44:36,616 --> 01:44:39,596
They reveal right away that there will be an alien in the trailer, right?

1795
01:44:39,696 --> 01:44:40,306
Yeah, yeah.

1796
01:44:40,736 --> 01:44:46,366
It's just that in the book, that only happens in the second quarter, I mean, nothing is clear at all there.

1797
01:44:46,476 --> 01:44:47,726
In short, we're not watching the movie.

1798
01:44:47,726 --> 01:44:48,776
That's it, we're not watching the movie.

1799
01:44:50,896 --> 01:44:51,856
Well, okay.

1800
01:44:52,276 --> 01:44:52,796
Alright.

1801
01:44:53,416 --> 01:44:56,016
And did they show the alien itself in the trailer?

1802
01:44:56,056 --> 01:44:56,496
They did.

1803
01:44:57,136 --> 01:44:58,296
And more than once.

1804
01:44:59,276 --> 01:45:00,116
No way.

1805
01:45:01,076 --> 01:45:03,056
It's just interesting, I haven't seen the trailer.

1806
01:45:03,056 --> 01:45:10,016
He looks like he's made of stone, if you've seen that Fantastic Four, remember there was a stone guy?

1807
01:45:10,296 --> 01:45:15,406
Well, he looks like a small, stone-like, kind of near-anthropomorphic creature, but you wouldn't say he's anthropomorphic.

1808
01:45:15,406 --> 01:45:16,956
But does he look like a spider or not?

1809
01:45:17,006 --> 01:45:17,656
He does, he does.

1810
01:45:17,816 --> 01:45:18,896
A kind of stone spider.

1811
01:45:19,406 --> 01:45:20,196
Yes, I said that...

1812
01:45:20,246 --> 01:45:21,656
Well, overall, overall it matches.

1813
01:45:24,146 --> 01:45:26,016
Anyway, Project Hail Mary.

1814
01:45:26,186 --> 01:45:27,136
What's the deal with the book?

1815
01:45:27,256 --> 01:45:31,056
The fact that it's modern, mind-blowing science fiction.

1816
01:45:31,266 --> 01:45:36,356
There are references to various studies for just tons of pages.

1817
01:45:38,606 --> 01:45:43,776
Some of it was really hard for me to even read, I'll be honest.

1818
01:45:44,526 --> 01:45:50,116
Not that I'm some big-shot physicist or chemist, but there's a lot about chemical and physical research.

1819
01:45:50,386 --> 01:45:52,716
And Weir had quite a few consultants.

1820
01:45:52,886 --> 01:45:59,796
That is, a lot of what's written in his books is done straight from scientific documents.

1821
01:45:59,986 --> 01:46:04,096
That's why I wouldn't say the book has a super-duper-mega plot.

1822
01:46:04,766 --> 01:46:08,786
There are practically no super plot twists or anything like that.

1823
01:46:09,696 --> 01:46:10,216
There.

1824
01:46:10,916 --> 01:46:11,436
But...

1825
01:46:11,436 --> 01:46:14,756
there is sci-fi in it.

1826
01:46:15,116 --> 01:46:18,396
It has sci-fi, there are a lot of science fiction things in it.

1827
01:46:18,956 --> 01:46:20,536
Oh, you even have "The Martian".

1828
01:46:20,696 --> 01:46:21,696
Even in Belarusian.

1829
01:46:22,006 --> 01:46:23,456
In Belarusian, no less, there you go.

1830
01:46:23,876 --> 01:46:24,716
Well, anyway.

1831
01:46:24,846 --> 01:46:26,916
Well, in terms of volume, it's about the same as "The Martian," by the way.

1832
01:46:29,576 --> 01:46:31,416
More or less, it's a one-to-one match.

1833
01:46:31,956 --> 01:46:32,736
So, there you go.

1834
01:46:32,966 --> 01:46:35,536
And that's what makes the book valuable, because...

1835
01:46:35,536 --> 01:46:39,416
Why am I talking about this now? Because now, in fact, I'm going to spoil everything for you.

1836
01:46:39,716 --> 01:46:41,216
This is an important clarification.

1837
01:46:41,236 --> 01:46:45,236
If you don't want spoilers, turn this off right away, because there will be spoilers from the first seconds.

1838
01:46:45,876 --> 01:46:52,836
But it will still be cool to read it yourself, even after I tell you about it, because I didn't remember everything.

1839
01:46:53,216 --> 01:46:59,776
And the valuable parts are the sci-fi details, of which I won't even be able to convey 90%, just so you know.

1840
01:46:59,916 --> 01:47:02,116
There will only be about 5 percent of them now, at most.

1841
01:47:02,746 --> 01:47:03,636
Anyway...

1842
01:47:03,636 --> 01:47:08,316
Listen, before you start, I'll add something here, I went on Weir's Wikipedia page.

1843
01:47:09,116 --> 01:47:10,736
The dude is basically one of us.

1844
01:47:10,916 --> 01:47:13,656
He is, as I understand it, a practicing programmer.

1845
01:47:14,136 --> 01:47:15,426
As far as I understand, yes.

1846
01:47:15,456 --> 01:47:16,756
Works in Mountain View.

1847
01:47:17,116 --> 01:47:20,256
And the dude worked at Blizzard, made Warcraft 2.

1848
01:47:21,816 --> 01:47:23,076
That's something I didn't know.

1849
01:47:23,126 --> 01:47:32,176
And the dude is a super-nerd, a geek, and a guy who has watched all the episodes of the series Doctor Who.

1850
01:47:32,456 --> 01:47:33,916
For that, he gets special respect.

1851
01:47:34,346 --> 01:47:35,376
Wow, wow.

1852
01:47:35,616 --> 01:47:37,016
So, those are the facts.

1853
01:47:37,076 --> 01:47:40,796
Especially for some seasons, major respect, because they are unwatchable.

1854
01:47:43,136 --> 01:47:45,776
Anyway, anyway, let's get to the book then.

1855
01:47:45,826 --> 01:47:46,656
Uh-huh.

1856
01:47:47,256 --> 01:47:50,716
Look, I'm going to tell it as I remember it.

1857
01:47:50,846 --> 01:47:52,396
To be honest, I don't have a single note.

1858
01:47:53,116 --> 01:47:56,316
If anything, just ask questions at any time.

1859
01:47:57,366 --> 01:48:05,316
Just so you know, I won't be able to answer many questions because, well, I either don't have enough knowledge, or I don't remember.

1860
01:48:05,416 --> 01:48:06,176
But go ahead and ask.

1861
01:48:06,456 --> 01:48:06,776
I will.

1862
01:48:07,346 --> 01:48:09,196
But I'm thinking, so as not to interrupt you at all.

1863
01:48:09,316 --> 01:48:11,296
You know I can interrupt every single word.

1864
01:48:20,726 --> 01:48:22,576
Anyway, how does the story begin?

1865
01:48:22,876 --> 01:48:28,016
It starts with some person, we don't know who, waking up in some bright room.

1866
01:48:28,776 --> 01:48:34,776
A whole bunch of sensors and catheters are connected to his body, and overall he feels like crap.

1867
01:48:35,156 --> 01:48:42,616
And he has no idea where he is, who he is, what he is, but he understands that he's in some kind of medical-ish room.

1868
01:48:43,176 --> 01:48:47,036
He tries to stand up, doesn't quite manage, and passes out.

1869
01:48:47,856 --> 01:48:50,556
Then after some time, he wakes up again.

1870
01:48:51,776 --> 01:48:55,856
He rips off these catheters, sensors, and stuff from himself.

1871
01:48:56,616 --> 01:49:01,576
He falls off this cot and notices that there are two more cots next to him.

1872
01:49:02,326 --> 01:49:05,996
And on them are people who are clearly already dead.

1873
01:49:06,156 --> 01:49:07,916
Two more dead people.

1874
01:49:08,776 --> 01:49:09,336
There.

1875
01:49:09,506 --> 01:49:13,996
And he's just freaking out, trying to somehow barely pull himself together.

1876
01:49:14,276 --> 01:49:15,636
He barely manages to do it.

1877
01:49:15,716 --> 01:49:17,976
He passes out again, then wakes up again.

1878
01:49:18,456 --> 01:49:23,076
And realizes that there's another exit in the room...

