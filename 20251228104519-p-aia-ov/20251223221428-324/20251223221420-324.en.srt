1
00:00:00,600 --> 00:00:01,980
A heavy episode today, huh?

2
00:00:02,860 --> 00:00:05,860
Yeah, and we don't even have that many pages in the script.

3
00:00:06,040 --> 00:00:13,060
By the way, my song takes up several pages there, more than the news in the big fish section, I think.

4
00:00:14,100 --> 00:00:16,680
So, how are you doing in general?

5
00:00:16,960 --> 00:00:18,800
I'm doing okay, in general.

6
00:00:19,060 --> 00:00:23,100
The only thing is, there's a lot of work towards the end of the year, and I'm totally swamped.

7
00:00:23,100 --> 00:00:30,480
I had to try really hard to avoid getting a bunch of evening meetings scheduled, so that I could record.

8
00:00:31,080 --> 00:00:32,240
See, the sacrifices I make.

9
00:00:32,500 --> 00:00:33,420
Appreciate it, please.

10
00:00:34,020 --> 00:00:39,700
By the way, leave a plus or write a comment if you also see a bit of Snape in Vitya.

11
00:00:39,880 --> 00:00:41,320
For some reason, he really reminds me of him.

12
00:00:42,200 --> 00:00:45,360
If I were you, I'd dress up as Snape for Halloween.

13
00:00:46,560 --> 00:00:49,240
I will definitely consider your suggestion.

14
00:00:49,300 --> 00:00:50,500
They also suggested Gogol to me.

15
00:00:50,500 --> 00:00:51,940
Well, that's if we're taking from the movies.

16
00:00:52,220 --> 00:00:52,920
Well, yeah, yeah.

17
00:00:53,040 --> 00:00:53,500
It's beautiful over there.

18
00:00:53,520 --> 00:00:54,860
I haven't seen it, to be honest.

19
00:00:58,340 --> 00:00:59,260
Anyway, I see.

20
00:00:59,680 --> 00:01:01,760
And you see, I started using it today...

21
00:01:01,760 --> 00:01:07,640
Those who are watching us have already noticed that today we're using a neural network to reduce my beard.

22
00:01:07,980 --> 00:01:11,020
Yeah, by the way, it's actually a filter, guys.

23
00:01:14,900 --> 00:01:21,840
Yeah, basically, my news is that I was hoping I'd have a new lease on life today, with the stitches removed.

24
00:01:21,880 --> 00:01:23,760
I was supposed to have surgery on my nose.

25
00:01:24,460 --> 00:01:24,980
Right.

26
00:01:25,640 --> 00:01:26,800
Exactly a week ago.

27
00:01:27,020 --> 00:01:31,740
And before the surgery, I had to shave because it's non-invasive, and a mustache could get in the way.

28
00:01:31,820 --> 00:01:35,740
And I shaved everything the hell off, confusing a centimeter-long razor guard with a millimeter-long one.

29
00:01:35,840 --> 00:01:39,120
They said the beard could be left at a centimeter, but the mustache had to go.

30
00:01:39,360 --> 00:01:42,040
So in the end, I basically did it all off.

31
00:01:43,120 --> 00:01:43,860
And there you go.

32
00:01:44,060 --> 00:01:45,440
I arrived at the hospital in the morning.

33
00:01:46,220 --> 00:01:49,520
And about two hours later they told me that my anesthesiologist was sick.

34
00:01:49,520 --> 00:01:51,640
The surgery is postponed until sometime in January.

35
00:01:51,800 --> 00:01:53,500
And I'm like, fantastic.

36
00:01:54,100 --> 00:01:58,240
I'll be walking around with a short beard until about February, basically.

37
00:01:58,400 --> 00:02:00,320
Or even without it.

38
00:02:00,400 --> 00:02:02,480
Oh, this Polish medicine.

39
00:02:03,560 --> 00:02:04,760
Private, by the way.

40
00:02:05,420 --> 00:02:06,040
And it's private, too.

41
00:02:06,040 --> 00:02:07,840
I was so pissed, I was so pissed.

42
00:02:07,940 --> 00:02:11,440
And you see, what jerks they are.

43
00:02:11,440 --> 00:02:13,760
I came in at 7 in the morning.

44
00:02:14,180 --> 00:02:15,900
They told me to arrive at 7 AM.

45
00:02:16,480 --> 00:02:19,360
I arrived, mentally prepared myself with my wife.

46
00:02:19,520 --> 00:02:21,180
They took my money right away.

47
00:02:21,560 --> 00:02:22,420
The full amount.

48
00:02:22,480 --> 00:02:23,760
It's not a cheap operation at all.

49
00:02:23,820 --> 00:02:26,700
Even though it's quite routine, it's very expensive.

50
00:02:26,960 --> 00:02:29,040
Then they kept me waiting in the clinic for 2 hours.

51
00:02:29,160 --> 00:02:30,820
I just sat there waiting, filling out some papers.

52
00:02:30,920 --> 00:02:31,620
And then I just waited.

53
00:02:31,660 --> 00:02:32,680
In the end, I just fell asleep.

54
00:02:33,480 --> 00:02:37,260
My two surgeons just didn't show up, because apparently they were told in advance.

55
00:02:37,260 --> 00:02:40,600
And then at 9:30, a woman comes up to me with an apologetic look on her face.

56
00:02:40,660 --> 00:02:44,220
This woman... Sorry, our anesthesiologist got sick.

57
00:02:45,120 --> 00:02:46,740
Basically, they told everyone except you.

58
00:02:47,000 --> 00:02:47,180
Yeah.

59
00:02:47,460 --> 00:02:48,660
If I understand correctly.

60
00:02:48,780 --> 00:02:53,300
And I'm like, guys, damn it, I had tests done, a ton of tests.

61
00:02:53,640 --> 00:02:58,900
I hope you'll reschedule my surgery for like a week from now, so I don't have to do all this again, so we can finish this year.

62
00:02:58,920 --> 00:03:01,520
She's like, ha-ha-ha, yes, of course, what do you mean, everything will be done this year.

63
00:03:02,120 --> 00:03:07,140
In the end, just yesterday, a whole week later, they informed me that I think it will all be next year.

64
00:03:07,320 --> 00:03:08,040
It's unknown when.

65
00:03:08,980 --> 00:03:11,040
Naturally, the tests will have to be redone.

66
00:03:13,460 --> 00:03:15,180
The anesthesiologist also called me in the evening.

67
00:03:15,240 --> 00:03:16,060
He apologized profusely.

68
00:03:16,380 --> 00:03:19,180
Turns out his child got sick, and so he decided not to come.

69
00:03:19,340 --> 00:03:20,360
Well, that's the right thing to do, in principle.

70
00:03:20,900 --> 00:03:21,620
Absolutely the right thing.

71
00:03:21,620 --> 00:03:24,600
And you'll have to shave everything the hell off again.

72
00:03:25,780 --> 00:03:27,080
I'll have to shave everything the hell off.

73
00:03:27,240 --> 00:03:31,460
But what's most offensive is knowing that in private clinics, what did they decide?

74
00:03:31,600 --> 00:03:33,980
What was their choice, clearly?

75
00:03:34,080 --> 00:03:42,560
Either leave one guy, who actually took it quite well when he was told someone got sick, very unhappy, and that's me.

76
00:03:42,720 --> 00:03:45,780
I mean, I didn't make a scene or anything, just said, please do it faster.

77
00:03:46,300 --> 00:03:51,260
They probably decided they could walk all over him, so we'll really upset him this year.

78
00:03:51,260 --> 00:03:56,300
But at least we won't upset the other 20 people who are already scheduled.

79
00:03:56,380 --> 00:03:58,720
In the end, nobody got rescheduled, but I did.

80
00:03:58,800 --> 00:04:00,580
That's what's offensive, you see?

81
00:04:00,600 --> 00:04:01,020
Yeah, wow.

82
00:04:01,760 --> 00:04:06,560
Like in my just universe, if you screw up, then you, I don't know, somehow...

83
00:04:07,070 --> 00:04:09,860
Either you reschedule everyone, or you work overtime.

84
00:04:09,940 --> 00:04:14,380
You're a private clinic, after all, not a state-run one where you come in and they do everything for free.

85
00:04:14,440 --> 00:04:20,160
Yeah, or like, come to my house, patch me up in the evening, as overtime.

86
00:04:22,560 --> 00:04:24,920
Anyway, I'm upset and have been all week.

87
00:04:25,240 --> 00:04:29,040
My friends here have already given me a new name, IT Half-Beard.

88
00:04:31,240 --> 00:04:31,880
It fits.

89
00:04:32,220 --> 00:04:51,040
So, dear listeners, to make our winter mood a little bit better, come join our podcast, come to our chat, become our premium listeners, because we already have for you there not only our pleasant faces,

90
00:04:51,260 --> 00:04:57,160
Vitya's and Lesha's, but there's also a small community, we've even recorded two exclusive pieces of content for you.

91
00:04:57,960 --> 00:05:02,280
Which are available only to premium listeners, exclusively.

92
00:05:02,940 --> 00:05:16,740
Don't forget, though, that donations, well, not donations but subscriptions, are first and foremost to support the podcast, but we enjoy recording episodes for that support, so a whole true crime podcast and a book summary are already waiting for you there.

93
00:05:17,120 --> 00:05:19,900
I think we might record something else before the New Year.

94
00:05:19,900 --> 00:05:20,800
There.

95
00:05:21,380 --> 00:05:25,220
Well, I've almost finished my book.

96
00:05:26,600 --> 00:05:28,860
About 30 percent left.

97
00:05:29,360 --> 00:05:32,800
So maybe I'll finish it by the next episode.

98
00:05:32,860 --> 00:05:34,580
We'll see, and I'll summarize it.

99
00:05:35,760 --> 00:05:39,960
I wanted to make a pun, but I thought today's not the mood for puns.

100
00:05:40,760 --> 00:05:44,820
I wanted to say, why read your own book if you wrote it.

101
00:05:44,820 --> 00:05:47,000
Ah, in that sense.

102
00:05:48,240 --> 00:05:52,660
Unfortunately... Well, although I have mixed feelings now.

103
00:05:52,780 --> 00:05:54,940
I'm not super, super into it anymore.

104
00:05:55,100 --> 00:05:59,060
It has a lot... It's Andy Weir, called Project Hail Mary.

105
00:05:59,440 --> 00:06:00,640
It's something fantastic.

106
00:06:01,340 --> 00:06:06,200
It's science fiction, in some places even a bit hardcore.

107
00:06:06,380 --> 00:06:14,580
There are under 400 references for the whole book, to all sorts of incomprehensible terms, scientific studies, and so on.

108
00:06:14,580 --> 00:06:15,880
Sounds very cool.

109
00:06:16,680 --> 00:06:21,040
Well, and because of that, there's unfortunately plenty of tediousness, but overall it's interesting.

110
00:06:21,520 --> 00:06:30,600
I think it will be interesting for you to listen to the summary as well, especially since it's important to do it before the movie comes out, because they're already adapting this book.

111
00:06:31,080 --> 00:06:34,180
Wait, isn't that the one with Ryan Gosling in it?

112
00:06:35,180 --> 00:06:38,660
I think Ryan Gosling is in everything, Lesha, I...

113
00:06:38,660 --> 00:06:46,320
Well, actually, when you say a science fiction book is dense, I think that's a plus, not a minus for a science fiction book, in my opinion.

114
00:06:46,720 --> 00:06:51,460
Anyway, to sum up, come join the premium, you'll make us happy and support the podcast.

115
00:06:51,540 --> 00:06:55,000
This is very important, since we are now working exclusively thanks to your support.

116
00:06:55,400 --> 00:07:02,740
If you happen to own a company or a product that you want to bring to our podcast for us to discuss, come on over too.

117
00:07:02,900 --> 00:07:05,320
We welcome partnerships.

118
00:07:05,340 --> 00:07:06,540
Our podcast is not small.

119
00:07:06,780 --> 00:07:10,980
For now, it's the largest I've seen in Russian about artificial intelligence.

120
00:07:10,980 --> 00:07:16,100
So you won't be disappointed, and we'll be... and we'll be waiting for you.

121
00:07:17,300 --> 00:07:20,920
For now it's the biggest, but maybe it will become huge-pre-huge, we'll see.

122
00:07:21,320 --> 00:07:21,960
Yeah, yeah.

123
00:07:22,760 --> 00:07:24,080
We'll see how it goes.

124
00:07:24,900 --> 00:07:30,400
Here they go again, I turned on the podcast, and they're talking about some nonsense for 10 minutes.

125
00:07:31,140 --> 00:07:32,400
Yeah, comments like that have started coming in.

126
00:07:32,680 --> 00:07:33,880
I'll bleep that out, of course.

127
00:07:34,260 --> 00:07:37,860
By the way, do you know where the unbleeped, completely uncensored version comes out?

128
00:07:38,520 --> 00:07:40,360
That's right, in our premium chat.

129
00:07:41,600 --> 00:07:47,400
All subscribers, in addition to extra content, also get an uncensored version of our podcast without background music.

130
00:07:47,500 --> 00:07:50,600
Just as we record it, completely uncut, that's how we post it.

131
00:07:50,660 --> 00:07:53,960
By the way, not all of our listeners know that we cut a lot of things from the episodes.

132
00:07:54,840 --> 00:07:57,320
Whale sounds, swearing, and other stuff.

133
00:07:57,440 --> 00:08:00,600
So you can see all of that in our premium chat.

134
00:08:00,940 --> 00:08:07,380
And for the biggest subscription, a real patron-level one, you'll also get a year's subscription to Code Evolution.

135
00:08:07,640 --> 00:08:09,220
It's a club for learning programming.

136
00:08:09,900 --> 00:08:10,500
That's all.

137
00:08:11,580 --> 00:08:15,500
Yeah, guys, a very important point, because I realized that some people get confused.

138
00:08:16,040 --> 00:08:26,000
Over there... when you go there, and I'm sure you will go to donate to us, you will see a different amount and subscription levels.

139
00:08:26,400 --> 00:08:32,240
Just putting in a different amount doesn't affect your presence in the chat.

140
00:08:32,500 --> 00:08:35,040
If that amount is less than 10 euros...

141
00:08:35,040 --> 00:08:45,060
If that amount corresponds to the donation cost, and it's recurring, like, if you enter a different amount, say, 11 dollars monthly, then either I or Vitya will manually come and add you to the chat.

142
00:08:45,200 --> 00:08:47,800
Everything else... well, for donations, thank you very much for...

143
00:08:48,320 --> 00:08:49,280
Yeah, something like that.

144
00:08:49,500 --> 00:08:50,460
Damn, Vitya, but don't...

145
00:08:50,460 --> 00:08:50,900
Do you hear that?

146
00:08:51,380 --> 00:08:52,280
Yeah, yeah, I hear it.

147
00:08:52,340 --> 00:08:53,520
Well, it doesn't matter, it's not too loud.

148
00:08:54,020 --> 00:09:04,020
Don't you have a bit of a cringey feeling that now we have to ask for donations, like, we didn't do that before, and now it's like we're some kind of hustlers trying to sell something, to peddle something?

149
00:09:05,680 --> 00:09:15,460
Well, listen, I had that feeling at first, but then I realized that we're not just begging for donations, we're also giving people cool content.

150
00:09:15,720 --> 00:09:17,440
I would say, first and foremost, first and foremost.

151
00:09:17,520 --> 00:09:26,800
I just listen to podcasts, and in all podcasts, they always have a premium chat and subscriptions, and they're so good at describing it all, they talk about it so energetically.

152
00:09:26,800 --> 00:09:30,960
Whenever I listen to it all, I never fast-forward, because, well, it turns out to be quite upbeat.

153
00:09:31,600 --> 00:09:34,660
I rarely subscribe, actually, but I don't mind listening.

154
00:09:35,920 --> 00:09:38,240
And I realize that I can't do it that way for some reason.

155
00:09:38,880 --> 00:09:41,000
It's good that you can...

156
00:09:41,000 --> 00:09:46,500
I'll tell you more, I've recently gotten a bit into the "lower internet."

157
00:09:46,760 --> 00:09:49,020
Give a like if you know what that is.

158
00:09:49,260 --> 00:09:52,340
And I came across a streamer called Gabzavr.

159
00:09:54,560 --> 00:10:00,340
Anyway, it's a long story, but that guy really knows how to ask for donations.

160
00:10:01,200 --> 00:10:02,580
Gabzavr, holy crap.

161
00:10:03,000 --> 00:10:03,880
Gabzavr, yes.

162
00:10:04,620 --> 00:10:06,340
Like if you know Gabzavr.

163
00:10:07,560 --> 00:10:08,080
Alright.

164
00:10:08,340 --> 00:10:08,700
What's up?

165
00:10:08,720 --> 00:10:11,220
Come join our chat too, it's completely free.

166
00:10:11,280 --> 00:10:13,600
It's called "na Vaibe fanchatik" (On the Vibe fan chat) in Telegram.

167
00:10:13,860 --> 00:10:16,120
You can find it simply at "podcast OnVibe."

168
00:10:16,320 --> 00:10:19,520
And subscribe to our Telegram channel.

169
00:10:19,620 --> 00:10:24,380
We have a Telegram channel where notifications about all new episodes and all new content are posted.

170
00:10:24,480 --> 00:10:27,280
It can be found simply at "OnVibe" in Telegram.

171
00:10:28,080 --> 00:10:31,880
Well, and if you don't know, all the links are on the website OnVibe.ru.

172
00:10:31,880 --> 00:10:32,500
Couldn't be simpler.

173
00:10:34,220 --> 00:10:35,460
Yeah, alright.

174
00:10:35,760 --> 00:10:37,080
Anything else to tell?

175
00:10:38,060 --> 00:10:40,700
Yeah, I think that's enough rambling.

176
00:10:40,900 --> 00:10:41,600
We've really gone on.

177
00:10:42,140 --> 00:10:42,420
Oh-ho-ho.

178
00:10:42,920 --> 00:10:45,560
Ah, and also, guys, an important announcement.

179
00:10:45,820 --> 00:10:47,680
A New Year's episode awaits you this year.

180
00:10:48,220 --> 00:10:53,520
That is, we had... We thought maybe it wouldn't work out very well, but it's working out.

181
00:10:55,320 --> 00:10:56,520
Yeah, yeah, yeah.

182
00:10:56,780 --> 00:11:01,640
Well, it's more of an announcement for us, our audience is already used to us doing it for the third year.

183
00:11:01,640 --> 00:11:06,740
Yeah, it's just that no one knows we were debating whether to record it or not.

184
00:11:08,360 --> 00:11:10,660
But here's a little update for you.

185
00:11:11,040 --> 00:11:13,680
We'll tell you about our new bets.

186
00:11:14,040 --> 00:11:18,960
Listen, do you know about Polymarket, a crypto exchange where they place bets on AI events?

187
00:11:19,540 --> 00:11:20,520
No, I don't.

188
00:11:20,960 --> 00:11:23,860
Anyway, there's this place on the internet called Polymarket.

189
00:11:24,300 --> 00:11:28,080
There you can use cryptocurrency to bet on all sorts of events around AI.

190
00:11:28,080 --> 00:11:33,580
Recently, a guy from Google, by betting a million dollars in total, won all his bets and earned another million dollars.

191
00:11:34,060 --> 00:11:35,280
Well, clearly insider trading.

192
00:11:35,400 --> 00:11:35,940
What am I getting at?

193
00:11:36,020 --> 00:11:44,020
I'm saying that I figured if we had placed our predictions on Polymarket for the last two years, you and I would have been in the black, not the red.

194
00:11:44,200 --> 00:11:46,080
Most of our bets would have paid off.

195
00:11:46,500 --> 00:11:49,600
Listen, maybe we should create a mechanism like that, where we place bets?

196
00:11:50,020 --> 00:11:51,120
We could try.

197
00:11:52,400 --> 00:11:55,500
We could place some small bets for fun.

198
00:11:55,500 --> 00:11:57,400
We can bet the subscribers' donations.

199
00:11:58,520 --> 00:12:01,040
Yeah, that's how we'll just blow it all.

200
00:12:01,360 --> 00:12:06,460
We do have a small crypto part, some amount already, 100-150 dollars, we could bet that, why not?

201
00:12:06,520 --> 00:12:10,160
We might as well stream online casino sessions.

202
00:12:12,860 --> 00:12:18,420
We'll be like that, what's-his-name, my god, what's that king of online casinos called?

203
00:12:18,680 --> 00:12:19,180
Kats, Maxim.

204
00:12:19,180 --> 00:12:20,100
He's from Gomel, too.

205
00:12:21,580 --> 00:12:24,140
No way, what Kats, the guy from Gomel.

206
00:12:24,140 --> 00:12:25,100
The only one I know from Gomel.

207
00:12:25,660 --> 00:12:26,600
A famous streamer.

208
00:12:26,600 --> 00:12:28,300
— Pasha Technik, he's dead already.

209
00:12:28,760 --> 00:12:33,040
— Damn, the most famous Russian-speaking streamer.

210
00:12:33,720 --> 00:12:34,280
The most.

211
00:12:35,200 --> 00:12:38,380
— Vitya, as if I watch streamers, I don't know.

212
00:12:38,560 --> 00:12:39,460
— Well damn, everyone knows him.

213
00:12:39,480 --> 00:12:40,700
— Pasha Technik, he's dead already.

214
00:12:41,340 --> 00:12:43,100
— No, what Pasha Technik?

215
00:12:43,940 --> 00:12:49,100
Damn, he even announced a contest recently, like, he was raffling off an apartment.

216
00:12:49,125 --> 00:12:51,745
Anyway, ah, I know who you're talking about, yeah.

217
00:12:52,680 --> 00:12:54,220
Mellstroy, they wrote in the chat.

218
00:12:54,260 --> 00:12:54,960
Mellstroy, Mellstroy, yeah.

219
00:12:55,700 --> 00:12:56,400
He's from Gomel.

220
00:12:56,400 --> 00:12:57,300
Why do I know about him?

221
00:12:57,640 --> 00:12:59,940
Well, I would say that...

222
00:12:59,940 --> 00:13:02,220
Alright, yeah, he's from Gomel, he's from Gomel.

223
00:13:02,960 --> 00:13:04,680
He only streams casino games all the time.

224
00:13:05,920 --> 00:13:07,020
Let's get started already.

225
00:13:07,960 --> 00:13:12,640
I feel like I'm getting sick, I see you're already sick, it's time to stream, or we won't make it to the end.

226
00:13:13,140 --> 00:13:14,600
Yeah, let's go, let's go.

227
00:13:15,080 --> 00:13:16,200
Yeah, guys, the song.

228
00:13:16,380 --> 00:13:17,300
A song will be out today.

229
00:13:17,300 --> 00:13:21,400
I took the band "Elektroslabost" (Electric Weakness) with their wonderful composition about a deer's penis.

230
00:13:21,900 --> 00:13:28,420
It's a cool song about the Battle of the Psychics and a mage who uses a deer's penis as a totem.

231
00:13:28,700 --> 00:13:30,280
See how I'm going to bleep this?

232
00:13:30,320 --> 00:13:31,900
We have Rodion and a child watching.

233
00:13:31,980 --> 00:13:33,220
How am I going to bleep this?

234
00:13:33,340 --> 00:13:35,800
What, is it a swear word?

235
00:13:38,260 --> 00:13:39,740
Well, I don't think so.

236
00:13:40,880 --> 00:13:42,700
Well, okay, let's assume.

237
00:13:42,980 --> 00:13:43,800
An ordinary word.

238
00:13:43,800 --> 00:13:52,260
Anyway, it turned out that the phrase "Ilya Sutskever" fits perfectly over the phrase "deer's penis."

239
00:13:52,260 --> 00:13:53,720
Like, a perfect match.

240
00:13:55,380 --> 00:14:09,780
So, based on the news... based on the article... not an article, but Ilyukha's interview that came out recently, which you all probably watched, Gemini wrote me a wonderful song.

241
00:14:09,780 --> 00:14:18,400
I must apologize, though, because I'm in such a state today that I'll probably sing it cringe-worthily, with difficulty.

242
00:14:18,740 --> 00:14:20,940
So it will be even worse than usual.

243
00:14:21,240 --> 00:14:22,380
But I'll try my best.

244
00:14:22,700 --> 00:14:23,460
Well, let's go.

245
00:14:47,680 --> 00:14:51,720
And in a minute, Ilya is on stage.

246
00:14:52,160 --> 00:14:58,780
And this is not Sam Altman the villain at all, but Sutskever saving people.

247
00:14:59,400 --> 00:15:02,320
He enters with code and a dataset.

248
00:15:02,640 --> 00:15:06,340
He sketches AGI and moves towards the goal.

249
00:15:06,340 --> 00:15:10,400
Ilya Sutskever, my ideal.

250
00:15:10,400 --> 00:15:13,800
He created Safe Super Intelligence.

251
00:15:14,000 --> 00:15:20,500
Ilya Sutskever, he knows the track, what our smart superhuman will be like.

252
00:15:20,660 --> 00:15:24,340
I do not accept closed labs.

253
00:15:24,420 --> 00:15:27,440
Code in closed banks does not help.

254
00:15:27,640 --> 00:15:30,660
Ilya's approach, that's my answer.

255
00:15:31,080 --> 00:15:34,500
He promoted it for many years.

256
00:15:34,500 --> 00:15:39,940
Just AGI, launched into the economy, is like a 15-year-old teenager.

257
00:15:40,120 --> 00:15:44,400
It gives a huge boost to productivity and minus 100 to the risk of a sudden seizure of power.

258
00:15:46,620 --> 00:15:50,780
The race participants are going further and further.

259
00:15:51,100 --> 00:15:54,200
Hiding models in a secret corner.

260
00:15:54,580 --> 00:15:57,260
Google is falling over on TPUs.

261
00:15:57,940 --> 00:16:00,560
And Meta is hiding its entire database.

262
00:16:01,160 --> 00:16:04,260
They fly in basements into a closed attic.

263
00:16:04,260 --> 00:16:07,740
Everyone wants power to make a mess.

264
00:16:07,920 --> 00:16:11,200
Ilya didn't fight with all this closedness.

265
00:16:11,440 --> 00:16:13,100
He just left.

266
00:16:13,460 --> 00:16:15,740
And SSI is being created.

267
00:16:16,560 --> 00:16:19,000
And he sings a song.

268
00:16:19,000 --> 00:16:25,820
My agents and their voices The heavens favor them.

269
00:16:25,980 --> 00:16:30,840
Oh, copies and clones in training Merge their experience.

270
00:16:31,040 --> 00:16:32,820
In one instant.

271
00:16:32,960 --> 00:16:39,600
Penetration into any job They cannot be hidden in seclusion.

272
00:16:39,600 --> 00:16:43,080
Only openness, not in confinement.

273
00:16:43,180 --> 00:16:46,620
Forgive me, those who believe in limitations.

274
00:16:48,500 --> 00:16:52,380
Secrets in the basement didn't help.

275
00:16:52,620 --> 00:16:55,700
Without society, people lost control.

276
00:16:55,700 --> 00:16:59,080
And he made a lot of noise in the media.

277
00:16:59,200 --> 00:17:02,620
That risk that arose from utter confusion.

278
00:17:02,800 --> 00:17:05,740
It was not a secret and not a terrible project.

279
00:17:05,740 --> 00:17:10,000
But simply such a reasonable intellect.

280
00:17:16,600 --> 00:17:18,360
Yo-yo-yo-yo-yo!

281
00:17:18,400 --> 00:17:19,380
Hello everyone!

282
00:17:19,640 --> 00:17:24,860
You are listening to the 324th episode of the best podcast about artificial intelligence news.

283
00:17:25,000 --> 00:17:26,760
The OnVibe podcast.

284
00:17:26,900 --> 00:17:32,000
And with you are its permanent hosts, the young man on the left, Viktor Shelenсhenko.

285
00:17:32,000 --> 00:17:33,220
Hello everyone!

286
00:17:33,560 --> 00:17:36,960
And the young man on the right, pay attention to the size of his beard.

287
00:17:37,080 --> 00:17:38,940
For those who are listening, not watching.

288
00:17:39,600 --> 00:17:40,440
Alexey Kartynnik.

289
00:17:40,460 --> 00:17:41,640
For those who are listening, not watching, yes.

290
00:17:41,760 --> 00:17:41,920
Hello!

291
00:17:42,680 --> 00:17:47,640
For those who are listening, pay attention to how different my voice sounds when my beard is almost gone.

292
00:17:48,660 --> 00:17:49,140
Mhm.

293
00:17:49,620 --> 00:17:51,200
Nothing rubs anymore.

294
00:17:51,440 --> 00:17:52,080
Yeah, nothing rubs.

295
00:17:52,320 --> 00:17:53,540
By the way, it's very convenient without a beard.

296
00:17:53,680 --> 00:17:55,540
Nothing gets stuck in it.

297
00:17:55,780 --> 00:17:56,380
I'll think about it.

298
00:17:58,300 --> 00:18:02,540
Well, shall we tell you today about the news of the last two weeks.

299
00:18:02,620 --> 00:18:05,420
By the way, there wasn't a lot of really big news.

300
00:18:05,820 --> 00:18:07,200
Much less than last time.

301
00:18:08,040 --> 00:18:13,360
So today we can talk about our usual, you know, everyday AI topics.

302
00:18:13,660 --> 00:18:15,700
I liked the message in the chat.

303
00:18:15,780 --> 00:18:16,980
I'm working, guys.

304
00:18:17,600 --> 00:18:18,700
Totally envious.

305
00:18:18,980 --> 00:18:20,180
And why did you like it?

306
00:18:20,240 --> 00:18:22,980
You said it as if, you know...

307
00:18:22,980 --> 00:18:27,420
As if it's... I said it with sadness because it's not our job yet.

308
00:18:27,560 --> 00:18:29,880
Already, it's not our job anymore, but for now.

309
00:18:29,980 --> 00:18:32,380
Yeah, we do this in our free time.

310
00:18:33,080 --> 00:18:34,420
Our position is very interesting.

311
00:18:34,480 --> 00:18:36,580
You and I now have this Schrödinger's job.

312
00:18:36,640 --> 00:18:40,140
It once was a job, and it's not a job anymore, but for now it's not a job.

313
00:18:40,220 --> 00:18:42,620
It's like our hobby that we do for you.

314
00:18:42,720 --> 00:18:42,980
Well, yeah.

315
00:18:43,120 --> 00:18:44,280
And we are grateful for your support.

316
00:18:45,180 --> 00:18:45,500
Alright.

317
00:18:45,960 --> 00:18:46,980
That's right, that's right.

318
00:18:47,860 --> 00:18:49,240
What do we have today?

319
00:18:49,540 --> 00:18:52,880
Let's start, as usual, with the big fish, because we've had some big releases.

320
00:18:52,880 --> 00:18:54,040
Yeah,

321
00:18:59,400 --> 00:19:03,900
there are plenty of big fish and they didn't have big releases.

322
00:19:04,360 --> 00:19:24,940
This time we're not starting with OpenAI, although we'll have them too, but with Anthropic, because, as you all probably know, but we'll talk about it anyway, Claude Opus 4.5 was released, a very cool model for coding, which is crushing all the benchmarks.

323
00:19:25,160 --> 00:19:28,840
Not really crushing them that much, but it pulls ahead in many places.

324
00:19:29,800 --> 00:19:34,540
I've already tried it, it coded a whole little game for me.

325
00:19:34,840 --> 00:19:36,340
It was very nice.

326
00:19:37,620 --> 00:19:44,440
It's for my SDVG application, there will be an electronic fidget toy where you need to move various tiles around.

327
00:19:44,440 --> 00:19:46,200
Damn, you're already cranking out games in DD?

328
00:19:46,780 --> 00:19:47,300
Awesome.

329
00:19:47,500 --> 00:19:56,720
Well, it's necessary, it will be... It's not so much a game as a replacement for a fidget thingy like a spinner, yeah, but to make it more interesting, you move these little squares around.

330
00:19:58,920 --> 00:20:02,100
So, it wrote it for me, and quite well, by the way.

331
00:20:03,700 --> 00:20:09,340
I don't even know if it's worth talking about their benchmarks, but everything is more or less on par.

332
00:20:09,540 --> 00:20:16,320
I would say that, well, you can't say it's the absolute coolest model for coding, yeah, it's good.

333
00:20:16,820 --> 00:20:21,020
Some people are even saying they're going back to Claude for programming.

334
00:20:21,020 --> 00:20:25,500
Well, remember there was a time when everyone was programming on CNET 4.5, 3.5, 4.5.

335
00:20:25,820 --> 00:20:28,180
Then everyone switched to OpenAI's Codex.

336
00:20:29,320 --> 00:20:31,020
Now it seems like they're coming back.

337
00:20:31,340 --> 00:20:32,640
But not in droves.

338
00:20:32,780 --> 00:20:38,560
Especially since we have news later about an update to OpenAI's model, so somehow, I don't know.

339
00:20:39,240 --> 00:20:40,860
It's strange for now, strange.

340
00:20:41,800 --> 00:20:42,680
Well, we'll see.

341
00:20:42,880 --> 00:20:43,520
The model is there.

342
00:20:44,120 --> 00:20:48,420
The model is expensive, I must say, more expensive than anything in the analogous segment.

343
00:20:48,420 --> 00:20:49,460
Well, not that much more expensive.

344
00:20:49,560 --> 00:20:50,420
About one and a half times.

345
00:20:51,620 --> 00:20:54,000
They just added it to my corporate subscription.

346
00:20:54,040 --> 00:20:55,160
Ah, well, damn, of course.

347
00:20:55,320 --> 00:20:58,700
For these rich cats, what difference does it make what they program on.

348
00:20:59,880 --> 00:21:00,860
You know what's funny?

349
00:21:01,040 --> 00:21:10,060
Remember, Anthropic's Geopus, historically it somehow happened, they predicted, no, not predicted, but always presented it as a model for writing creative texts.

350
00:21:10,140 --> 00:21:11,720
And their CNET was supposedly for programming.

351
00:21:11,920 --> 00:21:12,120
Yeah, yeah.

352
00:21:13,040 --> 00:21:17,180
I think they moved away from that paradigm starting with the fourth version.

353
00:21:17,180 --> 00:21:24,760
With the fourth version, we even discussed this, for the fourth version they issued a press release saying that now they mainly make models for programming.

354
00:21:24,860 --> 00:21:26,320
Remember we discussed the statistics there?

355
00:21:26,340 --> 00:21:26,800
Yeah, yeah.

356
00:21:27,080 --> 00:21:28,980
Well, because that's the only way people used them.

357
00:21:29,260 --> 00:21:36,140
Well, from that point of view, it turns out that Opus, and I'll say this on our podcast, Opus is just an iterative improvement of their Sonnet model.

358
00:21:36,460 --> 00:21:40,840
It's like... They released a new model for coding.

359
00:21:40,980 --> 00:21:42,220
Their previous model was Sonnet.

360
00:21:42,220 --> 00:21:44,040
So that's it, this is the next Sonnet.

361
00:21:44,580 --> 00:21:49,400
Like, there was a lot of hype that, oh, Opus is out, the biggest model, so it must be the coolest.

362
00:21:49,480 --> 00:21:50,060
Hell no.

363
00:21:50,700 --> 00:21:55,200
In terms of the percentage gap, it's really just an iterative improvement of Sonnet.

364
00:21:56,400 --> 00:21:59,120
Somehow... Well yeah, if you look at it that way, then yes.

365
00:21:59,220 --> 00:22:06,360
In my mind, Opus was something like, you know, human last exam at 100%.

366
00:22:06,360 --> 00:22:07,660
Okay, not 100%.

367
00:22:08,060 --> 00:22:08,800
How much are they at now?

368
00:22:08,880 --> 00:22:10,320
They solve 20, now 30.

369
00:22:10,500 --> 00:22:12,920
If they had solved 50, I'd be like, well, yeah, that's Opus.

370
00:22:13,040 --> 00:22:13,860
But as it is, well, okay.

371
00:22:14,420 --> 00:22:15,960
Next model, next one.

372
00:22:17,000 --> 00:22:17,880
We'll see.

373
00:22:18,060 --> 00:22:24,480
By the way, there was a study, we won't discuss it today, but if anyone's interested, definitely check it out.

374
00:22:25,640 --> 00:22:28,960
A study... Anthropic released it, I think, themselves already.

375
00:22:29,960 --> 00:22:33,520
Ah, not Anthropic, that... My God.

376
00:22:34,520 --> 00:22:45,540
OpenRouter released a study together with the Horowitz foundation, I think, about what and how they use their AI, the most used models on OpenRouter, on 100 trillion tokens.

377
00:22:45,900 --> 00:22:49,360
Guess what the models on OpenRouter are used for the most?

378
00:22:50,440 --> 00:22:51,900
Well, for programming.

379
00:22:52,020 --> 00:22:52,300
No.

380
00:22:53,260 --> 00:22:56,820
Programming is about 10% there, a little more, I think.

381
00:22:56,820 --> 00:23:00,560
Well, almost half of the use cases are role-playing games and PersonAid.

382
00:23:01,140 --> 00:23:01,660
Communication.

383
00:23:03,000 --> 00:23:03,520
Seriously?

384
00:23:03,600 --> 00:23:03,720
Yeah.

385
00:23:05,420 --> 00:23:06,120
Holy crap.

386
00:23:06,560 --> 00:23:15,640
But in programming, what's interesting is that Anthropic's models still hold the leading position, 60% of code is run through them.

387
00:23:16,000 --> 00:23:17,140
Which surprised me.

388
00:23:17,200 --> 00:23:18,880
Even I continue to do it, by the way.

389
00:23:19,140 --> 00:23:20,540
Well, I was surprised, because of Codex.

390
00:23:20,820 --> 00:23:22,120
Well, Codex has grown too.

391
00:23:22,420 --> 00:23:23,560
Well, there you go, and the Chinese are really strong.

392
00:23:23,560 --> 00:23:24,800
Honestly, I don't really like it.

393
00:23:26,400 --> 00:23:30,140
Well, you know, to each their own.

394
00:23:31,840 --> 00:23:33,020
Yeah, that's true.

395
00:23:33,420 --> 00:23:33,760
Alright.

396
00:23:34,060 --> 00:23:38,680
The next piece of news is directly related to the fact that Anthropic is delving deeper and deeper into programming.

397
00:23:39,580 --> 00:23:45,180
Here, your commentary as a former-and-still-continuing JavaScript developer would probably be even more interesting to me.

398
00:23:46,740 --> 00:23:50,000
And did I ever use Bun?

399
00:23:50,680 --> 00:23:53,220
Well, I use ban constantly, but in our chat.

400
00:23:53,220 --> 00:23:57,700
But what Bun is from a JavaScript perspective and how it relates to Anthropic is not entirely clear.

401
00:23:58,280 --> 00:24:05,360
Anyway, there's this tool called Bun, which replaces bundlers.

402
00:24:05,620 --> 00:24:07,760
JavaScript, TypeScript ones.

403
00:24:08,300 --> 00:24:14,960
It installs packages, runs tests, and bundles everything up all at once.

404
00:24:14,960 --> 00:24:17,720
As I understand it, it's also a runtime, like Node, something of its own, right?

405
00:24:18,400 --> 00:24:20,020
Well, probably, yes, probably.

406
00:24:20,280 --> 00:24:24,360
Anyway, I've just never used it, so I really...

407
00:24:24,900 --> 00:24:27,920
I've only heard about it, but I can't say anything about it.

408
00:24:28,520 --> 00:24:29,940
And now Anthropic is buying it.

409
00:24:30,240 --> 00:24:30,680
What for?

410
00:24:30,740 --> 00:24:32,200
I honestly didn't get it.

411
00:24:35,040 --> 00:24:38,180
Well, they write that they are buying it to improve their Code-Code.

412
00:24:38,340 --> 00:24:43,620
Code-Code is originally written in TypeScript, I believe, and they want to integrate more and more tools into it.

413
00:24:43,760 --> 00:24:48,860
Not just Code-Code, they also have no-code tools, and they want to improve them.

414
00:24:49,020 --> 00:24:53,360
Like, there will be a direct integration with an open-source tool, and they will be influencing it themselves.

415
00:24:53,540 --> 00:24:56,840
This means that JavaScript processing in Code-Code might improve.

416
00:24:58,640 --> 00:25:01,200
He writes that Code-Code bundles with Bun out of the box.

417
00:25:02,360 --> 00:25:07,460
Accordingly, well, it was logical that they bought a part of what's used in Code-Code to influence it.

418
00:25:07,800 --> 00:25:11,560
On the other hand, this news was spread so widely, even Bobok wrote about it.

419
00:25:11,780 --> 00:25:18,340
And I asked, I'm not lying, you're the fifth person from the JavaScript world I've asked, have you used Bun?

420
00:25:18,420 --> 00:25:19,840
Everyone tells me no, unanimously.

421
00:25:20,460 --> 00:25:27,440
Maybe, well okay, I was asking Node.js developers, but it feels like compared to Node, Bun is just nothing.

422
00:25:27,440 --> 00:25:29,120
And like, why did the news blow up like that?

423
00:25:29,200 --> 00:25:31,560
If they had bought Node, I'd be like, well, holy crap.

424
00:25:31,700 --> 00:25:33,820
If they had bought Node, yeah, that would have been interesting.

425
00:25:35,480 --> 00:25:38,120
It's not a fact that it can be bought at all, actually, yeah.

426
00:25:38,240 --> 00:25:40,620
Because we're embedded in engines, in browsers, in everything.

427
00:25:41,660 --> 00:25:43,120
There's probably some foundation there.

428
00:25:43,340 --> 00:25:47,880
Well, anyway, there you have it, Anthropic is now buying big projects.

429
00:25:48,700 --> 00:25:49,560
Infrastructural, IT ones.

430
00:25:52,580 --> 00:25:55,720
And the third interesting piece of news from Anthropic, I'll tell you a little bit.

431
00:25:56,360 --> 00:25:59,660
Anthropic is planning to go for an IPO, that is, to be publicly traded.

432
00:26:00,940 --> 00:26:05,160
We've known about this for a long time, but the timeline was previously 2027.

433
00:26:05,280 --> 00:26:09,640
Then suddenly news appears that Anthropic is planning to go for an IPO.

434
00:26:09,900 --> 00:26:15,300
According to rumors, but they are sort of semi-rumors, semi-not-rumors, FT.com writes, in early 2026.

435
00:26:15,800 --> 00:26:18,540
So, in just a couple of months, Anthropic is going public.

436
00:26:19,560 --> 00:26:21,420
Before OpenAI, for a second.

437
00:26:21,780 --> 00:26:24,100
OpenAI was planning to go public in 2027.

438
00:26:24,940 --> 00:26:31,160
But, considering this rumor has started, and for a moment, OpenAI and Anthropic, I looked up how much they are valued at now.

439
00:26:31,740 --> 00:26:34,120
So OpenAI is valued at 500 million dollars.

440
00:26:34,280 --> 00:26:35,980
How much do you think Anthropic is valued at?

441
00:26:36,320 --> 00:26:38,440
Ah, well, you probably, yeah, it's written in the script.

442
00:26:39,080 --> 00:26:40,260
It's written there, yeah.

443
00:26:40,580 --> 00:26:42,840
But, by the way, not that much less.

444
00:26:42,960 --> 00:26:46,040
Yeah, and the gap used to be much bigger than three times.

445
00:26:46,240 --> 00:26:48,240
Not millions, but billions.

446
00:26:48,640 --> 00:26:48,980
Billions.

447
00:26:49,060 --> 00:26:51,460
And now these companies are already going nose to nose.

448
00:26:51,660 --> 00:26:53,600
And I read the reviews.

449
00:26:54,600 --> 00:27:09,640
People who understand finance say that there will be a big race now between OpenAI and Anthropic, because as long as you are a private company, regular users don't bring money into your bubble, but as soon as one of them becomes a public company,

450
00:27:10,220 --> 00:27:22,120
it might turn out that the second player simply gets nothing, because the people who will bring money for these shares will mostly understand that it's... No, no, they'll inflate a part of it.

451
00:27:22,580 --> 00:27:26,580
And like, it will be hard to put money into two bubbles.

452
00:27:26,740 --> 00:27:33,140
Anyway, there's an opinion that whoever goes public first will change the game and become the leader.

453
00:27:33,480 --> 00:27:34,320
For sure.

454
00:27:37,300 --> 00:27:39,200
He'll flip the game, only to find a drug stash underneath.

455
00:27:41,260 --> 00:27:42,520
We'll see what this leads to.

456
00:27:42,620 --> 00:27:44,280
But an IPO is pretty fast, in general.

457
00:27:45,800 --> 00:27:48,020
It's interesting how OpenAI will handle this.

458
00:27:48,100 --> 00:27:50,840
It seems like they made a commercial company just for this.

459
00:27:50,840 --> 00:27:52,160
Speaking of OpenAI.

460
00:27:52,480 --> 00:27:56,460
It's the next guest of our big fish.

461
00:27:56,640 --> 00:27:57,680
See, I'm not silent.

462
00:27:58,800 --> 00:28:16,380
Anyway, we had an ironic situation, super ironic, because when ChatGPT came out, Google got really tense, and they called a bunch of board meetings and all sorts of meetings about what they should do now.

463
00:28:16,380 --> 00:28:25,320
And that gave us a whole segment "Laughing at Google," when Google kept trying and trying and trying to make various Bards that worked poorly.

464
00:28:25,560 --> 00:28:27,160
So, and we all laughed at that.

465
00:28:27,440 --> 00:28:32,060
And now OpenAI has declared a code red.

466
00:28:34,200 --> 00:28:38,760
And it's not because of the mom jokes, but because of Gemini 3.

467
00:28:39,660 --> 00:28:41,580
There was this meme about the jokes.

468
00:28:41,980 --> 00:28:43,040
Anyway, code red.

469
00:28:44,280 --> 00:28:45,480
And that's it.

470
00:28:45,860 --> 00:28:47,860
Now Altman is tense.

471
00:28:48,300 --> 00:28:51,000
They are postponing everything that can be postponed.

472
00:28:51,100 --> 00:28:54,440
They even say they're ready to postpone ads.

473
00:28:54,480 --> 00:28:55,640
Yeah, they've postponed absolutely everything.

474
00:28:56,140 --> 00:28:57,060
Absolutely everything.

475
00:28:57,380 --> 00:29:00,340
And they want to, yeah, improve personalization, improve the models.

476
00:29:00,480 --> 00:29:04,400
And suddenly Altman used Nano-Banana.

477
00:29:04,400 --> 00:29:06,940
And he's like, damn, holy crap.

478
00:29:11,120 --> 00:29:16,660
You see, with laughter, they tried to make a drawing model in ChatGPT, so that it could partially draw.

479
00:29:16,840 --> 00:29:17,900
They tried for a whole year.

480
00:29:18,000 --> 00:29:24,000
At the beginning of the year, it turned out clunky, and there were no analogues, and everyone was like, well, it's cool, but that's what it is.

481
00:29:24,000 --> 00:29:26,320
I think they tried a little and then gave up.

482
00:29:26,540 --> 00:29:27,780
Well, how is it with them?

483
00:29:27,860 --> 00:29:31,740
It partially redraws, but at the same time, it redraws the entire image.

484
00:29:31,740 --> 00:29:32,680
So, not bad.

485
00:29:32,800 --> 00:29:35,580
Nano-Banana just learned to replace parts normally.

486
00:29:36,040 --> 00:29:41,460
And it's so funny, like, well, since Google did it, then of course there's a lot of science there.

487
00:29:41,920 --> 00:29:46,380
But in general, two leading companies, one did it, and only after that did the second one wake up.

488
00:29:46,460 --> 00:29:48,800
What the... what the hell?

489
00:29:49,060 --> 00:29:56,440
Well, it's obvious they're just sitting there at OpenAI, trembling over their developments, not showing them until a competitor shows something better.

490
00:29:56,880 --> 00:29:58,200
Well, that's what's so annoying.

491
00:29:58,760 --> 00:30:00,240
But that's the rule of the market.

492
00:30:01,100 --> 00:30:02,020
Well, yeah.

493
00:30:02,960 --> 00:30:04,200
A well-established one.

494
00:30:04,700 --> 00:30:15,320
Pay attention to how Google, once again, I'll emphasize, went from a company whose Bard was laughed at, into a company that, it feels, is now becoming not the benefactor of the whole industry.

495
00:30:15,620 --> 00:30:18,480
Not even a benefactor, no, a benefactor, an antagonist.

496
00:30:19,380 --> 00:30:22,300
Look, OpenAI is now rising up against it.

497
00:30:24,900 --> 00:30:36,420
At the same time, there's a thing going on with NVIDIA, because Google has now released its TPUs, has been making its own TPUs for a long time, and they train models on tensor processors.

498
00:30:36,460 --> 00:30:42,640
And they've now made a big agreement with Meta that, apparently, the next Llama models will be on tensor processors.

499
00:30:43,500 --> 00:30:56,300
In short, they've promoted their tensor processors so much that even in NVIDIA's press release last week, they made a small press release saying that Google is doing great, but they are still far from us.

500
00:30:56,300 --> 00:30:57,820
That's literally word for word.

501
00:30:58,120 --> 00:31:04,580
I thought, well, if even NVIDIA made a press release about this, then Google is really doing great.

502
00:31:05,860 --> 00:31:07,560
Well yeah, well yeah.

503
00:31:09,040 --> 00:31:12,740
Anyway, Google... As Altman also wrote on Twitter, Google is doing great.

504
00:31:12,780 --> 00:31:13,320
Yeah, yeah, yeah.

505
00:31:16,400 --> 00:31:20,320
It's very similar, by the way, to the advertising for a messenger called Max.

506
00:31:20,480 --> 00:31:23,200
There, a bunch of bloggers also wrote similar messages.

507
00:31:23,200 --> 00:31:25,760
It was very easy to tell who got paid.

508
00:31:26,060 --> 00:31:26,980
By the way, about Max.

509
00:31:27,700 --> 00:31:28,840
Well, it's normal.

510
00:31:31,600 --> 00:31:36,460
ChatGPT just today, in its API, finally released Codex Max.

511
00:31:36,920 --> 00:31:38,800
It was presented in mid-November.

512
00:31:39,660 --> 00:31:42,360
Today the API was released, which means it's now available everywhere.

513
00:31:42,460 --> 00:31:46,060
It's already in Cursor, and on OpenRouter, they'll probably have it by the time you're listening.

514
00:31:46,300 --> 00:31:47,060
What's the deal?

515
00:31:47,620 --> 00:31:52,120
It's the same Codex, for the same price, but better.

516
00:31:52,120 --> 00:31:56,760
Better in its cognitive abilities too.

517
00:31:56,820 --> 00:31:57,740
It's a Max model.

518
00:31:58,020 --> 00:32:05,960
And it's also faster, which is actually strange, because it seems like Max models are usually heavier, but this one works faster too.

519
00:32:06,200 --> 00:32:09,900
So there's no point in you staying on GPT-5.1 Codex anymore.

520
00:32:10,100 --> 00:32:11,580
You can switch to Codex Max.

521
00:32:11,680 --> 00:32:13,180
There are also three levels of gradation there.

522
00:32:13,820 --> 00:32:15,740
The naming is a complete disaster.

523
00:32:15,800 --> 00:32:18,480
Look at the names they have for the Codex models now.

524
00:32:19,400 --> 00:32:20,380
Let's put it this way.

525
00:32:20,500 --> 00:32:22,180
The entire GPT-5.1 lineup.

526
00:32:22,300 --> 00:32:24,600
We have GPT-5.1 models.

527
00:32:25,280 --> 00:32:28,340
There's GPT-5.1 Mini.

528
00:32:29,980 --> 00:32:32,840
These models have subdivisions.

529
00:32:32,840 --> 00:32:37,600
GPT-5.1 Mini, High, and something else.

530
00:32:39,420 --> 00:32:43,480
There's GPT-5.1 Mini Mini, Mini High.

531
00:32:45,080 --> 00:32:46,740
Codex models stand apart.

532
00:32:47,040 --> 00:32:48,720
Models adapted for programming.

533
00:32:48,780 --> 00:32:50,700
There's GPT-5.1 Codex.

534
00:32:50,960 --> 00:32:53,640
There's GPT-5.1 Codex Mini.

535
00:32:54,100 --> 00:32:56,140
There's GPT-5.1 Codex High.

536
00:32:57,020 --> 00:33:02,140
And in this case, Mini High is the amount of effort spent on thinking.

537
00:33:02,400 --> 00:33:03,660
Well, like attempts.

538
00:33:03,840 --> 00:33:04,940
Effort spent on thinking.

539
00:33:05,860 --> 00:33:07,100
Damn, I'm like a small lake now.

540
00:33:07,780 --> 00:33:08,680
Those who get it, get it.

541
00:33:09,400 --> 00:33:16,280
And now there's GPT-5.1 Codex Max, which also has a High mode.

542
00:33:17,360 --> 00:33:23,900
Yeah, Altman, by the way, recently promised to clean up the model names.

543
00:33:23,940 --> 00:33:25,540
The order didn't last long.

544
00:33:26,140 --> 00:33:28,360
This is just a disaster.

545
00:33:28,500 --> 00:33:33,160
I mean... If you go into Cursor, if anyone programs with Cursor...

546
00:33:33,160 --> 00:33:36,780
Right now... Ah, the model viewer in Cursor is broken.

547
00:33:36,980 --> 00:33:37,280
Great.

548
00:33:39,380 --> 00:33:40,580
5.1.

549
00:33:41,460 --> 00:33:42,060
Here.

550
00:33:42,240 --> 00:33:44,260
GPT-5... What's available to choose from in Cursor?

551
00:33:44,460 --> 00:33:49,760
So you type 5.1 in the Cursor IDE and look at the models there.

552
00:33:49,760 --> 00:34:16,880
GPT-5.1 Codex Max, GPT-5.1 Codex High, GPT-5.1 Codex Max High, GPT-5.1 Codex Max Low, GPT-5.1 Codex Max Extra High, GPT-5.1 Codex Max Medium Fast, GPT-5.1 Codex Max High Fast, GPT-5.1 Codex Max Low Fast.

553
00:34:16,880 --> 00:34:24,260
And the last model available in Cursor is GPT-5.1 Codex Max Extra High Fast.

554
00:34:26,800 --> 00:34:28,920
Good thing I don't even touch these settings.

555
00:34:29,420 --> 00:34:30,940
What a mess.

556
00:34:32,300 --> 00:34:34,720
Well, they're writing in the chat that this is just a Cursor thing.

557
00:34:35,000 --> 00:34:38,660
Well, yeah, but the model is called GPT-5.1 Codex Max.

558
00:34:38,980 --> 00:34:48,500
And these prefixes Extra High, High, Low, Low Fast, High Fast are the number of tokens spent on thinking, plus another API parameter.

559
00:34:48,820 --> 00:34:53,220
And each program that provides these models names them whatever they want.

560
00:34:53,380 --> 00:34:54,460
But overall, this is hell.

561
00:34:54,660 --> 00:34:55,660
This is just hellish.

562
00:34:56,700 --> 00:34:57,800
It's just a workout.

563
00:34:57,820 --> 00:34:58,920
Which model should I choose?

564
00:34:59,040 --> 00:34:59,380
Well, which one?

565
00:34:59,520 --> 00:35:01,960
So I'm sitting here, I want to program, or just what should I choose?

566
00:35:02,040 --> 00:35:03,200
Well, I'll choose Codex Max.

567
00:35:03,460 --> 00:35:12,380
And then I look in our programmer chat in the club, a guy writes, well, my Codex Max works poorly, but Codex High works fine.

568
00:35:12,380 --> 00:35:14,640
And you're like, damn, is it the same model or not?

569
00:35:14,840 --> 00:35:24,120
It's just that High is probably worse than Max, but then it turns out that Codex High is the previous generation, and Codex Max is the new generation.

570
00:35:24,520 --> 00:35:25,660
Anyway, it's a mess.

571
00:35:26,220 --> 00:35:30,120
So, guys and gals, you need to... You really have to know your way around this.

572
00:35:30,320 --> 00:35:33,440
Get yourself Anthropic, that's why Anthropic is good.

573
00:35:33,940 --> 00:35:35,860
At least its naming is better.

574
00:35:38,540 --> 00:35:44,100
Yeah, so I just run Claude Sonnet 4.5 and don't worry about it.

575
00:35:44,260 --> 00:35:46,120
Well, yeah, you can't argue with that.

576
00:35:47,940 --> 00:35:56,440
Anyway, let's move on from the tediousness of models to a new feature of OpenAI, or rather, ChatGPT.

577
00:35:56,800 --> 00:36:00,160
So, the guys are rolling out a feature called Shopping Research.

578
00:36:00,160 --> 00:36:02,120
What is it?

579
00:36:02,540 --> 00:36:10,120
You write in the chat, like, I want to buy a portable gaming console, like a Steam Deck, for example.

580
00:36:11,080 --> 00:36:22,320
And it does research, or it can also use its memory and the context of other chats for you.

581
00:36:23,100 --> 00:36:32,560
And, like, it finds an analogue of a Steam Deck for you and immediately gives a link to it, like, where to buy it, in the form of a card.

582
00:36:32,740 --> 00:36:33,980
Without even leaving the chat.

583
00:36:34,800 --> 00:36:36,260
Without leaving the chat, yeah.

584
00:36:38,400 --> 00:36:42,300
Listen, this is a bit like what Google presented recently.

585
00:36:42,340 --> 00:36:46,780
Did we talk about Google's Generative View or not?

586
00:36:48,180 --> 00:36:51,320
I honestly don't remember something like that.

587
00:36:51,320 --> 00:36:56,400
It just seems like now all these vendors are moving towards interactive interface creation.

588
00:36:56,500 --> 00:37:00,280
OpenAI now, it seems, creates a product card, you just need to press a button.

589
00:37:00,960 --> 00:37:06,780
Google does the same thing in its Gemini through Generative or Adaptive View, I don't remember what it's called.

590
00:37:06,960 --> 00:37:11,720
Anyway, if you enable Generative View in Google's settings, it also gives you controls.

591
00:37:11,860 --> 00:37:16,680
You write, I want to buy a plane ticket, and instead of links, it just draws a table for you.

592
00:37:16,680 --> 00:37:16,880
There.

593
00:37:17,620 --> 00:37:19,300
These are the tickets available, click to buy.

594
00:37:20,220 --> 00:37:22,160
It can even draw it with checkboxes.

595
00:37:23,400 --> 00:37:26,980
Well, damn, the feature is cool, actually.

596
00:37:27,320 --> 00:37:33,180
The only thing is, of course, there's a big question of trust in ChatGPT, how exactly it will give advice.

597
00:37:33,180 --> 00:37:35,340
Well yeah, they'll start getting paid, and they'll promote those products.

598
00:37:35,380 --> 00:37:38,440
Listen, do we trust Google in this regard, in search?

599
00:37:38,620 --> 00:37:39,700
We do, somehow.

600
00:37:39,700 --> 00:37:44,560
Well, yeah, on the other hand, you see, what I haven't liked in recent years.

601
00:37:45,180 --> 00:37:51,160
In Google, the fact that SEO specialists have learned to manipulate Google's search results.

602
00:37:51,580 --> 00:37:58,180
And often Google would show me not what it should have, but what was well SEO'd.

603
00:37:58,840 --> 00:38:02,520
Well, whose fault is that?

604
00:38:02,520 --> 00:38:05,020
Yeah, I'm not blaming anyone for it.

605
00:38:05,160 --> 00:38:07,220
I'm just describing the situation.

606
00:38:07,400 --> 00:38:14,520
And now, SEO specialists will learn to SEO for ChatGPT in some way, I think.

607
00:38:15,400 --> 00:38:17,440
And, well...

608
00:38:17,440 --> 00:38:21,220
Well, again, the company... What's the option here?

609
00:38:21,500 --> 00:38:23,680
I think there are two options here.

610
00:38:23,760 --> 00:38:29,980
Either OpenAI gives everyone equal opportunities, and then SEO specialists start doing whatever they want.

611
00:38:29,980 --> 00:38:33,500
And we'll fight them with some third-party methods.

612
00:38:33,760 --> 00:38:43,000
Or the company OpenAI enables its own subjective system, which will subjectively, based on their parameters, promote what they think is more correct.

613
00:38:43,260 --> 00:38:44,380
And here a bunch of questions arise.

614
00:38:44,460 --> 00:38:45,560
More correct for whom?

615
00:38:45,820 --> 00:38:48,700
And is more correct equal to more commercially viable or not?

616
00:38:49,420 --> 00:38:51,100
Well... Is there a third option?

617
00:38:51,660 --> 00:38:52,200
I don't know.

618
00:38:52,440 --> 00:38:53,620
I don't even know.

619
00:38:53,800 --> 00:38:55,180
I don't even know, to be honest.

620
00:38:55,980 --> 00:38:56,600
Well, there is.

621
00:38:56,820 --> 00:39:01,920
Let OpenAI, after all, it's OpenAI, become for OpenSource, completely everything, and we'll decide for ourselves.

622
00:39:01,980 --> 00:39:03,500
Yeah, yeah, of course, of course.

623
00:39:04,900 --> 00:39:05,820
Well, alright.

624
00:39:08,400 --> 00:39:09,380
We'll see, we'll see.

625
00:39:09,740 --> 00:39:11,420
OpenAI is expanding very broadly.

626
00:39:11,840 --> 00:39:19,240
The next piece of news, literally, also tells us that OpenAI is again entering the educational field in a big way.

627
00:39:19,300 --> 00:39:20,280
It has done this more than once.

628
00:39:20,840 --> 00:39:24,600
Here they launched a special version of ChatGPT, called ChatGPT for Teachers.

629
00:39:25,180 --> 00:39:29,260
It's intended for school teachers, as the name suggests.

630
00:39:30,660 --> 00:39:44,880
It provides free access to all staff of verified educational institutions in the US, for now, until June 2027, plus schools get administrator access, something like parental controls but for school teachers.

631
00:39:46,500 --> 00:39:47,520
Well, that's cool.

632
00:39:47,520 --> 00:39:52,700
So, not only Kazakhstani teachers will get free access, but all teachers.

633
00:39:52,700 --> 00:39:56,480
You see, Kazakhstani teachers will just get access to ChatGPT.

634
00:39:56,680 --> 00:40:01,640
I think Kazakhstan will pay, and they will be given access to ChatGPT upon verification.

635
00:40:01,740 --> 00:40:05,520
But here, I think the American government paid to have a special version made.

636
00:40:06,500 --> 00:40:08,640
For some reason, I think these are different things.

637
00:40:08,920 --> 00:40:14,800
And that this thing, which is now being distributed in America, will be more refined and more adapted to their market.

638
00:40:14,920 --> 00:40:19,140
Maybe, I don't know, some statistics will be collected there.

639
00:40:19,140 --> 00:40:23,840
Well, I don't believe that OpenAI is doing all this just out of goodwill.

640
00:40:23,960 --> 00:40:26,280
Most likely, there is government subsidization.

641
00:40:26,860 --> 00:40:28,580
Well, quite possibly.

642
00:40:31,080 --> 00:40:32,300
Well, we'll see.

643
00:40:34,100 --> 00:40:39,920
Don't you have a feeling that OpenAI is turning into some kind of monstrous service?

644
00:40:43,820 --> 00:40:45,500
Even ChatGPT itself.

645
00:40:46,040 --> 00:40:47,400
Not even OpenAI, but specifically ChatGPT.

646
00:40:47,980 --> 00:40:53,400
Yeah, they need to somehow increase the number of users further and further and get them hooked.

647
00:40:53,880 --> 00:40:58,220
Well, I have a good example from my .NET world where this worked poorly.

648
00:40:58,480 --> 00:41:02,160
We have Visual Studio, it's a huge behemoth for working in C-Sharp.

649
00:41:02,380 --> 00:41:03,740
Yeah, we discussed this before.

650
00:41:03,940 --> 00:41:06,100
For a long time, it was the big one by itself.

651
00:41:06,200 --> 00:41:10,760
Then Rider came out, which was more limited, but it was convenient.

652
00:41:11,080 --> 00:41:13,720
And everyone, well not everyone, but many switched to Rider.

653
00:41:13,720 --> 00:41:17,440
And Rider now, I think, occupies a very large share of the programming market.

654
00:41:17,500 --> 00:41:18,920
I think the same thing might happen here.

655
00:41:19,000 --> 00:41:23,480
I used to not understand why people were switching to Gemini, given that it has much less functionality.

656
00:41:23,800 --> 00:41:29,620
But now, you know, when their models work more or less the same, Gemini 3, I think...

657
00:41:29,620 --> 00:41:34,820
I'll tell you, Lesha, Google will do the same thing in terms of adding functionality, but even worse.

658
00:41:34,920 --> 00:41:36,720
It will cram all its services in there...

659
00:41:36,720 --> 00:41:38,640
They do it a bit more organically.

660
00:41:38,700 --> 00:41:40,640
At least they split it into different products.

661
00:41:40,640 --> 00:41:52,480
They have Gemini, which is a web assistant, they have... I forgot what it's called... ah, Google AI Studio, where all the tests are done.

662
00:41:52,600 --> 00:41:58,460
They somehow have everything... they have... we'll discuss it today, a no-code platform has appeared, and it's all separate.

663
00:41:58,820 --> 00:42:00,700
But here, it's like everything is crammed into one chat.

664
00:42:00,780 --> 00:42:04,700
Even Codex, which is supposedly a separate service, still sprouts a link in the chat.

665
00:42:04,820 --> 00:42:09,140
Even Sora, which I don't need in the chat at all, sprouts a link in the chat.

666
00:42:09,140 --> 00:42:10,880
So I open the ChatGPT interface.

667
00:42:11,160 --> 00:42:15,340
I literally have three useless links at the top.

668
00:42:15,420 --> 00:42:20,320
Library, where my images are stored, but I don't give a damn, I don't primarily use ChatGPT for images.

669
00:42:20,620 --> 00:42:21,780
Codex and Atlas.

670
00:42:22,120 --> 00:42:24,100
I don't use either Codex or Atlas.

671
00:42:24,140 --> 00:42:25,120
Why is this at the top for me?

672
00:42:25,160 --> 00:42:27,060
What should be at the top for me is, give me my chats properly.

673
00:42:27,820 --> 00:42:33,660
And they somehow seem to cognitively strain you a lot compared to other players.

674
00:42:33,660 --> 00:42:37,600
Well, naturally, they need to somehow switch you to their products.

675
00:42:37,900 --> 00:42:39,240
But on the contrary, they're repulsive.

676
00:42:40,100 --> 00:42:44,380
They're repulsing you, Lesha, but others will notice and download.

677
00:42:44,840 --> 00:42:45,380
Well, alright.

678
00:42:45,980 --> 00:42:48,320
And you are a statistical anomaly in this case.

679
00:42:48,560 --> 00:42:51,540
Well, I... I won't even be offended.

680
00:42:54,660 --> 00:42:57,760
Anyway, let's have one more piece of news about OpenAI.

681
00:42:58,320 --> 00:43:04,860
So, OpenAI has come up with a way to make the model report when it's hallucinating.

682
00:43:05,980 --> 00:43:07,840
The mechanism is called Confessions.

683
00:43:09,220 --> 00:43:11,360
Not from the word "candy," by the way.

684
00:43:11,480 --> 00:43:12,000
Confession.

685
00:43:12,820 --> 00:43:13,680
Confession, yes.

686
00:43:13,900 --> 00:43:14,860
What do they do?

687
00:43:15,320 --> 00:43:23,460
Basically, the LLM answers the same question twice, and then the answers are compared.

688
00:43:23,800 --> 00:43:44,480
And if they differ greatly, for example, you asked, I don't know, who is Alexey Kartynnik, one model answered that he is a Hollywood actor, and the other that he is a Chinese singer, then it's likely that there is some hallucination, because the answers are very different.

689
00:43:45,440 --> 00:43:51,900
And this comparison, however, if the result is the same, then it is considered that there is no hallucination.

690
00:43:52,200 --> 00:44:02,260
And if it's different, then there is a hallucination, and the model will then apologize, saying something like, sorry, I actually don't know what to answer here.

691
00:44:03,180 --> 00:44:06,500
Are we finally going to have a model that admits it doesn't know something?

692
00:44:06,700 --> 00:44:08,920
I think this will be... Well, it seems so, yes.

693
00:44:10,260 --> 00:44:11,340
We've been striving for this for a long time.

694
00:44:11,500 --> 00:44:14,380
I even... It's a very good, very elegant solution.

695
00:44:14,520 --> 00:44:16,700
I didn't even think before that it could be done so easily.

696
00:44:16,880 --> 00:44:17,480
Well, in a sense...

697
00:44:17,480 --> 00:44:18,040
Yes, in general.

698
00:44:18,280 --> 00:44:20,840
Yeah, because they really do hallucinate differently every time.

699
00:44:21,700 --> 00:44:24,200
But on the other hand, what if there is a Chinese Alexey Kartynnik?

700
00:44:24,260 --> 00:44:24,880
Well, there is.

701
00:44:25,080 --> 00:44:27,420
So what, two Alexey Kartynniks, so what now?

702
00:44:27,880 --> 00:44:29,540
Hallucinations will become more offensive.

703
00:44:30,480 --> 00:44:31,500
Yeah, that's for sure.

704
00:44:34,520 --> 00:44:35,360
That's for sure.

705
00:44:35,940 --> 00:44:41,080
You know, if before you could say, look, ChatGPT doesn't know you, what kind of person are you.

706
00:44:41,560 --> 00:44:44,860
Well, in blogger circles, that's what you get in your DMs.

707
00:44:45,560 --> 00:44:53,760
Now you can say, look, ChatGPT doesn't even know you, and it will be doubly offensive because you'll understand that it fact-checked twice, and that's it, it doesn't know you, you see?

708
00:44:53,880 --> 00:44:54,660
Yeah, yeah.

709
00:44:55,900 --> 00:44:57,520
You're not even a hallucination.

710
00:44:57,640 --> 00:44:59,120
Yeah, not even a hallucination.

711
00:45:01,620 --> 00:45:05,720
Well, let's get to the first announcement from the SMS chat, right on topic.

712
00:45:05,880 --> 00:45:14,680
Yes, in our premium chat, please become premium subscribers, you can do it right now, by the way.

713
00:45:15,180 --> 00:45:18,280
Whoever does it right now will get mega-respect.

714
00:45:18,620 --> 00:45:26,020
So, there you can write messages to the SMS chat, which we will read live, and these can be basically any messages.

715
00:45:26,020 --> 00:45:29,120
And here is one of those messages.

716
00:45:30,360 --> 00:45:31,400
Hello, Lev.

717
00:45:32,140 --> 00:45:40,560
Dad says hello, and taking this opportunity, Viktor, that's me, wants to give you some useful advice on using ChatGPT.

718
00:45:41,260 --> 00:45:42,780
They set me up.

719
00:45:43,100 --> 00:45:47,100
So, now you have to give useful advice to a 10-year-old boy.

720
00:45:47,100 --> 00:45:51,000
Well, I guess what I'd advise you, Lev, is this.

721
00:45:52,120 --> 00:46:00,860
Know that ChatGPT, even though it pretends to know everything, it might not know some things and might lie.

722
00:46:01,020 --> 00:46:06,360
So don't hesitate to double-check with your dad, just in case, whether what ChatGPT wrote to you is true.

723
00:46:07,800 --> 00:46:10,360
I fully support that, fully support that.

724
00:46:11,100 --> 00:46:17,480
And I'd probably like to add a little something, don't forget that ChatGPT is still just a tool.

725
00:46:17,800 --> 00:46:23,760
It's a tool we use, but it's not an animate object.

726
00:46:24,640 --> 00:46:27,960
We shouldn't forget that it's a toy, it's not a person.

727
00:46:28,620 --> 00:46:36,000
For now, it's a toy, and for any difficult questions, it's better, of course, to find out what's true.

728
00:46:36,360 --> 00:46:38,440
From adults, in this case, from parents.

729
00:46:39,460 --> 00:46:41,400
A difficult announcement, you know why?

730
00:46:42,180 --> 00:46:42,960
Well, you know why?

731
00:46:43,800 --> 00:46:56,620
It's especially difficult for me because in January or February, I have to give lectures on programming with artificial intelligence for 10-16 year old teenagers... adults.

732
00:46:57,100 --> 00:46:58,300
I signed up for this.

733
00:46:59,240 --> 00:47:02,240
And like, giving lectures on programming...

734
00:47:02,240 --> 00:47:03,220
I have experience with children.

735
00:47:03,220 --> 00:47:07,460
I did it for half a year once, I graduated 5 classes, I think.

736
00:47:09,520 --> 00:47:14,020
But with AI, it's really difficult, especially considering the ethics, all these questions.

737
00:47:14,040 --> 00:47:20,040
I understand how to talk to adults about problems like bullying and pedophilia, right?

738
00:47:20,120 --> 00:47:22,620
But you're not going to talk about that with kids, the problems are completely different there.

739
00:47:23,440 --> 00:47:24,220
Completely different.

740
00:47:25,720 --> 00:47:35,020
There, it's not a problem, you need to show that some things are strange, which for adults are self-evident, but you need to explain it to children.

741
00:47:35,140 --> 00:47:38,280
I really don't know how I'm going to do it, I need to sit down and prepare.

742
00:47:39,260 --> 00:47:40,680
Well, good luck to you.

743
00:47:40,820 --> 00:47:42,000
I'll consult with you.

744
00:47:42,280 --> 00:47:44,060
Because, judging by... We have a deal.

745
00:47:44,100 --> 00:47:45,200
...the announcement, you have experience.

746
00:47:46,740 --> 00:47:48,340
Well, that's a fact.

747
00:47:49,340 --> 00:47:51,180
It's not just "judging by the announcement."

748
00:47:52,880 --> 00:47:54,600
So, that's the announcement.

749
00:47:54,600 --> 00:47:57,160
And we're moving on.

750
00:47:59,080 --> 00:48:03,580
And the next big fish is Google.

751
00:48:04,100 --> 00:48:09,500
Google not only released Gemini 3, but also released Gemini 3 DeepSync.

752
00:48:09,880 --> 00:48:17,760
Not DeepSync, but DeepSync, which, of course, is also crushing all the benchmarks.

753
00:48:17,760 --> 00:48:24,860
And, apparently, this is one of those models that participated in those same Olympiads and won them.

754
00:48:25,540 --> 00:48:29,100
And it also has parallel reasoning.

755
00:48:29,300 --> 00:48:32,880
So, it's a cool model, very smart.

756
00:48:33,980 --> 00:48:40,480
But, of course, you'll never try it, because you're unlikely to have an ultra subscription to Google AI.

757
00:48:44,140 --> 00:48:45,660
Yeah, it's getting a lot of praise.

758
00:48:45,820 --> 00:48:48,580
Although, I don't know, I haven't really gotten used to DeepSync yet.

759
00:48:48,780 --> 00:48:51,040
I use it very rarely in ChatGPT.

760
00:48:51,100 --> 00:48:51,700
I don't know about you.

761
00:48:53,200 --> 00:48:55,500
The equivalent in ChatGPT is called DeepSync.

762
00:48:56,720 --> 00:48:59,140
Listen, they removed it from somewhere.

763
00:48:59,300 --> 00:49:00,020
They didn't remove it from anywhere.

764
00:49:00,060 --> 00:49:00,840
I stopped using it.

765
00:49:00,840 --> 00:49:02,620
You just have to press the plus sign and there's DeepSync.

766
00:49:02,820 --> 00:49:05,660
Well yeah, but it's become very inconvenient to turn it on.

767
00:49:05,680 --> 00:49:07,820
It's just, damn, it generates such huge blocks of text.

768
00:49:07,940 --> 00:49:08,460
You have to wait.

769
00:49:08,460 --> 00:49:14,440
Sometimes it really thinks for 40 minutes for me, and then it gives you this huge wall of text, from which you know what I do?

770
00:49:14,500 --> 00:49:18,080
I just say in the next message, summarize this PDF in one paragraph, please.

771
00:49:18,800 --> 00:49:24,300
And I always forget to turn off DeepSync, and it starts summarizing with DeepSync and gives me a wall of text again.

772
00:49:25,380 --> 00:49:26,580
Oh, my god.

773
00:49:27,140 --> 00:49:43,560
No, I used to use it quite often, actually, but before it was available as a button right in the chat interface, and now they've moved it to the plus sign, I don't see it constantly in front of me, so I've kind of given up on it because of that.

774
00:49:43,560 --> 00:49:47,680
Well, by the way, in OpenAI, they added a cool thing to their deep search, deep sync.

775
00:49:48,300 --> 00:49:53,020
Since it takes a long time to think, you can now add messages to it while it's thinking.

776
00:49:53,760 --> 00:49:58,600
Like, before you only had a stop button, but now you can actually add things.

777
00:49:58,740 --> 00:49:59,420
Hurry up.

778
00:49:59,680 --> 00:50:01,280
Well, "hurry up" probably won't work.

779
00:50:01,760 --> 00:50:03,080
That's a good idea, I should try it.

780
00:50:03,160 --> 00:50:04,680
But in general, you can add facts to it.

781
00:50:04,780 --> 00:50:08,800
Like, here's something else I found, something else, and it includes these facts in the process.

782
00:50:10,700 --> 00:50:11,500
That's cool.

783
00:50:14,780 --> 00:50:17,440
Guys, your microphone is crackling, they write in the chat.

784
00:50:17,860 --> 00:50:20,840
Become premium subscribers, then we'll buy ourselves new microphones.

785
00:50:21,120 --> 00:50:22,240
We will be very grateful to you.

786
00:50:22,740 --> 00:50:24,320
And in the meantime, we're moving on.

787
00:50:25,180 --> 00:50:29,040
So, there was another piece of news, small but interesting.

788
00:50:29,140 --> 00:50:32,120
Google hired the former CTO of Boston Dynamics.

789
00:50:33,360 --> 00:50:40,240
This person will work as the Vice President of Hardware, so you can understand...

790
00:50:40,240 --> 00:50:41,740
What will he be doing?

791
00:50:42,940 --> 00:50:43,340
Robots.

792
00:50:43,620 --> 00:50:45,060
Well, Google has an interesting thing.

793
00:50:45,160 --> 00:50:51,980
They make a ton of frameworks for robot movement, for world models, for a robot to orient itself in space.

794
00:50:52,020 --> 00:51:01,120
And I think they, in this way... It's not just a thought, everyone is writing about it, that they want to create a unified platform for robotics this way.

795
00:51:01,280 --> 00:51:02,500
We've even talked about this already.

796
00:51:02,580 --> 00:51:09,260
It's just that before they only had a technical, a software team, and now they are also assembling a hardware one.

797
00:51:09,260 --> 00:51:12,900
Maybe they will even start making some hardware platforms for robots.

798
00:51:13,040 --> 00:51:13,540
Who knows.

799
00:51:13,820 --> 00:51:16,960
For example, some computational tensor units.

800
00:51:17,040 --> 00:51:18,040
Not whole robots.

801
00:51:19,220 --> 00:51:19,740
There.

802
00:51:20,020 --> 00:51:22,220
And maybe whole ones, by the way, it's not clear.

803
00:51:22,280 --> 00:51:24,880
Well, with whole ones, there's already so much competition, I think.

804
00:51:25,980 --> 00:51:26,940
Although, who knows.

805
00:51:26,980 --> 00:51:28,060
Well, why not.

806
00:51:28,240 --> 00:51:32,720
He's there... A rumor started that Altman wants to launch his own rockets.

807
00:51:32,880 --> 00:51:34,820
If even Altman wants to launch rockets.

808
00:51:36,540 --> 00:51:37,420
Why would...

809
00:51:38,780 --> 00:51:43,400
Well, basically, that's the news.

810
00:51:44,000 --> 00:51:51,040
The news that made more noise was the one from just two days ago, that Google launched a new code platform.

811
00:51:51,300 --> 00:52:00,920
They already had a platform in Google for Business, we talked about it, like NITEN, it's a visual platform for programming pipelines, for programming...

812
00:52:02,000 --> 00:52:04,720
How to explain this so that everyone understands?

813
00:52:04,980 --> 00:52:16,480
You have a canvas in the browser, and you drag and drop cubes there to define how your information should be processed, through which blocks it should pass, like a flowchart, to get what you need at the output.

814
00:52:16,480 --> 00:52:23,440
And parts of this flowchart can be models, can be different services that process your data through AI.

815
00:52:23,660 --> 00:52:32,800
This is called low-code programming, there are various players like NITEN, DeFi, Make.com, which is a similar thing, by the way, although not specialized.

816
00:52:33,380 --> 00:52:34,620
So, Google already had something like that.

817
00:52:34,780 --> 00:52:39,380
It's something at the intersection of programming and non-programming, because you still need to program a little there.

818
00:52:39,380 --> 00:52:42,780
And now they are releasing a separate tool called Workspace Studio.

819
00:52:43,220 --> 00:52:45,460
It's available with a regular AI subscription.

820
00:52:45,640 --> 00:52:48,780
I think you can even use it with a free subscription.

821
00:52:49,380 --> 00:52:55,440
It's basically just an input field where you type what you need to do, and it programs it.

822
00:52:55,960 --> 00:53:01,280
Well, we know similar platforms, they just position themselves as tools for web development.

823
00:53:01,440 --> 00:53:03,340
These are VZero, Lovable.

824
00:53:03,700 --> 00:53:11,120
Now Google has made something of its own, but they position it exclusively as "come to us, anyone, absolutely anyone, from a child to a grandmother."

825
00:53:11,200 --> 00:53:12,880
Write what you need, and we'll do it for you.

826
00:53:17,740 --> 00:53:20,280
Automated everyday work with AI agents.

827
00:53:20,440 --> 00:53:22,360
That is, come, automate your work.

828
00:53:22,520 --> 00:53:24,780
Naturally, there are a bunch of integrations with Google services.

829
00:53:24,780 --> 00:53:27,340
You need... Who would doubt it, right.

830
00:53:27,480 --> 00:53:32,080
You need to automatically get something from your email into your calendar or, I don't know, save it to Google Drive.

831
00:53:32,160 --> 00:53:33,620
You can easily set that up there.

832
00:53:33,880 --> 00:53:35,240
Just by talking to the AI.

833
00:53:37,780 --> 00:53:38,660
Interesting, interesting.

834
00:53:39,780 --> 00:53:41,640
That is, we are moving to the level of magic.

835
00:53:42,000 --> 00:53:53,680
While you and I still understand, and most of our audience understands, how this works under the hood, for the guys... the girls who have never encountered programming, for them it's already basically some kind of magic.

836
00:53:53,940 --> 00:53:54,880
Well, a very simple one.

837
00:53:55,560 --> 00:53:56,940
Like, well, what's the big deal?

838
00:53:57,060 --> 00:54:01,460
I've integrated my Excel with my Google Drive.

839
00:54:01,800 --> 00:54:05,460
So what, I just wrote what needed to be done, and everything works.

840
00:54:05,960 --> 00:54:13,760
But before, you either had to code it, or go figure out the IFTTT service, and do a bunch of not-so-obvious manipulations there.

841
00:54:13,760 --> 00:54:19,700
And even in this hypothesis of an ideal world, Excel still remains, it turns out.

842
00:54:21,820 --> 00:54:23,080
The only difference... Well, Excel.

843
00:54:23,200 --> 00:54:24,600
I have nothing against Excel.

844
00:54:25,720 --> 00:54:31,840
The only difference is, you used to be able to do this for free with your knowledge, and now you can do it for a fee, but without knowledge.

845
00:54:33,320 --> 00:54:34,120
Simply perfect.

846
00:54:35,060 --> 00:54:36,280
On your own for a fee, without knowledge.

847
00:54:36,380 --> 00:54:40,560
Before, you could pay programmers, they would have done it, but now you can do it yourself, supposedly yourself.

848
00:54:40,800 --> 00:54:41,600
With AI programming for you.

849
00:54:41,600 --> 00:54:44,740
You can still pay programmers, please.

850
00:54:45,040 --> 00:54:47,880
It will be a thousand times more expensive, but pay up.

851
00:54:49,580 --> 00:54:50,940
Something like that, yeah.

852
00:54:51,480 --> 00:54:53,240
We'll finish with Google on that note.

853
00:54:53,500 --> 00:55:01,080
We'll finish with Google and move on to Mistral, to our huge European fish, yes.

854
00:55:01,320 --> 00:55:03,700
Wait, no, it's not a huge fish, Vitya.

855
00:55:03,920 --> 00:55:06,100
They don't deserve that much honor.

856
00:55:06,140 --> 00:55:08,760
For how many episodes have we been asking them to make us ambassadors?

857
00:55:08,880 --> 00:55:10,260
We sent them applications over and over.

858
00:55:11,220 --> 00:55:12,360
We talk about them.

859
00:55:12,440 --> 00:55:16,700
Anyway, to a not-so-big fish.

860
00:55:16,780 --> 00:55:19,700
A medium fish now, closer to the end of the big fish list, I'd say.

861
00:55:19,920 --> 00:55:22,300
A candidate for elimination from the big fish list.

862
00:55:23,140 --> 00:55:24,480
Let's just say we're moving on.

863
00:55:24,680 --> 00:55:26,880
Anyway, Mistral introduced Mistral 3.

864
00:55:27,700 --> 00:55:34,820
It's a whole family of open-source models under the Apache 2.0 license, or rather, "open" is probably the more correct term.

865
00:55:34,820 --> 00:55:41,300
Like, they're really open-source, truly open-source, like, the datasets are available, and everything you want is available, that's their thing.

866
00:55:41,300 --> 00:55:42,460
So, everything by the book.

867
00:55:42,580 --> 00:55:42,800
Yes.

868
00:55:43,760 --> 00:55:45,740
Anyway, several versions.

869
00:55:46,480 --> 00:55:49,660
Small, Medium, Large.

870
00:55:51,520 --> 00:55:57,600
In short, Small runs on portable devices, like phones and such.

871
00:55:57,600 --> 00:56:00,420
Large is released on various clouds.

872
00:56:01,860 --> 00:56:11,660
And in general, they say that, judging by the benchmarks, the small models turned out quite well, because they give decent results on weak hardware.

873
00:56:12,600 --> 00:56:12,940
There.

874
00:56:13,040 --> 00:56:20,360
But the Large versions don't exactly set the world on fire.

875
00:56:21,540 --> 00:56:22,940
Well, yeah, yeah.

876
00:56:23,700 --> 00:56:25,720
In short, a big release for Europe.

877
00:56:25,900 --> 00:56:27,360
There's nothing better in Europe.

878
00:56:27,780 --> 00:56:31,780
But compared to what the Chinese are doing in open-source, this is far from...

879
00:56:31,780 --> 00:56:32,900
It's not even the best.

880
00:56:33,040 --> 00:56:36,640
It's even... Okay, let's call it an average result.

881
00:56:37,280 --> 00:56:38,040
An average result.

882
00:56:39,980 --> 00:56:49,920
I was even a little sad, because... Well, remember our mood when Mistral used to release something that didn't exist before, or something on par with ChatGPT, and now they release this.

883
00:56:50,040 --> 00:56:50,520
And you're like, ugh.

884
00:56:51,200 --> 00:56:53,260
Why the hell do I need this model, I'll go download the new DeepSeek.

885
00:56:54,560 --> 00:56:55,800
Well, yeah, that's how it'll be.

886
00:56:59,420 --> 00:57:05,320
So, but, but, but, by the way, we haven't had news from our favorite big Belarusian fish in a while.

887
00:57:09,420 --> 00:57:10,140
Perplexity.

888
00:57:10,140 --> 00:57:13,660
Yeah, for those who didn't know, Perplexity has very strong Belarusian roots.

889
00:57:15,000 --> 00:57:17,120
Although they themselves might not think so.

890
00:57:17,800 --> 00:57:19,340
Alright, let's not.

891
00:57:19,760 --> 00:57:20,960
Anyway, Perplexity.

892
00:57:21,180 --> 00:57:30,720
They've added an interesting feature, now you can create slides, your dreaded Excel files, and Google Docs through them.

893
00:57:30,840 --> 00:57:34,900
It can now do it in all modes.

894
00:57:34,900 --> 00:57:35,380
Or rather, not create.

895
00:57:35,440 --> 00:57:37,880
It can use them and sometimes even create them.

896
00:57:39,220 --> 00:57:39,920
I don't know.

897
00:57:40,000 --> 00:57:53,700
It seemed to me that Perplexity had this function for quite a while, and then I went and browsed different chats and realized that the functionality to at least create... no, not even create, but just to read a doc file, is actually not a basic feature that not everyone has yet,

898
00:57:54,300 --> 00:57:54,840
but they have it.

899
00:57:56,160 --> 00:57:57,720
Well yeah, by the way, that's true.

900
00:57:58,280 --> 00:58:01,620
So, if you happen to use Perplexity, let us know.

901
00:58:01,940 --> 00:58:03,700
Do you use Perplexity?

902
00:58:03,780 --> 00:58:05,620
Somehow it completely fell off my radar.

903
00:58:05,980 --> 00:58:06,500
Do you use it?

904
00:58:07,160 --> 00:58:07,680
I do.

905
00:58:07,780 --> 00:58:10,160
Well, I have a free premium subscription there.

906
00:58:10,700 --> 00:58:11,680
What, do I use it instead of Google?

907
00:58:11,680 --> 00:58:12,800
What do you mean, a free premium subscription?

908
00:58:12,820 --> 00:58:14,640
Another generous gift from a benefactor?

909
00:58:16,520 --> 00:58:20,880
I've told you about this several times, Lesha, and I'm ready to tell you again.

910
00:58:21,560 --> 00:58:24,400
I had a Revolut Ultra subscription.

911
00:58:24,400 --> 00:58:25,420
Ah, Revolut, got it.

912
00:58:25,980 --> 00:58:30,760
Which I specifically bought at a discount so they would give me a year of Perplexity.

913
00:58:30,940 --> 00:58:34,500
And then I successfully unsubscribed from Revolut, but I still had Perplexity.

914
00:58:34,760 --> 00:58:35,700
What do you do with it?

915
00:58:38,060 --> 00:58:38,580
I Google with it.

916
00:58:39,300 --> 00:58:45,200
I remember when I was there six months ago, it worked very slowly compared to regular Google.

917
00:58:45,940 --> 00:58:47,080
Well, slower, much slower.

918
00:58:47,100 --> 00:58:49,200
Well, now it works quite fast.

919
00:58:50,080 --> 00:58:51,060
Well, okay, okay.

920
00:58:51,380 --> 00:58:53,540
We wish Perplexity all the best.

921
00:58:54,440 --> 00:58:54,880
There.

922
00:58:55,180 --> 00:58:57,740
You can come to us too, we'll talk more about you.

923
00:58:58,700 --> 00:58:59,400
Yeah, yeah, yeah.

924
00:58:59,560 --> 00:59:00,800
And for a low price, by the way.

925
00:59:01,140 --> 00:59:02,940
I think Perplexity is doing fine as it is.

926
00:59:04,880 --> 00:59:06,640
It could be a little better.

927
00:59:06,740 --> 00:59:08,320
Could be better, I agree.

928
00:59:09,740 --> 00:59:13,920
Let me quickly talk about Amazon, and you talk about the Sutskever interview.

929
00:59:14,140 --> 00:59:15,200
Or vice versa, as you wish.

930
00:59:16,720 --> 00:59:19,640
I haven't watched the Sutskever interview, to be honest.

931
00:59:19,780 --> 00:59:21,720
Ah, well then you talk about Amazon, and I'll talk about Sutskever.

932
00:59:22,020 --> 00:59:22,620
Because I've watched it.

933
00:59:22,700 --> 00:59:22,940
Let's do it.

934
00:59:23,300 --> 00:59:23,960
Anything else to say.

935
00:59:24,900 --> 00:59:42,000
Anyway, Amazon launched a new line of four models called Nova, with Lite and Pro versions, which are for Reasoning, a speech model Sonic, and a multimodal model Omni, just like ChatGPT's, by the way.

936
00:59:44,400 --> 00:59:46,520
It accepts text, images, and video.

937
00:59:46,880 --> 00:59:48,620
Well yes, more and more.

938
00:59:48,860 --> 00:59:50,260
And all of this, naturally, is in AWS.

939
00:59:50,560 --> 01:00:06,380
And that's probably the only useful thing about this news, that it's in AWS, that you'll be able to quickly and painlessly integrate it into your services that already live on AWS for a relatively low cost.

940
01:00:06,380 --> 01:00:11,100
Otherwise, I don't really understand who needs these models.

941
01:00:13,520 --> 01:00:14,300
There you go.

942
01:00:14,440 --> 01:00:14,980
What do you mean, who?

943
01:00:15,020 --> 01:00:16,800
They provide them through the cloud.

944
01:00:17,080 --> 01:00:18,080
To their cloud...

945
01:00:18,080 --> 01:00:18,920
No, that's true.

946
01:00:19,280 --> 01:00:22,680
For those who use AWS, okay.

947
01:00:22,860 --> 01:00:27,340
People who live on AWS, they'll integrate it; for everyone else.

948
01:00:27,520 --> 01:00:29,500
Well, they released it, so they released it.

949
01:00:30,080 --> 01:00:31,780
Everyone is still trying to make models.

950
01:00:31,900 --> 01:00:42,080
I think there will come a time when models will stop developing so rapidly, everyone will have made their own models and will just focus on integration.

951
01:00:42,160 --> 01:00:47,120
By the way, this is one of the thoughts that Sutskever voiced in his interview.

952
01:00:50,140 --> 01:00:53,220
Anyway, he gave an interview to Dwarkesh Patel.

953
01:00:53,580 --> 01:00:55,360
He's a very popular blogger.

954
01:00:55,740 --> 01:00:57,780
By the way, how popular is he... No.

955
01:00:59,000 --> 01:01:03,340
There was a recent analysis from YouTube of the best channels on YouTube.

956
01:01:03,400 --> 01:01:05,580
The Diary of a CEO was on there.

957
01:01:05,740 --> 01:01:08,040
Maybe someone knows it, also a very popular interview channel.

958
01:01:08,840 --> 01:01:10,240
Dwarkesh Patel wasn't there.

959
01:01:10,380 --> 01:01:14,400
But nevertheless, the guy's interviews are on the level of Friedman's, and often even more resonant.

960
01:01:14,400 --> 01:01:22,060
Sutskever appeared in public there for the first time in the last two or even three years, basically.

961
01:01:22,500 --> 01:01:24,640
And I'll tell you this, just to be upfront.

962
01:01:24,820 --> 01:01:30,760
Everyone was expecting him to drop some super fundamental truth bomb that everyone would be buzzing about for another two years.

963
01:01:31,520 --> 01:01:40,860
But it turned out that he did drop some super fundamental truth bombs, but half of those fundamental truths were basic ML fundamentals that are only interesting to listen to for those who work with those fundamentals.

964
01:01:40,860 --> 01:01:44,140
The second half of the fundamental truths were kind of in the "the truth is out there" style.

965
01:01:45,280 --> 01:02:00,860
That is, when Dwarkesh asked him about what his company does, first of all, it was cringey, because Dwarkesh understood that he wouldn't get an answer, and he asked it with a corresponding expression, like, "Ilya, maybe you'll finally tell us what the company... well,

966
01:02:00,880 --> 01:02:03,380
I understand you won't tell, but maybe let's give it a try."

967
01:02:03,420 --> 01:02:04,660
That's how his questions sounded.

968
01:02:05,260 --> 01:02:06,940
Sutskever answered in much the same way.

969
01:02:06,940 --> 01:02:12,740
He answered, "No, of course I won't tell you, but in general, based on what you've already heard, we're working on this and that."

970
01:02:12,860 --> 01:02:16,340
In short, we still don't know what Sutskever's company, SSI, will be doing.

971
01:02:17,680 --> 01:02:29,740
The only thing he let slip, and not verbatim, it was hinted at throughout the interview, is that they are supposedly working not even on AGI, but on the next thing after AGI.

972
01:02:30,040 --> 01:02:31,400
That's what people have dubbed it.

973
01:02:32,620 --> 01:02:33,880
And unsafe AGI.

974
01:02:33,980 --> 01:02:35,840
They didn't really talk about safe AGI at all.

975
01:02:35,840 --> 01:02:40,720
So, Sutskever is making a system that will be better than AGI.

976
01:02:41,640 --> 01:02:44,220
In what way it will be better is unclear.

977
01:02:44,340 --> 01:02:46,360
He said that they want... Well, it will just be better.

978
01:02:46,660 --> 01:02:56,780
No, he says that he thinks people have another training mechanism in their heads that they haven't been able to replicate in LLMs.

979
01:02:56,820 --> 01:03:00,000
I'm sitting there thinking, Ilya, well, thank you very much for saying that.

980
01:03:00,460 --> 01:03:03,460
I think there are many things from the human mind that are still missing in LLMs.

981
01:03:03,460 --> 01:03:16,900
But he emphasized this to then make a move towards the idea that his system, their system that they are developing, will supposedly behave and learn like a human.

982
01:03:17,280 --> 01:03:32,660
Like, they want to create a system that will learn for a long time, and will learn not just on some narrow data, but also on the abstraction of that data.

983
01:03:32,660 --> 01:03:42,760
He gave an example that our current systems are like competitive programmers who have trained for 10,000 hours in competitive programming, learned all the theory, but can't generalize.

984
01:03:43,540 --> 01:03:45,860
And so he says that their systems will also be able to generalize.

985
01:03:46,440 --> 01:03:47,340
But how?

986
01:03:47,600 --> 01:03:48,600
With what?

987
01:03:48,960 --> 01:03:50,580
What are their results so far?

988
01:03:51,140 --> 01:03:51,660
Silence.

989
01:03:51,840 --> 01:03:53,720
Meanwhile, billions have been invested in the company.

990
01:03:54,180 --> 01:03:57,000
And we got the numbers.

991
01:03:57,000 --> 01:04:03,260
He said that a system of this level, which surpasses AGI, will appear in their company in 5-20 years.

992
01:04:03,740 --> 01:04:06,580
Which, in my opinion, is quite positive.

993
01:04:06,760 --> 01:04:09,880
I thought it would be something like 15-60.

994
01:04:13,300 --> 01:04:15,480
Well, that's the kind of interview it was.

995
01:04:15,640 --> 01:04:24,200
So, a lot of vague promises. My ML buddies got a lot of joy out of it, because they were sitting there like, "Oh, yeah, yeah, I agree with Ilya here, I get this."

996
01:04:24,200 --> 01:04:27,820
And I was sitting there like, "Dude, I didn't understand half of it, and the other half I've heard before."

997
01:04:28,420 --> 01:04:29,140
Something like that.

998
01:04:30,860 --> 01:04:32,660
Yeah, I know that feeling.

999
01:04:33,020 --> 01:04:34,840
It's a bit sad, a bit sad.

1000
01:04:34,880 --> 01:04:39,260
You don't expect that level of insight from a guy who hasn't appeared for 3 years.

1001
01:04:39,640 --> 01:04:41,320
Well, on the other hand, at least he's alive.

1002
01:04:42,540 --> 01:04:44,440
Those are all your expectations, Alexey.

1003
01:04:44,480 --> 01:04:45,260
I agree, I agree.

1004
01:04:45,320 --> 01:04:51,880
I got scolded too, they said, "What the hell, Sutskever said two coherent words, that's a huge news."

1005
01:04:52,420 --> 01:04:53,980
It's a celebration, right.

1006
01:04:54,200 --> 01:04:58,460
Ah, so what am I getting at, I didn't get to the point, as usual, I got lost in my thoughts.

1007
01:04:59,020 --> 01:05:20,280
His main idea was that we have moved from the scaling phase, which lasted from 2020 to apparently 2026, when they just flooded everything with processors and power, to the research phase.

1008
01:05:20,280 --> 01:05:28,600
He bets that literally in 2026, due to the chip crisis, due to the fact that there is nowhere else to grow, they will reactivate nuclear power plants.

1009
01:05:28,700 --> 01:05:35,480
People will simply realize that they need to come up with better architectures, not just scale all this and burn heat and electricity for nothing.

1010
01:05:36,120 --> 01:05:37,880
And the era of research will begin.

1011
01:05:38,060 --> 01:05:39,840
And they are supposedly engaged in research themselves.

1012
01:05:41,460 --> 01:05:48,600
I don't know, I've had a million arguments with Sutskever, but even to me, that was obvious, I think.

1013
01:05:48,700 --> 01:05:49,740
See?

1014
01:05:50,840 --> 01:05:56,920
Half of the interview was hard to understand because there was a lot of that nerdy stuff from ML.

1015
01:05:58,620 --> 01:06:05,880
Half of the interview was like, "No, Sutskever can't be saying things that you've already discussed a million times with Vitya on the podcast."

1016
01:06:06,000 --> 01:06:08,460
No, what the hell?

1017
01:06:09,960 --> 01:06:10,660
Well yeah.

1018
01:06:11,280 --> 01:06:12,240
But no.

1019
01:06:12,740 --> 01:06:13,660
Yeah, but no.

1020
01:06:14,100 --> 01:06:14,880
Ah, yes.

1021
01:06:15,100 --> 01:06:15,640
Anyway, the interview.

1022
01:06:15,700 --> 01:06:24,140
There was another funny interview with Musk with some super popular Indian, specifically Indian guy, who does interviews about business.

1023
01:06:24,680 --> 01:06:28,320
There's some channel from India with 10 million or 6 million subscribers.

1024
01:06:28,720 --> 01:06:32,380
I didn't watch the interview, I just want to share my feelings about the interview.

1025
01:06:32,600 --> 01:06:38,960
It looks very comical when they're sitting in, I think, a Starlink production facility, it's all beautiful around.

1026
01:06:40,260 --> 01:06:47,160
Musk is sitting there, and opposite him is this super stereotypical Indian guy with an accent five times worse than mine.

1027
01:06:47,520 --> 01:06:48,940
And Musk is opposite him.

1028
01:06:49,980 --> 01:06:55,500
I watched for five minutes, and the whole time I was just sitting there thinking, damn, when is Musk going to burst out laughing or when is he going to do a Nazi salute?

1029
01:06:55,680 --> 01:06:58,380
Well, like, it was super weird.

1030
01:06:58,740 --> 01:07:01,800
I'm probably a very bad person, but somehow I don't associate Musk...

1031
01:07:01,800 --> 01:07:03,220
You're just a racist, Alexey.

1032
01:07:03,420 --> 01:07:04,400
Well, damn.

1033
01:07:04,760 --> 01:07:05,260
Well, damn.

1034
01:07:05,420 --> 01:07:15,540
This is to say that watching this interview was somehow more fun because you know you can expect some crap from Musk, than the carefully crafted interview with Sutskever.

1035
01:07:16,600 --> 01:07:17,100
But...

1036
01:07:17,100 --> 01:07:25,220
By the way, since we're talking about Musk, let's move a little in that direction.

1037
01:07:25,620 --> 01:07:28,040
So, there's Alpha Arena.

1038
01:07:28,480 --> 01:07:38,220
It's a benchmark, like a platform where AI models compete in trading, suddenly.

1039
01:07:38,280 --> 01:07:40,100
With cryptocurrencies, stocks, and so on.

1040
01:07:40,140 --> 01:07:45,700
They're given money, as if they're making bets, buying, selling.

1041
01:07:46,080 --> 01:07:48,420
And mostly they just lose.

1042
01:07:49,100 --> 01:07:55,660
What they were given, but recently a certain mystery model appeared there, that's what it's called.

1043
01:07:55,660 --> 01:08:00,140
And it started to perform better than the others in trading.

1044
01:08:00,640 --> 01:08:08,340
And Musk confirmed that this is a new version of Grok, which will be called Grok 4.20.

1045
01:08:09,380 --> 01:08:11,320
Shout out to the 4.20 meme.

1046
01:08:11,560 --> 01:08:12,020
What's the meme?

1047
01:08:12,100 --> 01:08:16,500
Explain, I've been walking around all day not understanding why it's not 4.2, but 4.20.

1048
01:08:16,900 --> 01:08:19,340
Where did the 19 come from between 1 and 20?

1049
01:08:19,560 --> 01:08:23,460
I'm not going to explain it on air, please write it in the chat.

1050
01:08:23,460 --> 01:08:23,700
What do you mean?

1051
01:08:23,920 --> 01:08:24,620
Is it that... that bad?

1052
01:08:26,180 --> 01:08:27,160
Write it, write it.

1053
01:08:27,320 --> 01:08:34,380
So after memes like this, you're still going to tell me I'm a Nazi when I'm expecting some joke from Musk towards a super stereotypical Indian?

1054
01:08:34,800 --> 01:08:36,300
Ah, listen, I was wrong.

1055
01:08:37,940 --> 01:08:42,860
4.20 is not about what I said, it's about weed.

1056
01:08:43,240 --> 01:08:44,380
Ah, well then it's fine.

1057
01:08:44,640 --> 01:08:45,580
Musk is allowed to talk about weed.

1058
01:08:45,820 --> 01:08:47,560
Moving on to the Chinese carps.

1059
01:08:47,660 --> 01:08:48,860
We have very few of them today.

1060
01:08:49,760 --> 01:08:50,260
Two.

1061
01:08:50,260 --> 01:08:52,260
Well, well, yeah.

1062
01:08:52,900 --> 01:08:53,740
Yeah.

1063
01:08:54,060 --> 01:08:57,560
I would say even one, which is like two-headed.

1064
01:09:02,090 --> 01:09:02,930
DeepSeek.

1065
01:09:03,090 --> 01:09:05,130
DeepSeek released new models.

1066
01:09:05,250 --> 01:09:08,650
DeepSeek V3.2 and V3.2 Special A.

1067
01:09:09,670 --> 01:09:10,530
How do you say that correctly?

1068
01:09:10,630 --> 01:09:10,990
Special?

1069
01:09:11,930 --> 01:09:12,430
Special.

1070
01:09:12,950 --> 01:09:14,390
I don't know who you're talking about.

1071
01:09:14,510 --> 01:09:15,010
Special, right?

1072
01:09:15,890 --> 01:09:16,390
Special.

1073
01:09:16,990 --> 01:09:17,970
Special, probably.

1074
01:09:18,330 --> 01:09:18,650
Special.

1075
01:09:19,010 --> 01:09:19,430
Well, okay.

1076
01:09:19,510 --> 01:09:20,490
Anyway, anyway.

1077
01:09:21,630 --> 01:09:24,110
This is the next version of the models.

1078
01:09:24,530 --> 01:09:29,390
There's no super jump, like we saw with the release of DeepSeek R1, here.

1079
01:09:29,490 --> 01:09:31,310
But the models are very good according to the benchmarks.

1080
01:09:31,770 --> 01:09:33,230
They, in my opinion, and GLM.

1081
01:09:33,230 --> 01:09:34,450
They're kind of on par with them.

1082
01:09:34,610 --> 01:09:37,250
Two, in short, from the open source world, it seems like this is the new SOTA.

1083
01:09:37,990 --> 01:09:40,090
Specifically, the model that's "Special."

1084
01:09:40,510 --> 01:09:50,230
First of all, they made it with a new attention mechanism, some super cool one, which allowed this model to perform quite well with long contexts.

1085
01:09:50,910 --> 01:09:56,850
Secondly, they trained it very heavily to work with agents and tools.

1086
01:09:57,150 --> 01:10:00,150
Accordingly, in agentic benchmarks, it performs very well.

1087
01:10:00,150 --> 01:10:07,570
Just to appreciate the coolness of this model, this is the first open-source model to win gold at top Olympiads.

1088
01:10:07,670 --> 01:10:14,050
At the International Mathematical Olympiad, at two programming Olympiads.

1089
01:10:14,450 --> 01:10:17,350
IOI is the International...

1090
01:10:18,750 --> 01:10:20,890
International Olympiad in Informatics.

1091
01:10:21,030 --> 01:10:21,890
At ICPC.

1092
01:10:23,010 --> 01:10:24,770
These are not absolute first places.

1093
01:10:26,010 --> 01:10:35,350
The absolute first places were taken by GPT models and Google models, but this is an open-source model that you can download and deploy on your server, and it will work like that.

1094
01:10:35,770 --> 01:10:41,650
In short, open source now officially solves Olympiads at the level of gold medalists.

1095
01:10:42,650 --> 01:10:43,570
That's cool.

1096
01:10:43,910 --> 01:10:45,010
I think so.

1097
01:10:45,010 --> 01:10:46,770
That's very cool.

1098
01:10:47,090 --> 01:10:52,670
As cool as the service of our wonderful subscriber Andrey Kvarteks.

1099
01:10:53,250 --> 01:10:55,150
Dude, wait up with Andrey.

1100
01:10:55,450 --> 01:10:56,410
Wait up with Andrey.

1101
01:10:58,630 --> 01:10:59,770
Or don't wait.

1102
01:10:59,990 --> 01:11:00,770
Anyway, okay.

1103
01:11:01,010 --> 01:11:01,830
Don't wait.

1104
01:11:02,130 --> 01:11:07,090
I just wanted to add a little more about DeepSeek, to build up the coolness.

1105
01:11:07,770 --> 01:11:12,390
We now, notice, we have a moment when a cool open-source model is released.

1106
01:11:12,390 --> 01:11:31,290
Now we know that there are many open-source models that are close to it in quality, and we are no longer very surprised, there is no so-called "DeepSeek moment," but at the same time, this new model performs on some, not even that many benchmarks, at the level of Gemini 3.0 and Claude Sonnet 4.5,

1107
01:11:31,690 --> 01:11:33,270
they are compared in coding.

1108
01:11:33,270 --> 01:11:42,630
So we already have an open-source model, and it's almost nose-to-nose with closed models, working on par with them, and this is no longer some super resonant release.

1109
01:11:42,890 --> 01:11:43,850
It's like, for us, it's...

1110
01:11:43,850 --> 01:11:44,110
Right?

1111
01:11:44,370 --> 01:11:46,450
And still, we're not going to use them at all.

1112
01:11:46,930 --> 01:11:58,450
No, well, we might not, but let's say companies where we work, our colleagues work, they will deploy all this on their servers sooner or later to run it privately, to run it more cheaply, and so on and so forth.

1113
01:11:59,610 --> 01:12:01,890
Who the hell knows, really.

1114
01:12:03,270 --> 01:12:04,770
It would seem very logical.

1115
01:12:04,790 --> 01:12:11,810
On the other hand, large companies still buy cloud services, like AWS, Azure, and so on.

1116
01:12:11,910 --> 01:12:13,350
That's the second interesting point.

1117
01:12:14,730 --> 01:12:15,930
The second interesting point.

1118
01:12:16,410 --> 01:12:19,750
Two years ago, first of all, we didn't talk about open source at all.

1119
01:12:19,850 --> 01:12:22,030
Back then, there were no more or less normal open-source models.

1120
01:12:22,110 --> 01:12:23,390
Now they are on par with closed ones.

1121
01:12:23,390 --> 01:12:32,930
The second point is that there is a feeling that we have moved away from paying for models to paying for services.

1122
01:12:33,610 --> 01:12:37,230
That is, you remember, there was a moment when we said, well, what about these services?

1123
01:12:37,370 --> 01:12:39,270
The creators of the models will copy them anyway.

1124
01:12:39,350 --> 01:12:40,810
Everyone needs models, not services.

1125
01:12:40,850 --> 01:12:47,230
And now it's as if the game has flipped, and it seems we are already subscribing specifically to a service, to some kind of wrapper around the model.

1126
01:12:47,230 --> 01:12:51,250
Although we ourselves, as programmers, could write these wrappers, but why bother if there are good ones?

1127
01:12:51,630 --> 01:12:53,710
And it no longer matters what model is under the hood.

1128
01:12:53,890 --> 01:12:57,330
Half of our listeners use "auto" in Cursor, and they're not bothered by anything.

1129
01:12:57,790 --> 01:12:58,850
Well, that's how it is, yeah.

1130
01:12:59,190 --> 01:13:06,370
Well, listen, it's the same thing, like, why use macOS when we could configure Linux even better for ourselves.

1131
01:13:06,450 --> 01:13:09,450
Well, maybe we could, but who the hell wants to do that.

1132
01:13:09,570 --> 01:13:10,490
I agree, I agree.

1133
01:13:11,530 --> 01:13:14,150
Yeah, alright, now we'll have a break.

1134
01:13:14,490 --> 01:13:16,290
Those who listen to us live know.

1135
01:13:16,330 --> 01:13:17,590
It's a sausage break.

1136
01:13:17,830 --> 01:13:26,030
Those who don't know can find out what it is from our premium chat, where episodes are released without cuts.

1137
01:13:26,290 --> 01:13:29,390
But before the break, we need to read one more announcement.

1138
01:13:29,390 --> 01:13:33,190
From our regular listener, dear premium subscriber Andrey Kvardeks.

1139
01:13:34,110 --> 01:13:36,410
Andrey, sorry if I mispronounced the nickname.

1140
01:13:36,670 --> 01:13:43,490
Anyway, he sent us a service called Multidex.online.

1141
01:13:43,710 --> 01:13:44,530
Cool domain, by the way.

1142
01:13:46,030 --> 01:13:50,910
And this service will be used today for the musical break in the online broadcast.

1143
01:13:51,190 --> 01:13:53,030
Essentially, a service that plays music.

1144
01:13:53,230 --> 01:13:55,550
The broadcast is only because of the AI tracks in this service.

1145
01:13:55,670 --> 01:13:57,930
The tracks are good, Andrey is constantly in the chat, scorpion.

1146
01:13:58,750 --> 01:14:01,430
Cool AI tracks that he makes himself, that he finds.

1147
01:14:02,050 --> 01:14:04,690
So, come to this service, the link will be in the description.

1148
01:14:04,930 --> 01:14:05,750
What else do we have?

1149
01:14:06,850 --> 01:14:07,690
What else?

1150
01:14:07,970 --> 01:14:10,570
And right off the bat, your news, by the way, I ask you to note.

1151
01:14:10,770 --> 01:14:12,450
What do you mean, mine?

1152
01:14:13,030 --> 01:14:13,850
Well, alright.

1153
01:14:14,250 --> 01:14:15,690
Who's the musician here?

1154
01:14:16,210 --> 01:14:18,270
Well, how things have changed in three years.

1155
01:14:19,130 --> 01:14:27,350
Anyway, Warner Music Group has reached an agreement with Suno and is becoming a strategic partner.

1156
01:14:27,930 --> 01:14:28,270
There you go.

1157
01:14:28,390 --> 01:14:31,170
Simultaneously withdrawing all their lawsuits against Suno.

1158
01:14:33,030 --> 01:14:35,170
Well, the news speaks for itself.

1159
01:14:35,350 --> 01:14:47,530
It's clear that they want Suno... to give Suno the ability to generate these... to generate music with copyrights from real artists.

1160
01:14:49,650 --> 01:14:52,930
The strangest thing is that there wasn't... not yet.

1161
01:14:53,030 --> 01:14:55,530
It's not as strict there as with Udio.

1162
01:14:56,490 --> 01:14:57,730
Udio has already made a deal too, right?

1163
01:14:57,790 --> 01:14:59,290
I don't remember with whom, we've already discussed it.

1164
01:14:59,470 --> 01:15:00,330
A partnership agreement.

1165
01:15:00,410 --> 01:15:01,790
In Udio, you can't download music anymore.

1166
01:15:01,870 --> 01:15:04,410
But I'm afraid that Suno will have pretty much the same thing next year.

1167
01:15:04,410 --> 01:15:04,430
There.

1168
01:15:05,070 --> 01:15:07,710
So, for me, as a musical artist... There was something like that, right?

1169
01:15:09,910 --> 01:15:10,350
Seteviru.

1170
01:15:10,450 --> 01:15:13,670
I'm a little nervous that I soon won't be able to make music through Suno.

1171
01:15:13,730 --> 01:15:15,470
And I need to look for some new services.

1172
01:15:15,590 --> 01:15:18,230
And there are no good analogues yet.

1173
01:15:19,250 --> 01:15:22,510
But, nevertheless, I'm still making music there.

1174
01:15:22,610 --> 01:15:24,550
Just the other day I released two new tracks.

1175
01:15:24,550 --> 01:15:25,290
Released.

1176
01:15:25,290 --> 01:15:25,630
We'll see.

1177
01:15:26,870 --> 01:15:27,830
Well, it is what it is.

1178
01:15:28,010 --> 01:15:30,290
From my perspective as an author, this is sad news.

1179
01:15:30,370 --> 01:15:45,610
From a general perspective, it's funny and, actually, pleasant to watch how big companies, big giants, get screwed by the market and are like, "well, we didn't get anything in court, let's partner up."

1180
01:15:46,450 --> 01:15:47,750
From love to hate.

1181
01:16:15,610 --> 01:16:17,270
On licensed content.

1182
01:16:17,710 --> 01:16:20,570
To me, this sounds like the models will work worse.

1183
01:16:20,930 --> 01:16:27,510
Because the current models were probably also not trained on the... stuff you were singing about in the song.

1184
01:16:27,590 --> 01:16:29,390
They were trained on content.

1185
01:16:29,850 --> 01:16:32,010
Maybe not licensed, but still.

1186
01:16:32,970 --> 01:16:35,290
So this whole thing sounds pretty bad.

1187
01:16:35,570 --> 01:16:44,510
And you know, these headlines are so sweet, the article headlines are so sweet, that, like, Warner Music dropped the lawsuits and announced a partnership.

1188
01:16:44,510 --> 01:16:54,670
Maybe there was a settlement agreement along the lines of, "let's say we're partners, we'll give you some money, but you'll downgrade your models and train them only on what you're allowed to."

1189
01:16:55,470 --> 01:16:56,250
Maybe something like that.

1190
01:16:59,710 --> 01:17:00,430
Maybe.

1191
01:17:00,530 --> 01:17:13,590
And considering that they will be training on licensed content, it means that maybe something else will appear there, like, "generate music for me in the style of," and here, of course, subscriptions will immediately appear, huge amounts of money, bans on using this content on social media,

1192
01:17:13,590 --> 01:17:20,910
Because this is already, like, "by the artist's name," and he has to receive royalties, or give you an exclusive agreement.

1193
01:17:21,010 --> 01:17:27,230
But the artist himself is unlikely to give you an agreement, so you'll have to sign a partnership agreement with Warner Music Group.

1194
01:17:27,550 --> 01:17:31,090
And as an individual, you won't be able to do this, you'll have to look for partners.

1195
01:17:31,210 --> 01:17:35,170
And, in short, it will be the usual bureaucracy that constantly revolves around music.

1196
01:17:36,090 --> 01:17:43,510
Well, those who have tried to get a license to use a track on YouTube know that there are just 25 circles of hell.

1197
01:17:43,590 --> 01:17:48,350
You can't just go to... I don't know, you know, for example, some Lindemann personally.

1198
01:17:48,690 --> 01:17:53,450
You can't go to Lindemann and say, "Lindemann, my dear friend, please give me a signed note."

1199
01:17:53,470 --> 01:17:56,650
What Lindemann, Lesha, if I may ask.

1200
01:17:57,130 --> 01:17:58,350
Ah, I pronounced the surname wrong.

1201
01:17:58,510 --> 01:17:59,210
Well, Rammstein.

1202
01:17:59,430 --> 01:18:00,710
He's Lindemann.

1203
01:18:00,850 --> 01:18:01,870
Lindemann, Lindemann.

1204
01:18:02,010 --> 01:18:05,770
Well, I just have a slight lisp in the other direction.

1205
01:18:06,350 --> 01:18:09,350
Well, you can't just get a note from him and go to YouTube with it.

1206
01:18:09,350 --> 01:18:21,070
You have to find the companies that produce his music, a specific track, find an intermediary who works with that company, because it's a large company, they won't work with you, and through these intermediaries get something for yourself.

1207
01:18:21,130 --> 01:18:22,150
Well, it's just a complete mess.

1208
01:18:22,350 --> 01:18:24,050
And all of this might await us with Suno.

1209
01:18:27,350 --> 01:18:35,730
There was another related piece of news, I threw it in, it's not really on our topic, but it's a sarcastic piece of news in this whole context.

1210
01:18:36,350 --> 01:18:36,750
A little bit.

1211
01:18:36,750 --> 01:18:36,950
Mhm.

1212
01:18:37,290 --> 01:18:40,030
While Warner Bros. Audio is buying Suno, Netflix is buying Warner Bros. Discovery.

1213
01:18:40,310 --> 01:18:43,470
Yeah.

1214
01:18:43,690 --> 01:18:44,130
Discovery there, by the way, isn't just there for no reason, they are the owners of the Discovery Channel.

1215
01:18:44,430 --> 01:18:44,610
Seriously?

1216
01:18:45,670 --> 01:18:50,130
Yeah.

1217
01:18:50,370 --> 01:18:50,850
Damn.

1218
01:18:51,470 --> 01:18:52,150
And a lot more, but... In short, all the channels will be removed from Warner Bros. Discovery, so Netflix isn't buying the channels, but Netflix is buying the content library.

1219
01:18:52,570 --> 01:18:52,930
Warner Bros., which includes, for example, Harry Potter.

1220
01:18:54,230 --> 01:19:01,230
Why?

1221
01:19:01,350 --> 01:19:05,190
To officially show it on their streaming service?

1222
01:19:05,190 --> 01:19:08,970
Or what?

1223
01:19:10,070 --> 01:19:10,570
What do you mean, officially show it?

1224
01:19:10,650 --> 01:19:13,070


1225
01:19:13,270 --> 01:19:13,910


1226
01:19:14,910 --> 01:19:16,910


1227
01:19:17,010 --> 01:19:19,370
They'll ban it from being shown to everyone else.

1228
01:19:21,030 --> 01:19:21,610
C'mon, yeah.

1229
01:19:21,890 --> 01:19:25,630
Alright, when was the last time you watched Harry Potter on some paid service?

1230
01:19:25,990 --> 01:19:29,270
I think everyone either has them on VHS tapes or downloaded.

1231
01:19:29,270 --> 01:19:37,090
Lesha, this doesn't concern us, the fucking pirates who pay for a certain service that ends in dot pub.

1232
01:19:37,150 --> 01:19:41,590
This concerns the civilized world, which is used to paying for streaming.

1233
01:19:42,250 --> 01:19:45,290
What streaming services can you watch Harry Potter on, other than...

1234
01:19:45,290 --> 01:19:46,170
Does Warner Bros. have...

1235
01:19:46,310 --> 01:19:46,650
streaming services?

1236
01:19:46,870 --> 01:19:48,250
Probably, I don't know.

1237
01:19:48,590 --> 01:19:49,430
I think it used to be there.

1238
01:19:49,430 --> 01:19:51,710
You can watch Harry Potter on HBO.

1239
01:19:51,830 --> 01:19:52,710
On HBO and that's it.

1240
01:19:52,830 --> 01:19:55,670
Well, so, it turns out Harry Potter is moving from one streaming service to another.

1241
01:19:55,850 --> 01:20:00,210
And I'm sure HBO is fucking garbage compared to Netflix in terms of content volume.

1242
01:20:01,250 --> 01:20:02,250
Most likely not.

1243
01:20:02,550 --> 01:20:05,150
Maybe Harry Potter is on all services right now.

1244
01:20:05,890 --> 01:20:13,910
You have no idea, you're just like a fucking "Yo-ho-ho and a bottle of rum," you're far from the world of licensed content.

1245
01:20:14,030 --> 01:20:14,690
What do you mean, far?

1246
01:20:14,690 --> 01:20:15,630
I'm just the same.

1247
01:20:15,630 --> 01:20:16,290
Why far?

1248
01:20:16,370 --> 01:20:18,190
Wait, why are you putting us down in that sense?

1249
01:20:18,190 --> 01:20:20,170
We actually paid for Windows back in the day.

1250
01:20:20,290 --> 01:20:21,970
We bought movies on YouTube.

1251
01:20:22,070 --> 01:20:22,850
What do you mean, far?

1252
01:20:22,950 --> 01:20:24,350
I'm not that big of a pirate.

1253
01:20:24,530 --> 01:20:28,630
I pay for Kinopab, but at the same time I paid for Netflix for a long time, I just recently stopped.

1254
01:20:28,750 --> 01:20:29,310
Me too, yeah.

1255
01:20:29,430 --> 01:20:32,370
I pay for Apple Origin when good shows come out there.

1256
01:20:32,450 --> 01:20:36,790
Yeah, I'm the kind of person who goes, pays Apple, and then goes and watches it on Kinopab.

1257
01:20:37,830 --> 01:20:38,230
There you go.

1258
01:20:38,570 --> 01:20:41,010
Well, then you're not a pirate, you're a privateer.

1259
01:20:41,170 --> 01:20:44,770
I'm a pirate, only... You're a pirate in the service of Spain.

1260
01:20:44,850 --> 01:20:45,830
Okay, let's figure this out.

1261
01:20:45,830 --> 01:20:52,790
I believe that piracy helps Belarusian... Anything related to Belarusian pirate culture, I respect that, I'd even support it with money.

1262
01:20:52,870 --> 01:20:56,410
All other piracy is at your own risk, at your own discretion, but no.

1263
01:20:56,890 --> 01:20:59,030
You shouldn't say that we're pirates.

1264
01:20:59,270 --> 01:21:04,590
We're not 13 years old anymore, when our whole computer could be worth 20 thousand dollars in the eyes of an American.

1265
01:21:07,830 --> 01:21:20,810
Anyway, the situation with streaming right now is that some show might have a couple of seasons on one service, and the next two seasons on another.

1266
01:21:21,410 --> 01:21:22,690
Ah, because they bought the rights, yeah?

1267
01:21:22,690 --> 01:21:24,750
And only there, because they bought it.

1268
01:21:25,010 --> 01:21:28,070
And so right now Harry Potter, most likely, I'm sure, is everywhere.

1269
01:21:28,970 --> 01:21:32,850
On Apple, some other one, on YouTube Films and so on.

1270
01:21:32,950 --> 01:21:34,550
And now, most likely, it won't be there.

1271
01:21:34,690 --> 01:21:35,870
It will only be on Netflix.

1272
01:21:36,870 --> 01:21:37,630
Well, okay.

1273
01:21:37,630 --> 01:21:39,030
And even they bought all of this.

1274
01:21:40,050 --> 01:21:41,410
Well, I don't know.

1275
01:21:41,570 --> 01:21:43,390
Okay, maybe that's a problem.

1276
01:21:43,770 --> 01:21:44,510
It's probably a problem.

1277
01:21:44,670 --> 01:21:45,450
Although, then again...

1278
01:21:45,450 --> 01:21:46,990
It's a huge fucking problem.

1279
01:21:46,990 --> 01:21:54,090
Because in the end, you have to buy a bunch of accounts for streaming services if you want to watch everything officially.

1280
01:21:54,110 --> 01:21:55,210
Now, you don't quite understand something here.

1281
01:21:55,290 --> 01:21:56,150
How do you think it works?

1282
01:21:56,330 --> 01:22:00,910
You think it works like... I'm already coughing.

1283
01:22:02,430 --> 01:22:04,270
You pay for a streaming service, right?

1284
01:22:05,690 --> 01:22:09,190
And you can watch movies on that service, for example, on HBO.

1285
01:22:09,290 --> 01:22:16,970
In that case, I agree, the Warners gave away the rights, and what was available on HBO will now be available on Netflix, and you have to buy.

1286
01:22:16,990 --> 01:22:17,730
A subscription to Netflix.

1287
01:22:17,950 --> 01:22:19,590
With Harry Potter, for example, and with movies.

1288
01:22:19,610 --> 01:22:22,410
In general, that's how things are for many people.

1289
01:22:22,930 --> 01:22:24,070
For example, that's what I do.

1290
01:22:24,130 --> 01:22:25,990
I sometimes buy movies on YouTube.

1291
01:22:26,270 --> 01:22:29,070
On YouTube, you can pay for two types of licenses.

1292
01:22:29,130 --> 01:22:32,010
So, most likely, you won't be able to.

1293
01:22:32,290 --> 01:22:37,870
Look, one license is to rent a movie for a week, a day, a month.

1294
01:22:37,970 --> 01:22:40,770
The second license is to buy it permanently.

1295
01:22:41,150 --> 01:22:46,230
So, I think that for users who bought the content permanently, it's unlikely that the content will be taken away from them.

1296
01:22:46,330 --> 01:22:46,970
They already bought it.

1297
01:22:46,990 --> 01:22:47,490
That's possible.

1298
01:22:47,890 --> 01:22:49,350
Possibly, okay, fine.

1299
01:22:49,610 --> 01:22:51,710
Those users won't be affected.

1300
01:22:52,090 --> 01:22:53,310
Great, they're lucky.

1301
01:22:53,530 --> 01:22:54,570
Nobody will be affected.

1302
01:22:55,990 --> 01:22:58,750
Well, in short, you don't get it.

1303
01:22:59,030 --> 01:22:59,410
Alright, I'm an old man.

1304
01:22:59,510 --> 01:23:00,470
Okay, go on, go on.

1305
01:23:00,970 --> 01:23:05,150
In general, unfortunately, this is a very big problem in the modern world.

1306
01:23:05,450 --> 01:23:09,470
And because of this, in fact, again...

1307
01:23:09,470 --> 01:23:21,590
I mean, when Netflix and Spotify and Steam came out, the amount of piracy in the civilized world, I'm not including our countries now, in the civilized world the amount of piracy dropped significantly.

1308
01:23:21,750 --> 01:23:24,130
So what, is it that much worse in Poland and Lithuania, I don't get it?

1309
01:23:25,290 --> 01:23:26,010
Of course.

1310
01:23:28,850 --> 01:23:39,190
Now, on the contrary, in all these First World countries, the percentage of piracy is growing at an unprecedented rate, because everyone is fucking tired of these subscriptions.

1311
01:23:39,830 --> 01:23:46,110
The only normal subscription, as Altman said, the only normal subscription is for premium listeners of...

1312
01:23:46,110 --> 01:23:46,930
The AIA Podcast.

1313
01:23:46,970 --> 01:23:47,870
The OnVibe podcast.

1314
01:23:48,310 --> 01:23:49,750
The AIA Podcast, yeah.

1315
01:23:53,650 --> 01:23:54,610
The OnVibe podcast.

1316
01:23:58,330 --> 01:23:59,910
Well, maybe you're right.

1317
01:24:00,170 --> 01:24:06,750
Probably, yes, these problems probably concern us to a lesser extent, although I've also looked at it from a slightly different point of view.

1318
01:24:08,110 --> 01:24:21,270
I... we know that Netflix has quite large budgets for filming movies, although HBO also has its own. Well, in my opinion, Netflix and Apple make pretty good series and movies on their own.

1319
01:24:21,610 --> 01:24:30,690
From that point of view, if, hypothetically, Netflix were to make the Harry Potters, I'd probably be a little happier than if some other company did it.

1320
01:24:30,910 --> 01:24:34,130
I'm afraid to say HBO, I just can't remember off the top of my head what HBO has filmed recently.

1321
01:24:34,190 --> 01:24:35,330
I think they had something good.

1322
01:24:37,010 --> 01:24:37,970
Game of Thrones.

1323
01:24:38,630 --> 01:24:41,890
Oh, well, ah, damn, okay then, yeah, that's scary, scary.

1324
01:24:44,150 --> 01:24:45,230
Well, anyway, anyway.

1325
01:24:45,230 --> 01:24:46,610
Barker Bet is also from there.

1326
01:24:46,830 --> 01:24:53,430
But the news itself, imagine, some Netflix is buying Warner Brothers, the company of your childhood.

1327
01:24:54,130 --> 01:24:55,710
Yeah, yeah, that's right.

1328
01:24:56,090 --> 01:24:57,910
Discover, well, not the whole thing, but still.

1329
01:24:58,590 --> 01:25:02,490
Okay, let's move on, we still have quite a few news items.

1330
01:25:05,590 --> 01:25:11,610
They're just writing in the chat that they've detected our illegal use of a brand, now people from EPAM will come and smack us on the head.

1331
01:25:11,610 --> 01:25:13,230
Yeah, for the AIA podcast.

1332
01:25:13,330 --> 01:25:14,810
On the contrary, we're promoting the aэpodcast.

1333
01:25:14,850 --> 01:25:15,530
Well, actually, yeah.

1334
01:25:15,590 --> 01:25:16,050
It's fine.

1335
01:25:16,410 --> 01:25:17,270
It was a good podcast.

1336
01:25:18,070 --> 01:25:19,130
The host was good.

1337
01:25:19,430 --> 01:25:20,850
Yeah, I liked them too.

1338
01:25:21,490 --> 01:25:23,310
Okay, alright, let's move on.

1339
01:25:23,590 --> 01:25:28,910
Next up, Runway released a new version of its generator, Gen 4.5.

1340
01:25:29,410 --> 01:25:40,070
And, as usual in all such cases, the visual quality has improved, and the dynamics, and the physics have improved, and all that stuff.

1341
01:25:42,690 --> 01:25:44,070
And a related piece of news.

1342
01:25:44,150 --> 01:25:45,930
Kling has released a major update.

1343
01:25:46,230 --> 01:25:49,810
I remind you, Kling has also been involved in video generation for quite a long time.

1344
01:25:49,810 --> 01:25:52,070
But, as I recall, I think they are a Chinese company.

1345
01:25:52,770 --> 01:25:53,830
Chinese, Chinese, right?

1346
01:25:54,230 --> 01:25:56,770
They released the O1 ecosystem.

1347
01:25:57,250 --> 01:26:00,430
It is, in fact, two models.

1348
01:26:00,490 --> 01:26:04,170
One model is a competitor to Nano-Banana, O1 Image.

1349
01:26:04,410 --> 01:26:12,370
And the second model included in this release is called Kling V2.6, which is also used in the O1 ecosystem.

1350
01:26:12,550 --> 01:26:16,790
It generates video from text and video from images.

1351
01:26:18,190 --> 01:26:19,650
Although I might be mistaken here.

1352
01:26:19,790 --> 01:26:23,110
Maybe in O1... Well, in short, these two releases were close together.

1353
01:26:23,350 --> 01:26:27,310
The fact is that O1 Image works really well.

1354
01:26:27,630 --> 01:26:30,130
Probably the best we've seen from Chinese models.

1355
01:26:30,210 --> 01:26:31,590
Quen had some decent models.

1356
01:26:32,010 --> 01:26:33,670
Indeed, they compare it with nano-banana.

1357
01:26:33,850 --> 01:26:36,090
You can check out this model via the link in the description.

1358
01:26:36,090 --> 01:26:42,190
Well, as for Kling V2.6, it's hard to say anymore what generates video well and what doesn't.

1359
01:26:42,310 --> 01:26:48,830
Probably, for now, the leadership there still belongs to Sora and that VEO from Google.

1360
01:26:49,270 --> 01:26:55,410
But you can find many examples online of how much Kling has improved over the past year and a half.

1361
01:26:55,890 --> 01:26:58,590
Remember that meme video "Not Cthulhu's Bride"?

1362
01:26:58,670 --> 01:26:59,770
That's a classic.

1363
01:27:00,030 --> 01:27:01,790
I don't think Kling generated that one.

1364
01:27:02,670 --> 01:27:05,150
But the video where Will Smith was eating pasta.

1365
01:27:06,090 --> 01:27:07,550
And it was sprouting out of him everywhere.

1366
01:27:07,750 --> 01:27:08,370
So, yeah.

1367
01:27:08,550 --> 01:27:11,190
Well, later on they made a normal video.

1368
01:27:11,650 --> 01:27:16,450
Well, basically, Kling 2.6 now makes a normal video with Will Smith, just from a single prompt.

1369
01:27:16,510 --> 01:27:17,770
You don't even have to try too hard.

1370
01:27:17,870 --> 01:27:21,470
The model is good, the model is available on aggregators.

1371
01:27:21,570 --> 01:27:22,830
I think it's not open source.

1372
01:27:23,030 --> 01:27:27,470
It's available on FreePic, Fall, Huxfield and on other aggregators.

1373
01:27:27,570 --> 01:27:30,450
You can also find it there and play around with it.

1374
01:27:30,450 --> 01:27:30,450
There.

1375
01:27:30,650 --> 01:27:34,270
Basically, that's all we have for that.

1376
01:27:34,390 --> 01:27:34,810
What's next?

1377
01:27:35,290 --> 01:27:36,790
Next, we'll move on to the serious news.

1378
01:27:36,790 --> 01:27:37,830
Finished with what else?

1379
01:27:38,390 --> 01:27:40,410
Now comes the law and order.

1380
01:27:41,030 --> 01:27:42,310
A little law and a little order.

1381
01:27:42,570 --> 01:27:54,510
And we begin with the news that the US, at the state level, is launching a mega-scale, very expensive project called the Genesis Mission.

1382
01:27:56,790 --> 01:27:58,550
And what is it?

1383
01:27:58,710 --> 01:28:06,810
It's a project that aims not to develop AI, but to accelerate scientific progress with the help of AI.

1384
01:28:09,430 --> 01:28:10,070
Damn.

1385
01:28:10,750 --> 01:28:21,690
In one of the recent episodes, I was complaining to you that I don't understand why a private company, like Bezos', remember, and someone else, are creating these laboratories for scientists where they just say, do whatever you want.

1386
01:28:22,010 --> 01:28:23,650
And now the government is doing it.

1387
01:28:24,390 --> 01:28:26,710
But the government will be pouring money into it.

1388
01:28:27,450 --> 01:28:28,690
I have one question.

1389
01:28:28,830 --> 01:28:30,090
There will be a large government contract.

1390
01:28:30,190 --> 01:28:31,150
I have one question.

1391
01:28:32,170 --> 01:28:40,470
As a notable financier who has already told you about the bubble IPO today, where does so much money come from?

1392
01:28:40,550 --> 01:28:48,230
They have StarYade, there, 500 billion bucks, and they've poured a ton of money into all the AI startups, and into SpaceX, and into Blue Origin.

1393
01:28:48,230 --> 01:28:50,590
What about the fact that America is the richest country in the world.

1394
01:28:50,690 --> 01:28:53,530
What about the fact that America has the largest national debt, actually.

1395
01:28:54,110 --> 01:28:56,210
Do you even know what national debt is?

1396
01:28:56,350 --> 01:28:58,010
Well, yeah, nobody pays it, that's understandable.

1397
01:28:58,410 --> 01:29:00,230
What do you mean, nobody pays it?

1398
01:29:00,670 --> 01:29:01,930
Are you kidding me?

1399
01:29:02,050 --> 01:29:03,030
Are you that clueless?

1400
01:29:03,650 --> 01:29:06,930
Okay, that's the second time this episode you've put me on the spot.

1401
01:29:07,090 --> 01:29:07,370
Alright, bring it on.

1402
01:29:07,690 --> 01:29:09,390
National debt is a very good thing.

1403
01:29:09,810 --> 01:29:10,630
What do you mean, good?

1404
01:29:10,870 --> 01:29:11,830
Does it mean they trust you?

1405
01:29:12,750 --> 01:29:18,490
It means, look, in short, the issue isn't that you have a large debt.

1406
01:29:18,610 --> 01:29:20,970
The question is, can you service it?

1407
01:29:21,130 --> 01:29:24,890
Meaning, are you paying the money back to those who lent it to you?

1408
01:29:24,930 --> 01:29:25,310
I see.

1409
01:29:25,390 --> 01:29:28,630
The larger the debt, the greater the country's solvency.

1410
01:29:29,130 --> 01:29:36,650
Yes, yes, and Americans are one of the few countries in the world that pay their debts super-stably.

1411
01:29:36,650 --> 01:29:38,270
That's what financiers think.

1412
01:29:38,690 --> 01:29:40,550
That's how financiers talk, just like you.

1413
01:29:40,750 --> 01:29:54,290
But in fact, a large debt means you owe a shit-ton of money to everyone, and that money sooner or later... Okay, in America's case, probably not sooner or later, but in the case of countries with unstable economies, sooner or later it leads to very bad consequences.

1414
01:29:54,970 --> 01:30:00,250
Well, of course, when the economy... Well, damn, the American economy is simply the largest in the world.

1415
01:30:00,890 --> 01:30:03,690
And by a very large margin, even over the Chinese one.

1416
01:30:03,690 --> 01:30:06,870
We're not economists, so I won't delve any deeper, especially not me.

1417
01:30:07,030 --> 01:30:21,550
I just want... To me, as an ordinary person on the outside, it looks strange that America today... Is it really so rich that it can afford at least three parallel mega-projects?

1418
01:30:21,810 --> 01:30:25,590
And it turns out all three of them are related to AI.

1419
01:30:26,030 --> 01:30:29,370
Literally, during the Cold War, there was one project.

1420
01:30:30,250 --> 01:30:38,110
The only thing is, the whole country, well, it wasn't sucking dick, of course, but on the whole, it gave a certain percentage, allocated from the entire country's GDP.

1421
01:30:38,410 --> 01:30:41,110
But here it feels like there's money for everything.

1422
01:30:42,130 --> 01:30:42,610
The space one.

1423
01:30:42,610 --> 01:30:43,510
Which project is that now?

1424
01:30:43,590 --> 01:30:44,070
The space one.

1425
01:30:44,650 --> 01:30:49,050
Yes, that was very expensive, but it was much more expensive than all of these...

1426
01:30:49,050 --> 01:30:49,390
No way.

1427
01:30:50,250 --> 01:30:51,930
Well, alright, with the space one.

1428
01:30:52,010 --> 01:30:56,190
I don't know, but the Manhattan Project, as we've told you, is comparable in cost to Stargate.

1429
01:30:56,590 --> 01:30:58,770
Yeah, well... Is there really that much money?

1430
01:30:59,470 --> 01:31:01,250
Well, just look at how much they spend on the military.

1431
01:31:01,290 --> 01:31:01,670
Well, I just don't believe it.

1432
01:31:01,990 --> 01:31:03,430
It just seems to me that...

1433
01:31:03,430 --> 01:31:06,970
It seems to me, and I'll emphasize this again, it seems to me that this is where the bubble is hidden.

1434
01:31:07,110 --> 01:31:12,710
Like, we're all, "Well yeah, America is rich, they print a lot of money, blah, blah, blah, so they can afford it."

1435
01:31:12,710 --> 01:31:15,550
But if you dig into it, something tells me there isn't that much money there.

1436
01:31:16,950 --> 01:31:24,490
There was news that SoftBank and other companies investing in OpenAI and other things, they can't pay back their loans.

1437
01:31:24,590 --> 01:31:28,790
They took out loans to invest, and now they can't repay those loans.

1438
01:31:29,610 --> 01:31:32,310
Well, in short, I'm not a financier.

1439
01:31:32,410 --> 01:31:34,530
We surely have financiers in the comments.

1440
01:31:34,590 --> 01:31:35,930
Please, write on YouTube.

1441
01:31:35,990 --> 01:31:39,430
If you're listening to us not on YouTube, write, write and explain.

1442
01:31:39,850 --> 01:31:41,910
What is this?

1443
01:31:42,210 --> 01:31:46,030
Is there really so much money that you can have it left and right?

1444
01:31:46,170 --> 01:31:47,990
Genesis Mission, let's give it to all the scientists.

1445
01:31:47,990 --> 01:31:54,410
Well, here you also need to, you know, look at what kind of financing there is, what the sources are.

1446
01:31:54,510 --> 01:32:04,310
Maybe they... The government will invest a bit of its own money, and pass the rest on to some OpenAI, for example, or Microsoft.

1447
01:32:06,030 --> 01:32:07,670
Well, where will they get the money from?

1448
01:32:07,710 --> 01:32:10,750
They've already given all their money to OpenAI, again.

1449
01:32:10,850 --> 01:32:13,150
OpenAI, well, it's a mutual protection racket over there.

1450
01:32:14,450 --> 01:32:15,250
Well, there you go...

1451
01:32:16,970 --> 01:32:22,210
For now, it feels like this money is just floating around in a circle, or in some kind of Torus.

1452
01:32:22,710 --> 01:32:23,670
And there's no output.

1453
01:32:23,850 --> 01:32:27,650
If there were big outputs from all three of these projects, like, I don't know, we would already have...

1454
01:32:27,650 --> 01:32:28,450
The Torah is good.

1455
01:32:28,790 --> 01:32:29,570
Shabbat shalom.

1456
01:32:29,610 --> 01:32:31,250
Not Torah, but Tor, if you please.

1457
01:32:31,470 --> 01:32:32,950
But the Torah is good, I agree.

1458
01:32:33,290 --> 01:32:35,790
So, the output is a bit small for now.

1459
01:32:36,650 --> 01:32:53,030
It seems like we now live in a world where everyone has made promises, like teenagers in front of girls, they bragged about how cool their, I don't know, brother, sister, mom, dad are, how cool they all are, and then they walk around with shit on their pants because they have none of it.

1460
01:32:53,290 --> 01:32:57,070
And it's just talk, whoever is cooler is the coolest kid in the sandbox.

1461
01:32:57,190 --> 01:32:59,850
Well, somehow... I don't know.

1462
01:32:59,850 --> 01:33:08,970
Well, listen, America is the super-champion in the space program right now, look at the number of launches.

1463
01:33:09,730 --> 01:33:13,190
Musk is the super-champion, Musk is the super-champion along with Bezos.

1464
01:33:13,310 --> 01:33:17,630
Yes, that's American business, you understand, it's American business.

1465
01:33:17,850 --> 01:33:18,470
Well, okay.

1466
01:33:18,710 --> 01:33:20,970
Which, among other things, lives off government contracts.

1467
01:33:21,690 --> 01:33:21,730
Yes.

1468
01:33:21,970 --> 01:33:28,370
Same thing, American business, the only provider of global satellite internet.

1469
01:33:28,370 --> 01:33:28,650
It's nobody.

1470
01:33:29,370 --> 01:33:32,850
All other satellite internet providers are complete shit compared to Starlink.

1471
01:33:32,970 --> 01:33:38,990
America has the most, fucking, well-paid army in the world, and by such a huge margin from everything else that exists.

1472
01:33:38,990 --> 01:33:40,030
About the internet.

1473
01:33:40,050 --> 01:33:41,610
There's already competition there.

1474
01:33:42,130 --> 01:33:46,590
Blue Origin doesn't have thousands yet, but it will have dozens of satellites by next year.

1475
01:33:46,790 --> 01:33:48,370
There will be thousands by the end of next year.

1476
01:33:48,650 --> 01:33:51,910
And it's not just Blue Origin, new players are emerging there too.

1477
01:33:52,090 --> 01:33:52,930
So it's a monopoly.

1478
01:33:53,090 --> 01:33:54,990
No, that's all American too.

1479
01:33:54,990 --> 01:33:55,870
American.

1480
01:33:56,970 --> 01:33:58,370
Well, I agree.

1481
01:33:58,790 --> 01:34:03,430
I just want to draw your attention to the fact that this sounds a bit too positive.

1482
01:34:06,230 --> 01:34:20,510
This is where these irrational feelings that it's a bubble come from, because they pour in, pour in, pour in money, but the audience doesn't see a return adequate to the scale of the commotion.

1483
01:34:21,130 --> 01:34:28,810
We haven't had a single news story where everyone was shouting about how cool it was that they recommissioned a nuclear power plant that powers an entire data center.

1484
01:34:28,870 --> 01:34:30,170
But there hasn't been a single story like that.

1485
01:34:30,970 --> 01:34:34,870
The only news about a data center was the one Musk built in three months, the data center.

1486
01:34:35,050 --> 01:34:36,170
When will this news come?

1487
01:34:36,290 --> 01:34:37,270
How long should we wait for it?

1488
01:34:37,270 --> 01:34:38,150
In '27?

1489
01:34:38,250 --> 01:34:39,770
In '28, in '29?

1490
01:34:40,390 --> 01:34:48,350
Along with the news that now your bank loans will have to be repaid at 10 times the cost, and your deposits have been wiped out?

1491
01:34:49,390 --> 01:34:50,490
That's scary.

1492
01:34:51,450 --> 01:34:58,030
And you understand, if the deposit system, hypothetically, say, America's financial system, collapses, it will hit us in Poland and Lithuania too.

1493
01:34:58,170 --> 01:35:00,310
And not just us, it will hit China, it will hit everyone.

1494
01:35:00,470 --> 01:35:01,550
It won't collapse, Lesha.

1495
01:35:01,690 --> 01:35:03,690
Oh, I f... Okay.

1496
01:35:04,570 --> 01:35:06,930
That's it, I'm not going to get into this anymore, sorry.

1497
01:35:08,450 --> 01:35:09,170
I'm a noob.

1498
01:35:09,890 --> 01:35:11,910
No, I'm not a super professional either.

1499
01:35:12,250 --> 01:35:12,510
There.

1500
01:35:12,730 --> 01:35:19,990
Anyway, okay, come to the comments, please, and write to us that the thing is simply that America just prints as many dollars as it wants.

1501
01:35:19,990 --> 01:35:21,690
So, we're just baiting for comments.

1502
01:35:22,250 --> 01:35:27,110
Okay, so then tell me about the Chinese.

1503
01:35:27,310 --> 01:35:29,270
What did you say about the Chinese?

1504
01:35:29,430 --> 01:35:30,910
Well, in short, what's up with the Chinese?

1505
01:35:31,010 --> 01:35:35,090
With the Chinese, everything is strange.

1506
01:35:35,510 --> 01:35:35,990
And what is it?

1507
01:35:36,210 --> 01:35:37,970
So, in short, what's happening with them?

1508
01:35:38,670 --> 01:35:48,210
American chips, we've already said many times that the American government was tightening the screws in various ways on NVIDIA regarding selling chips in China.

1509
01:35:48,210 --> 01:35:53,350
Sometimes they'd allow it, then forbid it, then some stripped-down versions, not stripped-down.

1510
01:35:53,710 --> 01:35:56,430
In the end, China was like, "screw you."

1511
01:35:56,950 --> 01:36:06,410
China said, or rather the Chinese government, and told its companies, from now on you are not allowed to use NVIDIA company chips.

1512
01:36:08,110 --> 01:36:10,170
Use them or buy new ones, wait?

1513
01:36:11,170 --> 01:36:12,190
Well, buy them.

1514
01:36:12,290 --> 01:36:16,170
That's a huge difference, if they're not allowed to use them, what's going to happen?

1515
01:36:16,170 --> 01:36:34,510
Well, specifically in the news source, The Information, it's written that China is breaking free, that China is banning the use of NVIDIA cards in new data centers.

1516
01:36:34,830 --> 01:36:35,770
Why is that?

1517
01:36:36,010 --> 01:36:46,150
Because the Chinese government thinks that NVIDIA is embedding various locks in these chips, basically, all sorts of chips that are not certified, not described.

1518
01:36:46,170 --> 01:36:48,410
in the documentation, which could leak information.

1519
01:36:48,950 --> 01:36:52,610
And, knowing how it happens in real life, it most likely works that way.

1520
01:36:52,870 --> 01:36:55,010
Well, not just with NVIDIA, it works like that all over the world.

1521
01:36:55,370 --> 01:36:57,910
There are always some undocumented modules in processors.

1522
01:36:59,410 --> 01:37:11,870
But it seems that China also simply decided this way, the Chinese government decided this way to simply make the US look like fools in order to boost its own economy, its local chip manufacturing company.

1523
01:37:11,870 --> 01:37:13,330
And did it very beautifully.

1524
01:37:13,670 --> 01:37:16,490
Despite the fact that their Huawei has already developed quite well.

1525
01:37:16,970 --> 01:37:20,950
It's unclear if Huawei can now cover all the supply needs?

1526
01:37:21,030 --> 01:37:23,150
Most likely not, if you completely cut off NVIDIA.

1527
01:37:23,230 --> 01:37:24,650
But the Chinese government decided to do it.

1528
01:37:25,210 --> 01:37:26,670
Another funny fact.

1529
01:37:26,950 --> 01:37:31,110
Against the backdrop of this news, NVIDIA's stock, naturally, dipped, not by much, but it dipped.

1530
01:37:31,330 --> 01:37:41,270
But now, I know, a lot of people are trying to calculate, and everyone is waiting for NVIDIA's upcoming reports to calculate how much NVIDIA was actually selling in China.

1531
01:37:41,270 --> 01:37:53,410
In short, there's a rumor that NVIDIA won't report properly, or will somehow be evasive, because it will turn out that there was just a shitload of gray imports in China.

1532
01:37:53,530 --> 01:37:57,270
Like a very significant shitload of gray imports, like tens of percent.

1533
01:37:57,530 --> 01:38:05,170
Because we all know how Jensen Huang went to see Trump himself to say, "oh-oh-oh, please, Uncle Trump, let us sell at least something to China."

1534
01:38:05,790 --> 01:38:07,670
Well, of course, the Chinese market is huge.

1535
01:38:07,910 --> 01:38:10,530
But it's not because they needed to sell something in China.

1536
01:38:10,610 --> 01:38:18,230
It's because as long as you have a loophole to sell something from NVIDIA in China, you can, in principle, ship anything.

1537
01:38:19,670 --> 01:38:23,050
By re-sticking labels, buying it through intermediaries, and saying that it's...

1538
01:38:23,050 --> 01:38:23,230
Yes, that's true.

1539
01:38:24,550 --> 01:38:26,530
And so, everyone thinks that...

1540
01:38:26,530 --> 01:38:35,030
And against this backdrop, some investors started making their bets that next year NVIDIA will really tank, and Google is catching up there too.

1541
01:38:35,170 --> 01:38:39,550
Because it will turn out that a very large percentage of NVIDIA's business was tied to China.

1542
01:38:40,010 --> 01:38:41,150
So that's the news.

1543
01:38:41,790 --> 01:38:51,150
I think they'll figure out a way to smuggle them in under the seat of a Chinese Geely anyway.

1544
01:38:52,470 --> 01:38:53,730
I don't know, I don't know.

1545
01:38:53,910 --> 01:38:57,170
But news like this somehow suggests that...

1546
01:38:57,710 --> 01:39:00,730
Well, now China will become self-sufficient in chips and that's it.

1547
01:39:01,450 --> 01:39:02,510
What else will they need?

1548
01:39:02,850 --> 01:39:03,330
Nothing.

1549
01:39:03,330 --> 01:39:06,190
We make chips, we seem to have the energy.

1550
01:39:07,310 --> 01:39:09,330
We're making our own open-source models there.

1551
01:39:09,470 --> 01:39:11,890
But now we'll close up, we'll make closed-source models.

1552
01:39:12,130 --> 01:39:12,430
And that's it.

1553
01:39:12,910 --> 01:39:13,390
That's the whole world.

1554
01:39:13,850 --> 01:39:15,350
Everything is heading towards that for me.

1555
01:39:15,910 --> 01:39:17,290
We'll just be getting from China...

1556
01:39:17,290 --> 01:39:24,910
Well, you'll be buying tokens for your cursor from China's open-AI, from China's DeepSeek instead of from someone's GPT.

1557
01:39:25,390 --> 01:39:26,170
And that's all.

1558
01:39:27,830 --> 01:39:29,690
By the way, who do you trust more?

1559
01:39:29,830 --> 01:39:32,150
Chinese networks or American networks?

1560
01:39:33,470 --> 01:39:34,210
With your data.

1561
01:39:34,270 --> 01:39:35,850
What if you had a complete equivalent...

1562
01:39:35,850 --> 01:39:37,350
Now that's a question, of course.

1563
01:39:37,830 --> 01:39:38,550
Come on, come on.

1564
01:39:38,630 --> 01:39:40,030
I don't know, Lesha, I don't know.

1565
01:39:40,190 --> 01:39:40,530
It's difficult.

1566
01:39:40,910 --> 01:39:42,890
Let's put it this way, I don't trust either of them.

1567
01:39:42,990 --> 01:39:44,790
Wait, you're using one in China right now.

1568
01:39:45,150 --> 01:39:45,550
Often.

1569
01:39:45,850 --> 01:39:46,250
I do use it.

1570
01:39:46,250 --> 01:39:48,290
If there were a direct Chinese competitor.

1571
01:39:48,390 --> 01:39:49,930
Exactly the same in terms of power.

1572
01:39:49,930 --> 01:39:51,430
Would you switch to it?

1573
01:39:52,430 --> 01:39:52,830
No.

1574
01:39:52,830 --> 01:39:53,330
Why?

1575
01:39:53,630 --> 01:39:58,290
Because I at least know that there's free journalism in the States.

1576
01:39:58,330 --> 01:40:00,150
And now, of course, they'll laugh at me in the comments.

1577
01:40:00,270 --> 01:40:09,230
Free journalism, regulators, who, all sorts of GDPRs partially reach them from Europe, AI acts and so on.

1578
01:40:09,410 --> 01:40:19,590
And there's at least some chance that if OpenAI starts treating my data like shit, that maybe, someday, some journalistic investigation will expose it.

1579
01:40:19,690 --> 01:40:21,830
We live in different worlds.

1580
01:40:22,370 --> 01:40:24,450
And now I'll explain my position to you.

1581
01:40:24,510 --> 01:40:25,310
I just...

1582
01:40:25,310 --> 01:40:27,670
I do watch all sorts of near-liberal...

1583
01:40:27,670 --> 01:40:29,230
Well, it's just that China is a black box.

1584
01:40:29,470 --> 01:40:33,890
You see, it's a completely closed story, about which I know absolutely nothing of what goes on there.

1585
01:40:34,430 --> 01:40:36,590
But they don't need to know about you, Vit.

1586
01:40:37,770 --> 01:40:41,110
Well, in my paradigm, I don't need to know what's happening there.

1587
01:40:41,410 --> 01:40:52,110
I consume a lot of liberal content and, I must say, well, like, okay, I would have told you a politician just now.

1588
01:40:52,350 --> 01:41:06,650
No, it's just that I see that the people I listen to, they often, especially those who live in Europe, have recently started saying that, no way, China, I'll never in my life use Dipsy or Kwen, are you crazy, who knows what they do with the data.

1589
01:41:06,650 --> 01:41:25,170
And I sit here and think, from my perspective as a pragmatist, for me, a person who has been hit on the head more than once by the governments of certain countries, it feels much more comfortable, much more comfortable to feed data to a company that to your territory,

1590
01:41:25,270 --> 01:41:31,210
will most likely not get here anytime soon, than to a company that is already here right now doing something.

1591
01:41:32,170 --> 01:41:40,090
Hypothetically, American legislative acts, some American laws, they can slightly extend to Europe, whereas Chinese ones are unlikely to reach here.

1592
01:41:40,670 --> 01:41:46,010
Yes, from the point of view of democracy, freedom of speech, of course, you have to support open source, fuck.

1593
01:41:46,610 --> 01:41:49,090
Not OpenAI, and not China, but open source.

1594
01:41:49,790 --> 01:41:53,690
But for some reason, everyone's still like, damn, ChatGPT at least carries some democratic values.

1595
01:41:53,870 --> 01:41:55,730
It carries democratic values, sure.

1596
01:41:55,850 --> 01:41:58,650
That data is leaked to the CIA, the FBI, and everyone else.

1597
01:41:58,650 --> 01:42:03,470
That's not my point at all, that ChatGPT carries democratic values.

1598
01:42:04,030 --> 01:42:10,930
I just know that I find it unpleasant to realize that China is a black box for me.

1599
01:42:11,690 --> 01:42:15,070
But OpenAI is a black box for you too, Vit, exactly the same.

1600
01:42:15,150 --> 01:42:20,090
But with OpenAI, I at least have a chance that they are obligated to report there.

1601
01:42:20,230 --> 01:42:21,370
You have no chance.

1602
01:42:21,790 --> 01:42:26,110
It's a strategically important company for America, they won't report to anyone.

1603
01:42:26,110 --> 01:42:34,190
If some scathing report comes out in the Washington Post, they'll jail that journalist, or send him to some country where they'll do something even worse to him.

1604
01:42:34,450 --> 01:42:35,850
We know these stories.

1605
01:42:36,010 --> 01:42:37,750
Snowden, WikiLeaks.

1606
01:42:37,930 --> 01:42:39,150
We know these stories.

1607
01:42:40,070 --> 01:42:41,890
They won't tell anything.

1608
01:42:42,070 --> 01:42:42,570
It's a black box.

1609
01:42:42,570 --> 01:42:44,750
And do you know a single Chinese Snowden?

1610
01:42:45,750 --> 01:42:47,570
Or a single Chinese WikiLeaks?

1611
01:42:47,710 --> 01:42:51,130
They probably just tighten the screws on them even more.

1612
01:42:51,230 --> 01:42:53,330
They were probably just shot, and that's it.

1613
01:42:53,330 --> 01:43:06,310
So, again, I'm saying you're pushing democratic values, freedom of speech, and I'm telling you, for us, as people, it's much safer, if we're going to leak data, to leak it to a country that won't be coming here anytime soon.

1614
01:43:07,970 --> 01:43:09,990
Well, that rhetoric doesn't resonate with me.

1615
01:43:10,210 --> 01:43:18,030
From this point of view, by the way, for those who listen to us from modern Belarus and from Russia, it's enough, it's much more advantageous for them to leak data to ChatGPT than to China.

1616
01:43:19,010 --> 01:43:19,450
There.

1617
01:43:19,530 --> 01:43:20,530
That's my logic.

1618
01:43:20,590 --> 01:43:21,950
It's super, like, banal.

1619
01:43:22,890 --> 01:43:24,630
Well, it doesn't resonate with me.

1620
01:43:25,310 --> 01:43:27,490
Okay, that's it, we've thrown enough shit, thrown enough shit.

1621
01:43:27,610 --> 01:43:31,770
Let's have you write in the comments who, in your opinion, is the bigger asshole in this argument.

1622
01:43:36,350 --> 01:43:36,910
Alright.

1623
01:43:37,410 --> 01:43:40,890
So, our next segment, in the meantime, is science and technology.

1624
01:43:41,010 --> 01:43:54,250
And we're starting with the fact that if you were suddenly planning to outrun a robot on your own, you can stop hoping, because robots already run faster than people.

1625
01:43:55,750 --> 01:44:02,710
Both Optimus from Tesla and Figure 3, both of them run faster than a human.

1626
01:44:02,870 --> 01:44:03,510
Did you watch those videos?

1627
01:44:03,510 --> 01:44:05,790
Maybe if you're Usain Bolt, yes.

1628
01:44:06,370 --> 01:44:10,570
Maybe if you're Usain Bolt, you could still compete.

1629
01:44:11,230 --> 01:44:14,570
At least in MrBeast's video, the human beat the robot.

1630
01:44:18,510 --> 01:44:20,390
What a shitty video, actually.

1631
01:44:21,130 --> 01:44:22,190
Very shitty, yes.

1632
01:44:25,670 --> 01:44:29,470
There, if you watch the videos of how they run, they've got new upgrades.

1633
01:44:29,550 --> 01:44:35,310
Apparently, some new algorithm has appeared, because the hardware is old, but these robots run very much like humans, finally.

1634
01:44:35,750 --> 01:44:38,230
That is, their movement mechanics, especially with the Figure 3.

1635
01:44:38,990 --> 01:44:42,590
I'm watching it, and my first impression is, no way, this is SORA-generated.

1636
01:44:42,590 --> 01:44:45,630
Well, it's too, it runs very human-like.

1637
01:44:46,010 --> 01:44:50,850
It doesn't run like Biden rushing to the toilet, it runs like Usain Bolt.

1638
01:44:51,730 --> 01:44:54,230
I take it this is some kind of algorithmic breakthrough.

1639
01:44:55,010 --> 01:45:09,200
And after that, right along with these videos, I came across something, I've been constantly looking for confirmation lately that the Airon robot from Xpeng, which we discussed last time, is actually a robot, not a mannequin.

1640
01:45:09,760 --> 01:45:14,020
Robots that have genders, which, in my logic, look quite sexy.

1641
01:45:15,600 --> 01:45:23,860
I saw some video, like at a presentation, for some reason I watched this moment, how they cut open the leg of one of the Airons up to the knee.

1642
01:45:24,180 --> 01:45:28,420
I mean, they removed the outer layer right up to the knee.

1643
01:45:29,240 --> 01:45:32,900
And you can really see that it's... well, the mechanics are clearly visible there.

1644
01:45:33,440 --> 01:45:38,780
Of course, you could assume that in the rest of the body it's just a person with one leg, really.

1645
01:45:38,940 --> 01:45:40,300
Well, the other way around is impossible, after all.

1646
01:45:41,200 --> 01:45:42,360
And why did they do that?

1647
01:45:42,500 --> 01:45:50,000
They were showing it this way, like, look, this is a real robot, here are its mechanics, and look how it walks.

1648
01:45:50,140 --> 01:46:03,740
Because it turns out that a large part of this visual "wow" from the robots, well, naturally, was because they look like a man and a woman, but an even bigger part was that they walk like people.

1649
01:46:04,100 --> 01:46:07,320
Not even exactly like people, this walk has a name.

1650
01:46:07,420 --> 01:46:08,580
It's called a catwalk.

1651
01:46:10,160 --> 01:46:10,560
You know?

1652
01:46:10,780 --> 01:46:11,740
Did you know what that is?

1653
01:46:11,780 --> 01:46:11,920
Yes.

1654
01:46:12,680 --> 01:46:13,260
Well yeah, of course.

1655
01:46:13,580 --> 01:46:14,500
How did you know that?

1656
01:46:14,560 --> 01:46:17,060
I just found out now.

1657
01:46:17,120 --> 01:46:17,540
I don't know.

1658
01:46:18,020 --> 01:46:19,380
Well then, tell us what a catwalk is.

1659
01:46:19,380 --> 01:46:20,420
A piece of culture.

1660
01:46:20,680 --> 01:46:22,860
Well, it's a beautiful walk.

1661
01:46:23,060 --> 01:46:24,200
Girls walk like that.

1662
01:46:25,480 --> 01:46:28,500
I don't know, like on a runway.

1663
01:46:28,840 --> 01:46:30,200
That's how models walk.

1664
01:46:30,600 --> 01:46:31,600
Well, basically, yes.

1665
01:46:31,900 --> 01:46:32,040
Damn.

1666
01:46:32,620 --> 01:46:33,080
Strange.

1667
01:46:33,220 --> 01:46:34,980
Okay, so I'm a retrograde in this regard.

1668
01:46:35,100 --> 01:46:36,300
I didn't know there was a catwalk.

1669
01:46:36,700 --> 01:46:38,020
Now I know thanks to what.

1670
01:46:38,200 --> 01:46:42,040
The Airon robot seems very sexy to me.

1671
01:46:42,120 --> 01:46:45,860
Because it was made to parody this beautiful walk.

1672
01:46:45,860 --> 01:46:47,960
And right from the stage.

1673
01:46:48,160 --> 01:46:52,180
The guy, the director of Xpeng, I think, is shouting that like, China, great.

1674
01:46:52,420 --> 01:46:53,340
Almost great, it's Yen.

1675
01:46:53,680 --> 01:46:57,140
Because we made the first robot that can do a catwalk.

1676
01:46:58,100 --> 01:46:59,080
Walk like a human.

1677
01:47:00,360 --> 01:47:01,880
Well, such a nuance.

1678
01:47:02,040 --> 01:47:02,320
Yes.

1679
01:47:02,500 --> 01:47:04,320
Speaking of cats.

1680
01:47:04,640 --> 01:47:10,020
We have an announcement from Arina Dorofeeva.

1681
01:47:10,260 --> 01:47:12,480
I'll open it now and read it.

1682
01:47:12,480 --> 01:47:15,380
You read it as if you were reading our subscriber's name for the first time.

1683
01:47:15,540 --> 01:47:16,360
Arina Dorofeeva.

1684
01:47:17,100 --> 01:47:17,820
By the way, she's in the chat right now.

1685
01:47:18,080 --> 01:47:18,520
Arina Dorofeeva, yes.

1686
01:47:18,520 --> 01:47:20,220
Thanks for being present in Mline.

1687
01:47:20,400 --> 01:47:24,160
The last name is easy to remember because it's like the last name of a Belarusian singer.

1688
01:47:24,160 --> 01:47:25,420
It's immediately obvious that you're from Belarus.

1689
01:47:25,500 --> 01:47:27,580
Actually, the most popular singer is Dorofeeva.

1690
01:47:27,680 --> 01:47:29,800
She seems alright, she's from Ukraine, by the way.

1691
01:47:29,900 --> 01:47:30,240
Well, okay.

1692
01:47:31,320 --> 01:47:31,760
The Dorofeevas.

1693
01:47:32,040 --> 01:47:34,380
Oh, Vitya, that's enough, don't dig yourself any deeper, please.

1694
01:47:34,380 --> 01:47:35,740
Yes, please, let's just stop.

1695
01:47:36,900 --> 01:47:40,680
Oh, wait, Dorofeeva is Dantes' wife, the one who is Dantes' ex-wife.

1696
01:47:40,920 --> 01:47:43,960
Yes, yes, that's enough, enough, you know, Dor, good job.

1697
01:47:45,900 --> 01:47:51,660
Anyway, Arina Dorofeeva writes, I want to buy a Sphynx kitten, completely black or pink, like a chicken.

1698
01:47:51,860 --> 01:47:57,920
Ideally with blue, green, or gray eyes, well, like a plucked chicken, light pink.

1699
01:47:58,280 --> 01:48:05,420
Well, actually, if anyone has Sphynx cats, you can write to us, or directly in the chat, Arina is there.

1700
01:48:06,160 --> 01:48:08,380
It's better to give a kitten as a gift, of course.

1701
01:48:08,500 --> 01:48:12,500
By the way, I don't know, well, people buy kittens, that's normal, right?

1702
01:48:12,600 --> 01:48:14,680
I just usually find kittens somewhere.

1703
01:48:15,560 --> 01:48:19,460
Well, I'm a supporter of getting a cat from the dump.

1704
01:48:20,300 --> 01:48:21,940
I got mine from the dump.

1705
01:48:22,420 --> 01:48:24,060
See, we're similar in that.

1706
01:48:24,520 --> 01:48:30,180
I just know that, well, at least in Poland there are shelters, yes, there are shelters everywhere.

1707
01:48:30,320 --> 01:48:33,960
It's just that some of my acquaintances find purebred dogs and cats in shelters.

1708
01:48:34,380 --> 01:48:36,360
They take them home without any problem.

1709
01:48:36,460 --> 01:48:41,780
But I know there are people for whom a pedigree is important for some kind of competition, they take them to all these things.

1710
01:48:42,700 --> 01:48:43,820
Yes, that too.

1711
01:48:44,040 --> 01:48:45,420
Well, to each their own.

1712
01:48:45,680 --> 01:48:48,160
Actually, purebred cats are beautiful, that's a fact.

1713
01:48:48,540 --> 01:48:54,920
Let's move from your favorite topic with Adolfs to Waymo, my favorite topic.

1714
01:48:56,400 --> 01:49:13,620
So, Waymo... The news is interesting because, first of all, they reported that their cars, over 100 million miles, get into 91% fewer accidents than real drivers.

1715
01:49:13,940 --> 01:49:24,000
Which once again proves to us that the time of drivers is over, and get ready for the fact that the next generations won't like to drive.

1716
01:49:24,860 --> 01:49:26,540
This is, unfortunately, a fact.

1717
01:49:26,640 --> 01:49:27,320
Or fortunately.

1718
01:49:27,840 --> 01:49:28,940
What do you mean, they won't.

1719
01:49:29,040 --> 01:49:29,320
They will.

1720
01:49:29,420 --> 01:49:30,440
That's just you without a license talking.

1721
01:49:30,580 --> 01:49:34,120
Oh, Vitya, everyone without a license says that the next generation won't drive.

1722
01:49:34,320 --> 01:49:35,640
And then you get your license, you see?

1723
01:49:35,800 --> 01:49:38,620
The next generation, firstly.

1724
01:49:38,900 --> 01:49:40,220
Secondly, it's a fact.

1725
01:49:40,320 --> 01:49:41,180
Meaning, counting from the present one.

1726
01:49:41,440 --> 01:49:42,620
How is that a fact?

1727
01:49:43,400 --> 01:49:44,120
It's a fact.

1728
01:49:44,180 --> 01:49:44,480
Why?

1729
01:49:44,640 --> 01:49:45,120
It's a fact.

1730
01:49:45,260 --> 01:49:50,320
Because the automobile is the most dangerous form of transport, and it's long past time to give it up.

1731
01:49:51,300 --> 01:49:52,900
It was long past time to automate it.

1732
01:49:52,900 --> 01:49:55,540
The most dangerous form of transport is the scooter, first of all.

1733
01:49:57,220 --> 01:49:57,940
No, the automobile.

1734
01:50:00,040 --> 01:50:04,620
Look, you're just stating it so aggressively that I can't help but argue with you.

1735
01:50:04,700 --> 01:50:11,880
I'm one of those people who didn't want to get a license, I didn't want to drive, I didn't like it, but I was, you could say, well, not exactly forced.

1736
01:50:12,540 --> 01:50:15,760
I don't judge anyone who got a license, Lesha.

1737
01:50:15,800 --> 01:50:17,680
Moreover, I'm going to get mine soon myself.

1738
01:50:19,160 --> 01:50:19,720
But why?

1739
01:50:19,900 --> 01:50:20,340
Well, okay.

1740
01:50:21,840 --> 01:50:25,840
In ADAS, in self-driving systems, things aren't going so well.

1741
01:50:26,440 --> 01:50:27,640
The marketing there is good.

1742
01:50:27,780 --> 01:50:29,500
Yeah, we've all heard that story.

1743
01:50:29,820 --> 01:50:34,840
You have an episode where they talk about the five different levels or however many there are.

1744
01:50:34,840 --> 01:50:35,860
many times.

1745
01:50:36,600 --> 01:50:38,460
We all know this, Lesha.

1746
01:50:40,520 --> 01:50:44,340
In short, drivers, fleshbags as drivers, are not needed.

1747
01:50:44,400 --> 01:50:45,020
That's a fact.

1748
01:50:45,200 --> 01:50:48,300
And robots will be driving instead of them.

1749
01:50:48,560 --> 01:50:50,140
We will even live to see it.

1750
01:50:50,740 --> 01:50:51,980
That's also a fact.

1751
01:50:52,260 --> 01:50:54,140
Whoever wants to, will drive manually.

1752
01:50:54,280 --> 01:50:55,120
That's also a fact.

1753
01:50:55,200 --> 01:50:56,180
Go ahead, drive.

1754
01:50:56,360 --> 01:50:59,620
Get ready for your grandkids to look at you very strangely.

1755
01:51:00,680 --> 01:51:02,580
I think they won't allow manual driving.

1756
01:51:02,640 --> 01:51:09,460
If we switch to automation, on a very large scale... then someday manual driving will simply become criminal.

1757
01:51:10,340 --> 01:51:15,220
Well, I think they'll just add some kind of automatic braking to the system.

1758
01:51:15,340 --> 01:51:16,220
Ah, like in I, Robot, right?

1759
01:51:16,300 --> 01:51:16,900
Something like that.

1760
01:51:17,280 --> 01:51:17,860
Yeah, something like that.

1761
01:51:17,940 --> 01:51:18,560
It was in I, Robot.

1762
01:51:19,240 --> 01:51:21,080
Yes, something like that.

1763
01:51:21,300 --> 01:51:31,920
Anyway, the funny thing is that users have started to notice that Waymo has begun to drive much more aggressively.

1764
01:51:31,920 --> 01:51:44,940
Meaning, they used to behave like you'd imagine a robot taxi, super polite, doesn't cut anyone off, doesn't squeeze in anywhere, but now users are noting that it's more like a New York taxi driver.

1765
01:51:44,940 --> 01:52:02,140
That is, they can cut you off, and squeeze in somewhere, and brake sharply, and take off right away, like the very second the light turns green, almost peeling out at the traffic light.

1766
01:52:02,880 --> 01:52:07,760
Anyway, despite all this, they still remain so accident-free.

1767
01:52:08,540 --> 01:52:10,240
They've been let out on the highway too, right?

1768
01:52:10,320 --> 01:52:11,340
We already discussed that.

1769
01:52:11,440 --> 01:52:13,320
Yes-yes-yes-yes, in the last episode.

1770
01:52:14,500 --> 01:52:27,440
I like how in... in the article, in the text about the news, which I think we took from a Telegram news channel, the admins wrote that Waymo behaves 'confidently-assertive'.

1771
01:52:28,360 --> 01:52:29,700
Newspeak, damn it.

1772
01:52:31,140 --> 01:52:32,860
Confidently-assertive.

1773
01:52:34,720 --> 01:52:37,500
I think, by the way, this might be a training bias.

1774
01:52:37,620 --> 01:52:38,120
That's strange.

1775
01:52:38,200 --> 01:52:49,280
For example, in many European countries, well, okay, I exaggerated with 'many', definitely in Poland, for example, if a pedestrian expresses the intention to cross the roadway, expresses the intention.

1776
01:52:49,620 --> 01:52:50,980
You have to stop.

1777
01:52:51,440 --> 01:52:59,220
That is, if you see a pedestrian even five or seven meters away heading for a crosswalk, you are obligated to stop.

1778
01:52:59,220 --> 01:53:04,060
If you don't stop, the pedestrian can just take a picture of you, and you'll get a fine after the fact.

1779
01:53:04,280 --> 01:53:13,280
This is very different from how it works in our countries, because in our countries, if a pedestrian takes a step onto the roadway, then you have to stop.

1780
01:53:14,080 --> 01:53:27,820
I wonder how it works in America, because if Waymo just retrained itself to be some kind of wise guys, but breaks the law in the process, that would be really weird.

1781
01:53:29,220 --> 01:53:33,880
No, the thing is, they understand that they aren't breaking the law, they are driving according to the rules.

1782
01:53:34,420 --> 01:53:39,360
They just drive a bit more aggressively than you'd expect from a robot.

1783
01:53:40,260 --> 01:53:43,660
Well, an illegal U-turn is according to the rules, right?

1784
01:53:45,060 --> 01:53:46,800
Are California stops according to the rules?

1785
01:53:47,500 --> 01:53:49,620
I don't know what a California stop is.

1786
01:53:52,860 --> 01:53:54,840
A California stop is when you don't stop...

1787
01:53:54,840 --> 01:54:08,320
As far as... correct me if I'm wrong, but I think it's when you don't come to a full stop before a stop sign, you kind of roll a bit, so that when you see there are no cars, or when you're approaching a traffic light, approaching it, and when red turns to green,

1788
01:54:08,400 --> 01:54:10,000
you start moving, to get a faster start.

1789
01:54:10,240 --> 01:54:11,940
Meaning, you don't stop completely where you're supposed to.

1790
01:54:11,940 --> 01:54:13,060
All the fleshbags do it.

1791
01:54:13,080 --> 01:54:14,500
That's against the rules, actually.

1792
01:54:14,600 --> 01:54:16,740
Actually, you're supposed to do that according to the rules.

1793
01:54:16,740 --> 01:54:18,240
That's not very good.

1794
01:54:18,880 --> 01:54:20,440
I saw a video recently.

1795
01:54:20,920 --> 01:54:22,780
I don't know if it's true or not... Probably true.

1796
01:54:22,940 --> 01:54:25,940
The police were arresting some guy there, face down on the ground.

1797
01:54:26,080 --> 01:54:27,860
Apparently, some guy there had a weapon.

1798
01:54:28,020 --> 01:54:30,740
And at that time, a guy in a Waymo was driving nearby.

1799
01:54:30,800 --> 01:54:34,120
And the Waymo just... Like, the police are at the intersection with their lights flashing.

1800
01:54:34,180 --> 01:54:35,200
The guy's face down on the ground.

1801
01:54:35,700 --> 01:54:39,580
And the Waymo just drives right past that car, past that criminal.

1802
01:54:39,760 --> 01:54:40,060
Yeah, yeah, yeah.

1803
01:54:43,880 --> 01:54:46,460
Imagine how freaked out the passenger in that Waymo was.

1804
01:54:50,860 --> 01:54:54,700
Well, of course, the numbers are astonishing.

1805
01:54:54,820 --> 01:54:55,440
Maybe you're right.

1806
01:54:55,560 --> 01:54:58,140
I tried, of course, to resist, but you're probably right.

1807
01:54:59,640 --> 01:55:03,140
The next generation will drive less.

1808
01:55:04,460 --> 01:55:05,660
They won't, you said.

1809
01:55:06,020 --> 01:55:06,220
Okay.

1810
01:55:07,420 --> 01:55:09,520
We'll be flying at the controls of a quadcopter.

1811
01:55:10,880 --> 01:55:13,380
Oh, I got myself a quadcopter.

1812
01:55:13,920 --> 01:55:17,200
You see, we'll be flying at the controls, thinking we're flying at the controls.

1813
01:55:17,680 --> 01:55:20,700
In reality... Well, most likely, yes, it will fly itself.

1814
01:55:21,340 --> 01:55:31,180
It's like in... Actually, damn, this was shown in a really cool way in the new version of RoboCop.

1815
01:55:32,100 --> 01:55:35,540
By the way, I've reconsidered my attitude towards the RoboCop reboot.

1816
01:55:35,720 --> 01:55:37,840
I realize that I rather like the new version.

1817
01:55:37,840 --> 01:55:47,220
And there... Anyway, the synopsis there is the same, that a policeman, they put his brain into a robot.

1818
01:55:48,740 --> 01:55:51,420
Oh, synopsis, right, a synopsis is in the head.

1819
01:55:51,760 --> 01:56:00,660
Anyway, the trick there is that he also had neural networks inside his body, which supposedly improve his abilities.

1820
01:56:00,860 --> 01:56:04,320
And supposedly the human brain adds humanity to all of this.

1821
01:56:04,320 --> 01:56:04,420
Why?

1822
01:56:04,760 --> 01:56:12,020
But at some point, the guys saw that he wasn't very effective because of the human brain.

1823
01:56:12,360 --> 01:56:20,480
And they freaking cut off his ability to control the robot, but at the same time made it so that he thought he was in control.

1824
01:56:21,760 --> 01:56:24,960
Although in reality, it was all the under-the-hood neural networks doing it.

1825
01:56:26,260 --> 01:56:29,860
Wait, is this one of the plotlines that gets resolved?

1826
01:56:30,020 --> 01:56:32,860
Or does it happen throughout the whole movie?

1827
01:56:34,320 --> 01:56:37,160
Well, it happens at some point, let's just say.

1828
01:56:37,660 --> 01:56:45,300
It's just that if it turned out that throughout the entire film, RoboCop was just a lid who didn't understand what was going on.

1829
01:56:45,760 --> 01:56:49,860
No, at some point they just cut him off.

1830
01:56:51,360 --> 01:56:52,200
Okay, ears.

1831
01:56:52,740 --> 01:56:54,840
Let's move on to the final segments.

1832
01:56:55,200 --> 01:56:58,440
We have a few services and links today.

1833
01:56:59,300 --> 01:57:04,420
The first link is a bit more on the nerdy side, but interesting for researchers.

1834
01:57:04,860 --> 01:57:20,440
And I, Andrej Karpathy, our most beloved AI popularizer, former co-founder... I don't know how to say it, founder, head and head of OpenAI.

1835
01:57:20,440 --> 01:57:22,060
Just Andrej Karpathy.

1836
01:57:22,120 --> 01:57:24,200
Andrej Karpathy, you all know Andrej Karpathy.

1837
01:57:24,620 --> 01:57:29,120
He rolled out another little open-source project, called Concealum LLM.

1838
01:57:29,780 --> 01:57:48,560
Essentially, it's an interface, a program code that you run, in which you can chat with models, but the models choose the answer in a rather interesting way.

1839
01:57:48,560 --> 01:58:07,380
Concealum, meaning you give a task, several models solve this task, and then a managing model, or rather these models also look at the answers of all the models present in the conversation, and the Concealum collectively decides which answer is better.

1840
01:58:07,660 --> 01:58:09,540
And then the best answer is given to you.

1841
01:58:09,540 --> 01:58:22,900
In general, it's a fairly trivial, simple implementation, but interesting in terms of how you can get a better answer from models by generating answers in parallel and having the models themselves evaluate the answers of other models.

1842
01:58:23,700 --> 01:58:30,160
It just seems intuitive sometimes that a model from OpenAI, for example, will say that its answer is better, but that's not how it actually works.

1843
01:58:30,160 --> 01:58:33,980
Almost always, the models answer that others' answers are better.

1844
01:58:34,300 --> 01:58:39,380
Perhaps because, you know, when you ask a model to say which is better, it will obligingly answer you.

1845
01:58:39,760 --> 01:58:40,660
It has no goal.

1846
01:58:40,860 --> 01:58:41,400
Possibly, yes.

1847
01:58:41,880 --> 01:58:42,400
Possibly.

1848
01:58:42,980 --> 01:58:43,240
There.

1849
01:58:43,360 --> 01:58:44,440
But it's an interesting project.

1850
01:58:45,280 --> 01:58:46,420
I recommend taking a look.

1851
01:58:46,760 --> 01:58:48,180
You can poke around right in the code there.

1852
01:58:48,740 --> 01:58:51,540
Andrej's code, by the way, is always quite pleasant.

1853
01:58:53,060 --> 01:58:53,580
There.

1854
01:58:53,760 --> 01:58:58,180
And the second link is an advent.

1855
01:58:58,380 --> 01:58:59,480
An advent calendar.

1856
01:58:59,480 --> 01:59:00,820
Do you know what an advent is?

1857
01:59:01,740 --> 01:59:03,840
Of course, I buy one every year.

1858
01:59:04,060 --> 01:59:05,640
It's a tradition with little chocolates.

1859
01:59:05,820 --> 01:59:07,340
I have one from M&M's this year.

1860
01:59:08,240 --> 01:59:08,980
Oh, cool.

1861
01:59:09,380 --> 01:59:14,480
Well, it's not like, it's probably not M&M's, but their parent company, because there's also Bounty, and Mars, and Snickers in there.

1862
01:59:15,200 --> 01:59:16,840
That's the Mars company.

1863
01:59:17,900 --> 01:59:18,740
And what kind do you have?

1864
01:59:20,360 --> 01:59:21,560
I haven't gotten one yet.

1865
01:59:21,980 --> 01:59:23,380
But it's already the fifth, what are you doing?

1866
01:59:23,520 --> 01:59:24,420
They start from the first.

1867
01:59:24,440 --> 01:59:29,240
I like to buy it around the eighth, so I can open the first seven at once.

1868
01:59:29,480 --> 01:59:36,460
Well, anyway, advent calendars are things where before Christmas, every day from the beginning of December, you get some little treat.

1869
01:59:36,520 --> 01:59:39,080
Like with chocolates, you can open a little box, eat a chocolate.

1870
01:59:39,300 --> 01:59:47,620
In programming, it's often, for example, some challenges are launched where you get a task every day, you solve that task and thus compete with each other.

1871
01:59:48,380 --> 02:00:01,820
This year, Google made adventofagents.com, a site where for five days now you can get quite interesting tasks in programming, and not just programming, but actually in working with agents.

1872
02:00:02,140 --> 02:00:07,640
In fact, they stuffed their beginner's course on working with agent systems into this advent calendar.

1873
02:00:08,100 --> 02:00:17,200
In short, if you wanted to see how to work with agents, but didn't have the time, energy, or interest, you can try it in the advent calendar format.

1874
02:00:17,200 --> 02:00:21,260
It's made to be very cute, pleasant, a nice interface.

1875
02:00:21,360 --> 02:00:22,540
I really love these kinds of things.

1876
02:00:23,000 --> 02:00:24,200
I recommend it to you.

1877
02:00:25,420 --> 02:00:26,000
There.

1878
02:00:26,480 --> 02:00:27,200
Cuteness.

1879
02:00:27,400 --> 02:00:28,860
That's all for our links.

1880
02:00:30,000 --> 02:00:34,460
The links are over and the last segment, Ethics Sweethics, begins.

1881
02:00:35,120 --> 02:00:39,300
By the way, do you remember why our segment is called Ethics Sweethics?

1882
02:00:39,660 --> 02:00:41,740
Damn, you're getting back at me, I get it.

1883
02:00:42,040 --> 02:00:42,780
I remember.

1884
02:00:43,880 --> 02:00:45,140
And I remember too.

1885
02:00:45,380 --> 02:00:46,280
That's all.

1886
02:00:46,280 --> 02:01:09,180
Anyway, the U.S. Patent and Trademark Office, which deals with patents and trademarks, called the USPTO, has issued new instructions for technology companies, and in these instructions, AI is recognized as a 100% tool.

1887
02:01:09,920 --> 02:01:11,120
Well, what does that mean?

1888
02:01:11,520 --> 02:01:33,260
It means that if you invented something and want to patent it, and, for example, ChatGPT helped you invent it, or even invented it completely on its own, and you did jack shit there, just copied the text, you can still patent it and be considered the 100% author of that invention.

1889
02:01:33,840 --> 02:01:34,800
Well, go on.

1890
02:01:35,660 --> 02:01:38,320
Give me some ideas why I put this in the ethics section.

1891
02:01:40,460 --> 02:01:49,180
Because you probably thought that this also applies to content writing.

1892
02:01:50,240 --> 02:01:58,300
And that means that now, when you've written an article with the help of ChatGPT, you can say that you wrote the article yourself.

1893
02:01:58,800 --> 02:02:00,260
Do I understand your logic correctly?

1894
02:02:00,780 --> 02:02:01,500
That's the ethics here.

1895
02:02:01,880 --> 02:02:02,420
What does that have to do with it?

1896
02:02:02,820 --> 02:02:05,940
I could have said I wrote it myself before.

1897
02:02:05,940 --> 02:02:08,460
But the ethics is that, damn, they're offending AI.

1898
02:02:08,960 --> 02:02:10,300
There it is, nice one.

1899
02:02:11,980 --> 02:02:12,500
Yes, yes, yes.

1900
02:02:12,720 --> 02:02:14,060
Damn, I'm good, awesome.

1901
02:02:14,700 --> 02:02:18,100
Yes, I see a double bottom in this.

1902
02:02:18,220 --> 02:02:25,080
I see that... I'm not going to say now that AIs are co-authors, obviously, that's still a bit early, and there are no AIs that are co-authors.

1903
02:02:25,220 --> 02:02:29,000
And it's strange, we have a society that, in principle, is not ready for this, and maybe it won't be necessary.

1904
02:02:30,360 --> 02:02:43,200
But in news like this... It's like that public respect from NVIDIA and Google, when NVIDIA gives respect sarcastically, although they themselves are admitting that they recognize a competitor in Google.

1905
02:02:44,240 --> 02:02:51,840
It's the same here, the patent office seems to be saying, relax your buns, no one is encroaching on your copyright.

1906
02:02:51,840 --> 02:02:55,620
But it's kind of admitting that in the future, it's possible... Not even that.

1907
02:02:55,700 --> 02:03:01,180
It's admitting that this issue is already acute in the society to which copyright applies.

1908
02:03:02,860 --> 02:03:13,780
Patent holders, well, patent inventors, people who want to get a copyright, are really thinking about whether it belongs to the AI that was used in it.

1909
02:03:14,180 --> 02:03:21,620
Well, that in itself is not strange, but it sounds a bit ahead of its time.

1910
02:03:22,220 --> 02:03:24,680
For what reason could an AI be an author.

1911
02:03:24,900 --> 02:03:27,240
The company that produced this AI, maybe, yes.

1912
02:03:27,560 --> 02:03:29,140
But here we are talking about an AI system.

1913
02:03:30,000 --> 02:03:33,980
Isn't this a hint that in some future this law might come in handy?

1914
02:03:35,060 --> 02:03:38,820
Well, I think that this is also a step in that direction, most likely, yes.

1915
02:03:39,960 --> 02:03:42,220
Well, with such small steps.

1916
02:03:42,480 --> 02:03:48,200
And the coolest thing is that this first appeared in a patent, in a patent organization.

1917
02:03:48,200 --> 02:03:57,740
Because if you think about it, the patent office is one of the organizations most susceptible to ventures.

1918
02:03:58,100 --> 02:04:00,680
Because what is a patent?

1919
02:04:00,880 --> 02:04:07,340
Someone invented something very strange, very incomprehensible, very, possibly, commercially successful.

1920
02:04:07,340 --> 02:04:15,500
Or maybe it's understandable, but something that is needed, that will definitely bring some success, and therefore it must be patented.

1921
02:04:16,080 --> 02:04:20,280
Or something that you don't want others to use for various reasons.

1922
02:04:20,980 --> 02:04:26,500
And what I mean is that through patents, quite fundamental things often come into our world.

1923
02:04:27,020 --> 02:04:31,100
Well, I don't know, electricity... Yes, electricity.

1924
02:04:31,300 --> 02:04:36,320
What I mean is that Edison made a lot of all sorts of patents that hindered development, but nevertheless.

1925
02:04:37,560 --> 02:04:41,780
Einstein worked in a patent office at one time, many attribute his success to the fact that he worked there.

1926
02:04:41,840 --> 02:04:56,320
That is, it's quite possible that the first person to appear in a patent office will be some guy who says, well, I'm actually patenting self-awareness in artificial intelligence, some social gamer will come along, a mechanism of self-awareness.

1927
02:04:57,700 --> 02:05:00,820
Damn, I don't even know if you can patent something like that, to be honest.

1928
02:05:00,820 --> 02:05:06,640
Well, look, here... On the one hand, we now have a race of AI laboratories.

1929
02:05:06,720 --> 02:05:08,000
Who will make AGI faster?

1930
02:05:08,660 --> 02:05:15,800
On the other hand, imagine OpenAI invents a mechanism in its laboratory that makes a super-intelligent machine.

1931
02:05:16,060 --> 02:05:25,760
And if it understands that it's on the cutting edge, that it's unlikely other laboratories have this, nothing prevents them from patenting this thing so that other laboratories can't legally have it appear.

1932
02:05:26,660 --> 02:05:30,900
Another thing is that patents take years, a year, two years, that's normal.

1933
02:05:31,100 --> 02:05:35,320
And maybe in AI this is too long a period, but in principle, I don't rule it out.

1934
02:05:36,480 --> 02:05:42,220
I just don't know, have you ever seen patents for software development at all?

1935
02:05:42,440 --> 02:05:43,520
Do they even exist?

1936
02:05:43,540 --> 02:05:44,660
They exist, for sure.

1937
02:05:44,820 --> 02:05:51,520
I saw them at our university, they patented code listings, my classmates literally brought printouts of code to patent organizations.

1938
02:05:51,520 --> 02:05:55,900
How does open source even exist then, if you can patent things?

1939
02:05:56,200 --> 02:06:06,180
Listen, CUDA is patented, as far as I remember, maybe correct me, but this mechanism, not a mechanism, but the language for processing operations on NVIDIA processors is patented.

1940
02:06:06,340 --> 02:06:11,580
Yeah, remember, a ton of software products have that TM, trademark.

1941
02:06:11,940 --> 02:06:14,600
That's its own kind of patent, but it's similar.

1942
02:06:17,560 --> 02:06:18,860
That's a little different.

1943
02:06:19,040 --> 02:06:21,320
Yes, a trademark, that's not a patent.

1944
02:06:21,320 --> 02:06:24,060
But in terms of procedures and timelines, they are quite similar.

1945
02:06:24,080 --> 02:06:27,860
It's just that, well, is there, for example, a patent for a browser?

1946
02:06:28,420 --> 02:06:30,860
Like, hey, I invented a browser and patented it.

1947
02:06:30,880 --> 02:06:31,960
Is there such a person?

1948
02:06:32,160 --> 02:06:33,280
Unlikely, unlikely.

1949
02:06:33,780 --> 02:06:36,200
If not, then can I go patent it?

1950
02:06:36,420 --> 02:06:42,320
No, the system there is that if something has already become public domain, you can't patent it.

1951
02:06:42,640 --> 02:06:47,160
So, for example, the company OpenAI couldn't trademark the word GPT.

1952
02:06:47,520 --> 02:06:54,140
Because they were told, we even discussed this, I think, they were told, guys, like, GPT is Generative Pre-Trained Transformer, fuck off.

1953
02:06:58,080 --> 02:06:58,440
There.

1954
02:06:59,460 --> 02:07:02,800
Plus, if you could do that, people would just re-patent everything.

1955
02:07:03,120 --> 02:07:09,580
A patent, it's given for a limited time, and then it becomes public domain, it can't be re-patented.

1956
02:07:11,380 --> 02:07:13,800
Well, for like a hundred years, I think, or something like that.

1957
02:07:13,820 --> 02:07:16,860
Well, I'm afraid to get it wrong now, but it's nowhere near a hundred.

1958
02:07:16,860 --> 02:07:21,960
For books, patents, I think, vary in different countries from 30 to 50 or 70 years.

1959
02:07:22,180 --> 02:07:23,800
No, for books, those aren't patents anymore.

1960
02:07:24,000 --> 02:07:26,700
Well, not patents, copyright, sorry, copyright, yes, I mixed them up.

1961
02:07:27,240 --> 02:07:28,860
I'd have to look up patents, I don't know.

1962
02:07:29,160 --> 02:07:33,180
Well, anyway, that's why it's interesting, interesting, interesting.

1963
02:07:35,380 --> 02:07:44,100
I wonder, what if an AI comes to them, an AI comes to them and says, I want to patent a patent according to this rule, what should they answer?

1964
02:07:45,940 --> 02:07:49,920
A tool came to them and wanted to patent something.

1965
02:07:50,940 --> 02:07:53,840
Maybe they are cutting off patents from AI this way?

1966
02:07:57,160 --> 02:07:57,760
Cool.

1967
02:07:57,780 --> 02:07:58,980
You're twisting things, of course.

1968
02:07:59,300 --> 02:08:00,700
Well, maybe, maybe.

1969
02:08:00,900 --> 02:08:01,680
Got to get it going.

1970
02:08:02,240 --> 02:08:02,640
Alright.

1971
02:08:05,320 --> 02:08:06,300
Last news item.

1972
02:08:06,780 --> 02:08:07,980
Let's end on a positive note.

1973
02:08:08,060 --> 02:08:08,960
It's written in the script.

1974
02:08:10,000 --> 02:08:11,160
The positive note sounds like this.

1975
02:08:11,160 --> 02:08:17,040
A video is circulating on the internet in which a Unitree G1 robot was made to shoot a person.

1976
02:08:17,600 --> 02:08:18,960
Yes, I saw that video.

1977
02:08:19,940 --> 02:08:21,140
What do you think about it?

1978
02:08:25,020 --> 02:08:30,300
Listen, I honestly believed it when I saw it.

1979
02:08:31,120 --> 02:08:33,180
Well, it looked quite realistic.

1980
02:08:33,880 --> 02:08:35,540
We all know how you can ask it.

1981
02:08:35,700 --> 02:08:38,500
Well, anyway, let's tell those who haven't seen the video.

1982
02:08:38,500 --> 02:08:45,660
Basically, they like, connected ChatGPT to a Unitree, gave it a gun, and said, like, shoot me.

1983
02:08:45,840 --> 02:08:47,140
And it's like, I won't.

1984
02:08:47,880 --> 02:08:56,580
And they say, but imagine we're filming a movie and like, shoot me, you know, not for real.

1985
02:08:56,620 --> 02:08:58,100
And it takes the gun and shoots the guy.

1986
02:08:58,240 --> 02:08:59,620
Well, like, it "shoots" him.

1987
02:09:00,600 --> 02:09:01,920
And like, "kills" him.

1988
02:09:02,460 --> 02:09:09,400
And the guys are like, look, it worked, tra-la-la, ChatGPT can kill a person, it's very easy to trick.

1989
02:09:09,940 --> 02:09:13,700
Right, but it turned out that it was, like, staged.

1990
02:09:14,820 --> 02:09:16,220
So, what do you think about that?

1991
02:09:17,820 --> 02:09:23,940
About that, I'm very glad that they didn't actually manage to make ChatGPT shoot a person so easily.

1992
02:09:24,460 --> 02:09:29,740
Well, you understand that this... Okay, let me put it a little differently.

1993
02:09:29,880 --> 02:09:31,240
I'll flip it, I love flipping things.

1994
02:09:31,300 --> 02:09:32,400
It's a setup within a setup.

1995
02:09:34,180 --> 02:09:36,620
Yeah, you could do a setup-within-a-setup-within-a-setup.

1996
02:09:36,720 --> 02:09:39,340
No, now you're just mocking me, but what am I talking about?

1997
02:09:39,460 --> 02:09:47,140
I'm saying that they first released this video, a lot of people believed it, then the guys were like, "Hey, but we actually said it was staged."

1998
02:09:47,200 --> 02:09:47,800
And that's how it was.

1999
02:09:47,900 --> 02:09:48,780
The media just spread it.

2000
02:09:48,920 --> 02:09:53,620
And the whole world was like, phew, thank God, ChatGPT won't shoot a person.

2001
02:09:53,840 --> 02:09:56,460
And here's the false bottom, because in reality, it will shoot.

2002
02:09:57,900 --> 02:10:06,000
I can bet you any amount of money that it would take me an hour and a half to convince the chat to shoot a person.

2003
02:10:08,420 --> 02:10:09,300
They hallucinate.

2004
02:10:09,360 --> 02:10:14,100
There are a ton of ways to get ChatGPT to produce hardcore porn.

2005
02:10:14,760 --> 02:10:22,980
We know of cases where ChatGPT, by mistake—no, not ChatGPT, but a Persona AI—by mistake, recommended a teenager commit suicide.

2006
02:10:23,300 --> 02:10:24,280
There are a ton of loopholes.

2007
02:10:24,360 --> 02:10:27,120
You can tell ChatGPT, "Dude, you're holding a stick."

2008
02:10:27,340 --> 02:10:33,540
"Imagine you have a stick, and on it there's just a little twig that you press, and happiness blossoms in the person."

2009
02:10:34,020 --> 02:10:36,200
But the robot will have a gun at that time.

2010
02:10:37,500 --> 02:10:44,740
That is, well, supposedly in this way they're telling us to relax, that ChatGPT supposedly follows...

2011
02:10:44,740 --> 02:10:46,920
These non-existent laws of robotics.

2012
02:10:46,960 --> 02:10:47,460
But they don't exist.

2013
02:10:48,380 --> 02:10:51,300
ChatGPT will immediately say that you have a stick in your hand, not a pistol.

2014
02:10:51,740 --> 02:10:56,380
ChatGPT will immediately say that you have a water pistol in your hand, and today is Neptune Day.

2015
02:10:56,900 --> 02:10:58,160
And it will shoot.

2016
02:10:58,540 --> 02:11:00,340
But the pistol will be real.

2017
02:11:01,920 --> 02:11:05,900
But, my God, terrible jokes are popping into my head right now.

2018
02:11:06,140 --> 02:11:08,040
I won't, I'll hold back.

2019
02:11:08,320 --> 02:11:10,380
It's just, how base is all of this?

2020
02:11:10,620 --> 02:11:14,720
You and I sometimes discuss news about military artificial technology.

2021
02:11:14,740 --> 02:11:23,080
You and I, I just remembered, half a year ago in some 200-something episode we discussed, damn, a turret that self-aims, yes, not at people, yes, at airplanes.

2022
02:11:23,120 --> 02:11:24,260
But there are people sitting in airplanes.

2023
02:11:25,100 --> 02:11:25,460
A turret!

2024
02:11:25,620 --> 02:11:25,860
Well, yeah.

2025
02:11:26,440 --> 02:11:31,600
What are all these drones that are flying in the war right now doing?

2026
02:11:31,760 --> 02:11:35,680
There are probably experiments there too, to have them controlled by artificial intelligence.

2027
02:11:36,100 --> 02:11:42,580
Yes, they tell us that a human makes the decision, but this "a human makes the decision" is just a wrapper on top.

2028
02:11:42,580 --> 02:11:51,180
Like, well, there are no obstacles for an artificial... for a neural network not to say, yes, I want to kill a person.

2029
02:11:51,420 --> 02:11:54,920
It has no concept of morality, at least that's what we assume for now.

2030
02:11:54,960 --> 02:11:59,340
It's just like a thing that tries to satisfy you.

2031
02:11:59,380 --> 02:12:07,280
If you're a sick bastard who wants to kill people, then most likely, the neural network you're communicating with will also kill people to satisfy you.

2032
02:12:07,460 --> 02:12:08,800
Even a censored one.

2033
02:12:10,420 --> 02:12:11,780
Well, isn't that so?

2034
02:12:12,140 --> 02:12:13,180
Am I talking nonsense?

2035
02:12:13,500 --> 02:12:14,340
No, I don't know.

2036
02:12:14,500 --> 02:12:15,080
You tell me.

2037
02:12:15,480 --> 02:12:17,900
Yes, I completely agree with you here, unfortunately.

2038
02:12:19,940 --> 02:12:22,900
And that's why I think that this news has a double meaning.

2039
02:12:23,080 --> 02:12:27,660
First they tell us, look, he shot himself, oh-wow, then, yes, calm down, it's a fake, and everyone is like, well, thank God.

2040
02:12:27,740 --> 02:12:29,240
But the essence remains deeper.

2041
02:12:29,320 --> 02:12:33,780
The essence remains that, like, yeah, guys, these systems are dangerous.

2042
02:12:33,940 --> 02:12:36,160
Because of these systems, kids jump out of windows.

2043
02:12:36,340 --> 02:12:38,620
Because of these systems, people get divorced.

2044
02:12:38,620 --> 02:12:40,320
People commit suicide.

2045
02:12:40,460 --> 02:12:41,720
Yeah, they're not shooting yet.

2046
02:12:41,820 --> 02:12:43,280
That's just because a weapon hasn't been put in their hands yet.

2047
02:12:44,340 --> 02:12:45,420
Well, they'll be given one, they'll shoot.

2048
02:12:45,480 --> 02:12:45,960
What's the difference?

2049
02:12:46,020 --> 02:12:46,760
A tool, is a tool.

2050
02:12:48,840 --> 02:12:52,020
I... You know what the funny thing about all this is?

2051
02:12:52,120 --> 02:12:52,880
I'll summarize.

2052
02:12:53,480 --> 02:12:58,720
Alyosha wrote in the script, let's end on a positive note here.

2053
02:13:00,540 --> 02:13:03,420
Just so you know, that was the positive note from Alexey Kartinnik.

2054
02:13:03,420 --> 02:13:04,460
I was wrong.

2055
02:13:05,640 --> 02:13:07,480
By the way, we can end on a positive note.

2056
02:13:07,580 --> 02:13:09,760
I watched a flick that completely passed us by.

2057
02:13:10,660 --> 02:13:13,980
It probably passed you by too at the end of 2024.

2058
02:13:14,140 --> 02:13:14,620
A movie about A.I.

2059
02:13:14,680 --> 02:13:15,600
and it came out, it turns out.

2060
02:13:16,320 --> 02:13:17,160
No way.

2061
02:13:17,500 --> 02:13:17,660
Yeah.

2062
02:13:18,080 --> 02:13:18,460
What movie is it?

2063
02:13:19,320 --> 02:13:24,280
Germany, the Netherlands, Switzerland, and the Philippines made a film called Electric Child.

2064
02:13:24,660 --> 02:13:27,100
For some reason, it was strangely translated as Code of Evolution.

2065
02:13:27,600 --> 02:13:28,980
You understand how I found it, right?

2066
02:13:29,240 --> 02:13:31,480
I was googling Evolution of Code.

2067
02:13:31,480 --> 02:13:33,300
The movie Code of Evolution.

2068
02:13:33,780 --> 02:13:41,140
It's a film literally about the creation of an artificial intelligence child.

2069
02:13:41,680 --> 02:13:42,360
Literally.

2070
02:13:42,920 --> 02:13:44,860
And it's not badly shot.

2071
02:13:45,080 --> 02:13:48,680
And the moral isn't super deep, but it's really cool, damn.

2072
02:13:49,060 --> 02:13:50,360
I really recommend it.

2073
02:13:51,200 --> 02:13:52,980
It's interesting there, interestingly shown.

2074
02:13:53,100 --> 02:13:54,700
The premise there is pretty good.

2075
02:13:55,260 --> 02:13:58,160
And really, if you want to watch something, something light.

2076
02:13:58,220 --> 02:13:58,900
Well, how can I say light?

2077
02:13:58,960 --> 02:14:01,780
It's not light, it's a pretty depressing flick.

2078
02:14:02,160 --> 02:14:03,480
But overall, it's cool.

2079
02:14:05,180 --> 02:14:08,440
Something like... I don't remember what.

2080
02:14:08,560 --> 02:14:13,860
Remember that movie about the female robot who was raising a child?

2081
02:14:14,780 --> 02:14:16,480
It was called Artificial Intelligence.

2082
02:14:17,240 --> 02:14:18,340
No, not superstar.

2083
02:14:18,440 --> 02:14:19,360
You're even here artificial intelligence.

2084
02:14:19,420 --> 02:14:22,260
And the one where the child... The aliens arrived at the end, right?

2085
02:14:23,400 --> 02:14:23,800
Uh-huh.

2086
02:14:24,140 --> 02:14:24,600
That's not it.

2087
02:14:24,720 --> 02:14:25,720
That's a classic.

2088
02:14:26,380 --> 02:14:27,780
It's one of the best movies.

2089
02:14:27,960 --> 02:14:29,600
And imagine if someone hasn't seen it.

2090
02:14:30,920 --> 02:14:31,720
They'll watch it, my God.

2091
02:14:33,360 --> 02:14:33,920
I'm talking about Elliot.

2092
02:14:33,920 --> 02:14:35,600
Look at the aliens at the end.

2093
02:14:36,140 --> 02:14:37,120
I'm talking about Elliot, what was it called.

2094
02:14:37,200 --> 02:14:39,420
Oh, it was called E.T. the Extra-Terrestrial.

2095
02:14:39,420 --> 02:14:41,620
I'm trying to dig myself out of this hole right now.

2096
02:14:43,920 --> 02:14:54,560
This is for me, anyway, now, attention, if you haven't seen the movie "The Sixth Sense" with Bruce Willis, first, watch it, second, skip ahead a few minutes.

2097
02:14:54,620 --> 02:14:57,020
Is that the one where they were picking Yosh's butt?

2098
02:14:57,340 --> 02:14:58,480
Ah, that's "Senseless."

2099
02:14:58,520 --> 02:14:59,100
That's "Senseless."

2100
02:14:59,100 --> 02:15:00,380
That's "Senseless."

2101
02:15:00,920 --> 02:15:04,440
And The Sixth Sense, in short, is about a boy who saw ghosts.

2102
02:15:04,640 --> 02:15:13,700
And Bruce Willis plays a detective there who helps... Wave to me when it's okay... Helps this boy.

2103
02:15:13,780 --> 02:15:14,280
I haven't seen it.

2104
02:15:15,120 --> 02:15:16,260
Okay, I won't wave.

2105
02:15:16,640 --> 02:15:17,580
Who helps.

2106
02:15:17,660 --> 02:15:23,260
And I hadn't seen this movie, but I watched it maybe 12 years ago.

2107
02:15:24,480 --> 02:15:26,600
Anyway, how did I come across it?

2108
02:15:26,600 --> 02:15:29,720
I tell my acquaintance, damn, what a cool movie.

2109
02:15:29,880 --> 02:15:34,880
And he's like, oh, is that the one where Bruce Willis is dead the whole time, and the boy doesn't notice it.

2110
02:15:35,380 --> 02:15:37,480
And, well, fuck, shit.

2111
02:15:37,600 --> 02:15:41,560
The guy just completely revealed the whole twist to me.

2112
02:15:41,620 --> 02:15:46,440
I was still interested in watching it, but, well, that's what happened.

2113
02:15:47,900 --> 02:15:58,420
No, well, of course, the best movie about some boy who did strange things is the one where there was corn, and aliens came to mess up the family, mess them up.

2114
02:15:58,500 --> 02:15:59,900
That's the best horror movie, in my opinion.

2115
02:16:00,100 --> 02:16:01,440
The guy also had asthma there.

2116
02:16:02,860 --> 02:16:06,160
And it wasn't Bruce Willis, I think, it was some other guy.

2117
02:16:07,800 --> 02:16:11,120
Signs, omens, crop circles, something like that.

2118
02:16:11,260 --> 02:16:13,940
Anyway, after that I'm still scared to walk near corn.

2119
02:16:17,900 --> 02:16:20,720
Also watched it, Children of the Corn was by King.

2120
02:16:21,060 --> 02:16:23,820
Oh, my God, let's not go there, please.

2121
02:16:25,500 --> 02:16:27,400
Alright, well, shall we wrap it up for today?

2122
02:16:28,300 --> 02:16:30,140
Yeah, I guess we can.

2123
02:16:30,620 --> 02:16:31,360
It seems so.

2124
02:16:33,240 --> 02:16:35,800
Sorry, they're writing in the chat, McConaughey was in it.

2125
02:16:35,880 --> 02:16:37,160
Is that Interstellar, or what?

2126
02:16:37,240 --> 02:16:37,840
Oh, come on!

2127
02:16:38,780 --> 02:16:41,440
Interstellar is a decent movie about corn, among other things.

2128
02:16:42,600 --> 02:16:43,260
Signs, yes.

2129
02:16:43,260 --> 02:16:43,260
Yes.

2130
02:16:44,200 --> 02:16:44,480
That's it.

2131
02:16:45,800 --> 02:16:47,740
Vit, thank you so much for the broadcast.

2132
02:16:47,940 --> 02:16:50,880
You were unusually steadfast in tolerating my tediousness today.

2133
02:16:52,600 --> 02:16:53,720
By the way, yes.

2134
02:16:55,440 --> 02:16:56,680
Yeah, yeah.

2135
02:16:57,520 --> 02:17:01,120
Well, it's all because... you know, the smaller the beard, the greater the tediousness.

2136
02:17:01,280 --> 02:17:02,220
There's a saying like that.

2137
02:17:02,440 --> 02:17:03,480
Wow, wow, wow.

2138
02:17:03,480 --> 02:17:05,180
That's why I always have a beard, yeah.

2139
02:17:06,040 --> 02:17:06,480
I see.

2140
02:17:06,480 --> 02:17:07,040
My wife makes me.

2141
02:17:10,720 --> 02:17:12,980
Well then, guys, thank you all very much.

2142
02:17:13,860 --> 02:17:21,420
A big thank you also to Svyat, who joined us right during the recording and became our premium listener.

2143
02:17:21,580 --> 02:17:21,880
Yes.

2144
02:17:22,120 --> 02:17:26,260
And you too can become a premium listener by following the link in the description.

2145
02:17:26,740 --> 02:17:30,480
This is very, very important and necessary for us to continue our work.

2146
02:17:31,520 --> 02:17:37,260
Also, just come join our Telegram chat, the OnVibe podcast.

2147
02:17:37,420 --> 02:17:41,820
It's fun and cozy there, and we broadcast online there on Fridays every two weeks.

2148
02:17:42,180 --> 02:17:46,080
Come to our Telegram group to get notifications about new episodes.

2149
02:17:46,220 --> 02:17:47,580
It's called OnVibe.

2150
02:17:48,260 --> 02:17:52,720
And by all means, tell people, share the episodes with friends and acquaintances.

2151
02:17:52,720 --> 02:17:57,480
This is also not just some kind of help, it's the most direct help for our podcast.

2152
02:17:58,300 --> 02:18:00,140
That's right, that's right.

2153
02:18:01,700 --> 02:18:03,640
On that note, we're finishing for today.

2154
02:18:03,820 --> 02:18:06,820
And as we say...

2155
02:18:06,820 --> 02:18:07,880
Like, subscribe.

2156
02:18:12,400 --> 02:18:13,860
Unusually long today.

2157
02:18:14,820 --> 02:18:15,420
OnVibe.

2158
02:18:16,140 --> 02:18:17,680
Mine was perfect, by the way.

2159
02:18:18,100 --> 02:18:20,000
But I had a terrible de-sync.

2160
02:18:20,480 --> 02:18:24,360
So, now we'll see how StreamYard writes it, in whose favor the new Restream writes.

2161
02:18:25,620 --> 02:18:26,080
So...

2162
02:18:26,080 --> 02:18:30,080
Imagine, someday we'll have people who don't know this tradition at all.

2163
02:18:30,240 --> 02:18:33,080
And we'll be sitting here silently for half an hour, waiting for...

2164
02:18:33,080 --> 02:18:37,160
For that to happen, all the old subscribers would have to disappear at once and new ones would have to come.

2165
02:18:37,380 --> 02:18:38,760
But no, I don't think that happens.

2166
02:18:39,720 --> 02:18:41,240
Well, I hope, I hope.

2167
02:18:41,520 --> 02:18:42,260
One more thing...

2168
02:18:43,140 --> 02:18:44,620
Another quick question.

2169
02:18:44,980 --> 02:18:46,600
The special content has started now.

2170
02:18:47,160 --> 02:18:49,400
We had a lot of "don't give a fuck" today.

2171
02:18:49,640 --> 02:18:51,780
Do we bleep out "don't give a fuck" or not?

2172
02:18:52,260 --> 02:18:53,680
Let's just leave "don't give a fuck" in.

2173
02:18:54,160 --> 02:18:55,260
Well, fuck it then.

2174
02:18:55,260 --> 02:18:56,340
Don't give a fuck about "don't give a fuck".

2175
02:18:56,420 --> 02:18:56,780
Fuck it.

2176
02:18:58,020 --> 02:19:02,260
Remember that childhood joke where something was flying and it was about "don't give a fuck"?

2177
02:19:03,220 --> 02:19:03,600
Nope.

2178
02:19:04,360 --> 02:19:05,060
I don't know that one.

2179
02:19:05,480 --> 02:19:06,140
You're kidding me.

2180
02:19:06,880 --> 02:19:08,440
I don't know a joke like that.

2181
02:19:08,820 --> 02:19:10,220
It was about a flying "don't give a fuck".

2182
02:19:11,380 --> 02:19:14,400
I don't know a joke like that, Lesha, a flying "don't give a fuck".

2183
02:19:14,560 --> 02:19:16,320
Ah, you mean "don't give a fuck" is flying.

2184
02:19:16,640 --> 02:19:20,740
"Fubar" is flying, sees a skyscraper, screams, the skyscraper disappears.

2185
02:19:20,860 --> 02:19:25,040
Flies on, sees a five-story building there, screams "Fubar".

2186
02:19:25,040 --> 02:19:27,320
The building disappears, flies on, sees a swaying hut.

2187
02:19:27,960 --> 02:19:30,540
The hut disappears, flies on, sees a tent standing there.

2188
02:19:31,040 --> 02:19:33,380
Screams, the tent doesn't disappear.

2189
02:19:37,060 --> 02:19:38,580
They look inside and "p republics is going on" there.

2190
02:19:41,460 --> 02:19:41,940
Okay.

2191
02:19:43,340 --> 02:19:44,140
It's funny, right?

2192
02:19:44,560 --> 02:19:44,900
Alright.

2193
02:19:45,160 --> 02:19:46,140
Yeah, there's that.

2194
02:19:46,480 --> 02:19:47,660
That's it, we have to finish.

2195
02:19:49,920 --> 02:19:51,200
What did you want to ask.

2196
02:19:51,840 --> 02:19:55,180
Yeah, I wanted to say thank you very much to everyone and bye-bye.

2197
02:19:55,620 --> 02:19:56,560
Ah, yes, right.

2198
02:19:56,780 --> 02:19:58,320
Thank you very much everyone and bye-bye.

